# Pandas DataFrames  

## General  
- Pandas is built on top of Numpy and Matplotlib.  
- Better than Numpy array as DataFrames can contain multiple data types.  
- You can think of DataFrame columns as single-dimension arrays called Series.
-	We can convert a dictionary to a dataframe   
    
### Creating a data frame  
- Noted: missing data is indicated by NaN (Not-a-Number)  

**1. First create a list/s**  
years = [2011,2012,2013,2014,2015,2016,2017,2018,2019,2020]  
durations = [103,101,99,100,100,95,95,96,93,90]  

**2. Then create a dictionary of those lists**  
movie_dict = {'years':years,'durations':durations}  

**3. Then convert that dictionary to a DataFrame**  
durations_df = pd.DataFrame(movie_dict)  

**Import and read a csv file into a DataFrame**  

    import pandas as pd  
    brics = pd.read_csv(“brics.csv”, index_col = 0)  
    index_col = 0 shifts the DataFrame a column left  

**Select column with type DataFrame**  

brics[[‘country’]]  
brics.loc[ :, [‘country’, ‘capital’]] - selects the country and capital columns
brice.loc[ :, ‘country’: ‘capital’] - selects columns from country to capital
brics.iloc[ :, [0,1]]

Row access (and all columns):
brics[1:4] - can only get row access through slicing if you don’t use loc
brics.loc[[‘RU’, ‘IN’, ‘CH’]]
brics.iloc[[1]]
brics.iloc[[1,2,3]]

Row and column access:
brics.loc[[‘RU’, ‘IN’, ‘CH’], [‘country’, ‘capital’]]
brics.iloc[[1,2,3], [0,1]]

loc - label based
iloc - integer based location

- To change the index numbers in the far left column:  

    brics = [‘BR’, ‘CH’, ‘DU’]  

---

Filtering Pandas DataFrames

brics[brics[“area”] > 8] - first checks the column “area” for values that exceed 8 and then outputs all the rows where that condition is met

brics[np.logical_and(brics[“area”] > 8, brics[“area”] < 10)] - searches the “area” column for values exceeding 8 and less than 10 and outputs the rows in the DataFrame which meet these conditions

Calling the .iterrows() method when iterating over a Pandas DataFrame:
Used to iterate over a pandas Data frame rows in the form of (index, series) pair. This function iterates over the data frame column, it will return a tuple with the column name and content in the form of a series.   

for lab, row in brics.iterrows():
	print(lab +  “: “ + row[“capital”])

On each iteration this for loop generates two pieces of data: the ‘label’ in the row and then the actual data in the row as a series.

row is a bit of a misnomer as it refers to the row data in the specified column name.

Pandas
.head() - prints first 5 rows of a DataFrame
.tail() - prints last 5 rows of a DataFrame
.info() - displays info about the data such as type and non-null items
.dtypes - returns the data types in the DataFrame
.shape - this attribute displays the number of rows and columns in a DataFrame
.describe() - computes summary stats for columns with numeric values
.values - prints the data in the DataFrame
.columns - prints column names
.index - prints the number of rows

Sorting and subsetting DataFrames

.sort_values(“weight_kg”) - sorts the column “weight-kg” in ascending order
.sort_values(“weight_kg”, ascending = False) - sorts by descending order
.sort_values([“weight_kg”, “height_cm”], ascending = [True, False]) - sorts “weight_kg” by ascending order and then sorts “height_cm” by descending order


dogs[[“breed”, “height_cm”]] - inner brackets create a list of column names to subset and outer brackets subset the DataFrame
dogs[dogs[“date_of_birth”] > “2015 - 10 - 25”] - outputs all rows in the DataFrame which have a “date_of_birth” that exceeds “2015 - 10 - 25”
dogs[dogs[“color”].isin([“Black”, “Brown”])] - outputs all rows which have a “color” equal to black or brown
dogs[“height_m”] = [45,52,61,39] - creates a new column with the heading “height_m” with the data in the list

DATA MANIPULATION WITH PANDAS


Summary statistics in DataFrames

Methods in DataFrames

dogs[“height_cm”].mean() - calculates the mean of the “height_cm” column
.median()
.mode()
.min()
.max()
.var()
.std()
.sum()
.quantile()
.agg(function) - allows you to pass a defined function into the .agg method. This method allows for non-standard methods to be run on DataFrames based on what the function is
.cumsum() - calculates the cumulative sum
.cummax() - shows the maximum value seen so far
.cummin()
.cumprod()


Counting in DataFrames

.drop_duplicates(subset = ‘name’) - drop duplicates in the name column
.drop_duplicates(subset = [‘name’, ‘breed’]) - drop elements where name and breed are duplicated
dogs[‘breed’].value_counts(sort = True, normalize = True) - counts the number of dogs of each type of breed. Normalize argument turns count into proportion of the total.


Grouped summary stats in DataFrames

dogs.groupby(‘color’)[“weight_kg”].mean() - group by color, select the weight column and calculate the mean for each colour group.
dogs.groupby(‘color’)[“weight_kg”].agg([min, max, sum]) - groups data by color and calculates the min, max and sum of the weight for each group.
dogs.groupby([‘color’, ‘breed’])[“weight_kg”].mean() - groups data by color and then by breed (for each colour) and calculates the mean of the weight for each group.



Pivot tables in DataFrames
A pivot table is a DataFrame with sorted indexes

dogs.groupby(‘color’)[“weight_kg”].mean()  
OR
dogs.pivot_table(values = ‘weight_kg’, index = ‘color’) - groups data by color and calculates the mean of the weight for each colour group. By default the pivot table calculates the mean value for each group.

dogs.pivot_table(values = ‘weight_kg’, index = ‘color’, aggfunc = [np.mean, np.median]) - groups data by color and calculates the mean and median weight for each colour group

dogs.groupby([‘color’, ‘breed’])[“weight_kg”].mean() - groups data by color and then by breed (for each colour) and calculates the mean of the weight for each group. 
OR
dogs.pivot_table(values = ‘weight_kg’, index = ‘color’, columns = ‘breed’) - produces the same result although it looks a bit different. The index argument is the column to group by and display in rows. The column argument is the column to group by and display in columns. 

dogs.pivot_table(values = ‘weight_kg’, index = ‘color’, columns = ‘breed’, fill_value = 0, margins = True) - fill_value fills all missing values with 0. Margins calculates the mean for all rows and columns

You can slice pivot tables in the same way you normally would a DataFrame

.mean(axis = ‘index’) - calling the mean function on a pivot table calculates the mean on the index values i.e. by row
.mean(axis = ‘columns’) - calling the mean function on a pivot table calculates the mean on the column values i.e. by column

Explicit indexes
Setting indexes violates the ‘tidy’ data principle of Python as there is separate syntax.
DataFrames are composed of three parts:
1.	Numpy array for data
2.	Index to store row details
3.	Index to store column details

dogs.set_index(‘name’) - sets the ‘name’ column as the index i.e. the far left column. This column is indented to the left, unlike the date in the DataFrame which is indented to the right.
dogs.reset_index(inplace=True) - resets the DataFrame to its original
dogs.reset_index(drop = True) - deletes the data in the index column

dogs[dogs[‘name’].isin([‘Bella’, ‘Stella’])
OR if you subsetted the ‘name’ column and called this new object dogs_ind
dogs_ind.loc[[‘Bella’, ‘Stella’]]

REMEMBER: when subsetting multiple columns you need to use double square brackets because the inner square brackets generate a list and the outer brackets do the subsetting.

dogs.set_index([‘breed’, ‘color’]] - the inner index ‘color’ is nested inside the outer index ‘breed’

dogs.loc[[‘Labrador’,’Chihuahua’]] - subset outer index
dogs.loc[[(‘Labrador’, ‘brown’),(‘Chihuahua’, ‘black’)]] - subset inner index with tuples. The result must match both conditions

dogs.sort_index() - sorts from outer to inner (in ascending order by default)
dogs.sort_index(level = [‘color’, ‘breed’], ascending = [‘True’, ‘False’]) - controlling the sort index

dogs.set_index(‘name’).sort_index() - sets and sorts the index

dogs.loc[(‘Labrador’, ‘brown’):(‘Schnauzer’, ‘grey)] - slicing the inner index using tuples. Only use one set of square brackets when slicing (as opposed to subsetting).

dogs.iloc[2:5, 1:4] - unlike .loc the end value is exclusive

### Creating DataFrames
DataFrames can be constructed in two ways:
1.	A list of dictionaries i.e. row by row
a.	list_of_dics = [{‘name’: ‘Karl’, ‘age’: ‘24’}, {‘name’: Jack, ‘age’: ‘26’}]
2.	A dictionary of lists i.e. column by column
a.	dict_of_lists = {‘name’: [‘Karl’, ‘Jack’], ‘age’: [‘24’, ‘26’]}
Option 2 is more efficient

JOINING DATA WITH PANDAS

Inner Joins

Data merging basics

ward_licenses = ward.merge(licenses, on = ‘ward’, suffixes = (‘_ward’, ‘_lic’) - matching the ward table and the licenses table based on values in the ward column in each table. The new table ward_licenses will include column names with suffixes where there are multiple columns with the same name

Code
-	The agg function counts the number of accounts by title
# Merge the licenses and biz_owners table on account
licenses_owners = licenses.merge(biz_owners, on = 'account')

# Group the results by title then count the number of accounts
counted_df = licenses_owners.groupby('title').agg({'account':'count'})

# Sort the counted_df in desending order
sorted_df = counted_df.sort_values('account', ascending = False)

# Use .head() method to print the first few rows of sorted_df
print(sorted_df.head())


Task 10
# Load user_reviews.csv
reviews_df = pd.read_csv('datasets/user_reviews.csv')

# Join the two dataframes
merged_df = pd.merge(apps, reviews_df, on = "App")

# Drop NA values from Sentiment and Review columns
merged_df = merged_df.dropna(subset = ['Sentiment', 'Review'])



Output
                     			account
    title                   
    PRESIDENT           	6259
    SECRETARY          	5205
    SOLE PROPRIETOR     	1658
    OTHER               		1200
    VICE PRESIDENT       	970


Merging multiple DataFrames

Total riders in a month
-	Notice the loc call in the print statement - we filter row data by calling the variable filter_criteria (that specifies specific row data) and then we call the ‘rides’ column. We then sum this column
# Merge the ridership, cal, and stations tables
ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \
                           .merge(stations, on='station_id')

# Create a filter to filter ridership_cal_stations
filter_criteria = ((ridership_cal_stations['month'] == 7)
                 & (ridership_cal_stations['day_type'] == 'Weekday')
                  & (ridership_cal_stations['station_name'] == 'Wilson'))

# Use .loc and the filter to select for rides
print(ridership_cal_stations.loc[filter_criteria,'rides'].sum())


Three table merge
-	Notice the agg function finds the median income by alderman 
# Merge licenses and zip_demo, on zip; and merge the wards on ward
licenses_zip_ward = licenses.merge(zip_demo, on='zip') \
                       .merge(wards, on='ward')

# Print the results by alderman and show median income
print(licenses_zip_ward.groupby('alderman').agg({'income':'median'}))


alderman                            	income                           
    
Ameya Pawar                 	66246.0
Anthony A. Beale            	38206.0
Anthony V. Napolitano       	82226.0
Ariel E. Reyboras           	41307.0
Brendan Reilly             	110215.0
Brian Hopkins               	87143.0
Carlos Ramirez-Rosa         	66246.0


Left Join
If your goal is to enhance or enrich a dataset, then you do not want to lose any of your original data. A left join will do that by returning all of the rows of your left table, while using an inner join may result in lost data if it does not exist in both tables.

movies_financials = movies.merge(financials, on = ‘id’, how = ‘left’) - the default value for how is inner. As such for an inner join we don’t have to specify this argument, but for a left join we must specify the argument. Match the movies and financials DataFrames/tables on id

If those rows in the left table match multiple rows in the right table, then all of those rows will be returned. The returned rows must be equal to if not greater than the left table.

Right Join

tv_movies = movies.merge(tv_genre, how = ‘right’, left_on = ‘id’, right_on = ‘movie_id’) - if we want to merge data on id but the column names, where the id values are held, have different names in each table, then we must specify the arguments left_on and right_on.

Outer Join
Useful to find rows that do not have a match in the other table

how = ‘outer’

Self Join
This merger is an inner join with a table on itself where a row in the table relates to another row in the same table

The how argument is an inner join, therefore no need to specify the argument inner as it is the default argument when calling the merge function

Merging on indexes
Works the same as merging on columns i.e. can inner, self, left or right join. Therefore, if you can merge on indexes there is no need to turn the indexes into columns, just merge on the index.

movies_genres = movies.merge(movies_to_genres, left_on = ‘id’, left_index = True, right_on = ‘movie_id’, right_index = True) - when merging on indexes, if you specify a left_on and right_on argument then you must also specify the left_index and right_index arguments in order to tell the merge function to use the separate indexes

Do sequels earn more?
-	Notice the second block of code is a self-join on an index, which we haven’t seen before. As such we only set the right_index = True and not the left_index
# Merge sequels and financials on index id
sequels_fin = sequels.merge(financials, on='id', how='left')

# Self merge with suffixes as inner join with left on sequel and right on id
orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel',
                            right_on='id', right_index=True,
                            suffixes=('_org','_seq'))

# Add calculation to subtract revenue_org from revenue_seq
orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']

# Select the title_org, title_seq, and diff
titles_diff = orig_seq[['title_org','title_seq','diff']]

# Print the first rows of the sorted titles_diff
print(titles_diff.sort_values('diff',ascending = False).head())

sequels_fin

             		title 		sequel     	budget    		revenue
id                                              
19995        		Avatar   	<NA>  		2.370e+08  		2.788e+09
862       		Toy Story    	863  		3.000e+07  		3.736e+08
863     			Toy Story 2  	10193  	9.000e+07 		4.974e+08
597         		Titanic   	<NA>  		2.000e+08  		1.845e+09
24428  		The Avengers   <NA>  	2.200e+08  		1.520e+09

If we are merging sequels_fin on itself, when setting left_on and right_on - if we look just at Toy Story:
-	Our left table would be the row highlighted above. We want to find a row (which is itself) that has an id of 863, which in our left table is the sequel column. As such we set our left_on = ‘sequel’ and search for a row where id = 863 - we therefore set right_on = ‘id’ i.e. our right side table


Filtering Joins

Semi-join
We want to output only the left table based on a characteristic that exists in both tables.
Using the .isin() method

Steps
1.	Merge the left and right table
2.	Search the left table for the results in the merged table using .isin() creating a Boolean Series
3.	Subset step 2 to output the actual results

Anti-join

indicator = True - adds a column called _merge which outputs both (exists in both tables) or left_only (exists only in the left table). We then filter the _merge column == ‘left_only’ as this excludes rows that exist in both tables i.e. anti-join


Summary of .merge() arguments
on = [‘country’, ‘date’]
how = ‘left’/’right’/’inner’/’outer’
indicator = True adds a column called _merge, useful for anti-joins
left_on = when left and right table are matched on data that have different column names
right_on = when left and right table are matched on data that have different column names
suffixes = (‘_ward’, ‘_lic’)

For indexes
left_index = specify this argument when left_on is used
right_index = specify this argument when right_on is used


Concatenate tables vertically (as opposed to horizontally)
Used to combine tables vertically

pd.concat([df1, df2])

pd.concat() method and set axis = 0 to concatenate vertically. The axis argument is set to 0 by default for the .concat() method and so we don’t need to explicitly write this.

Tables are combined in the order they are passed into the .concat() method.

Tables maintain their original index numbers once combined. If we want to reset the index numbers we pass the argument ignore_index = True into the method.

If we want to set keys for each table (basically a second index with names for each table in the combined table) then we pass the argument keys = [‘key1’, ‘key2’, ‘key3’] into the method. Be sure to set ignore_index = False.

The sort = True argument alphabetically sorts the column names

Vertically concatenate tables with different columns
Set the join = ‘inner’ argument to concatenate only columns that are common to all tables. By default the argument is set as join = ‘outer’ meaning all columns from all tables are included. When you specify the join = ‘inner’ argument the sort = True argument has no effect as the order of the columns will be the same as in the original tables.

Append method
.append() is a simplified .concat() method and supports both the ignore_index and sort arguments but does not support the keys or join arguments (join is always set to outer) 

df1.append(df2)

Verifying Integrity
Duplicated data can skew the data set e.g. impact the mean, alter the line plot etc.

When concatenating or merging tables there may be duplicate values which will create an unintentional one-to-many or many-to-many relationship

Merge
Provide one of the following arguments based on what you expect the output relationship to be:

validate = ‘one_to_one’ - no duplicate keys
validate = ‘one_to_many’ - if there is a duplicate key in the right table
validate = ‘many_to_one’ - if there is a duplicate key in the left table
validate = ‘many_to_many’ - if there is a duplicate key in one of the tables

Python will output/validate whether this is true or not

Concatenating
Pass in the argument verify_integrity = True. This checks for duplicate values in the index and NOT the columns. By default this is set to False i.e. it does not check for duplicate values.


Ordered merge
Useful when working with ordered or time-series data where order matters. The method orders data based on the on = argument. You can still use all the same arguments as .merge()

Use the pd.merge_ordered() method

Difference between .merge() and pd.merge_ordered():
-	df1.mege(df2) vs pd.merge_ordered(df1, df2)
-	Default for .merge() is how = ‘inner’ but for merge_ordered() it is how = ‘outer’

Forward fill
Fills in missing data in the table by filling missing values with the previous value by using the fill_method = ‘ffill’ argument

.corr() function in python calculates the correlation

Be careful with forward fill
“When you merge on date first, the table is sorted by date then country. When forward fill is applied, Sweden's population value in January is used to fill in the missing values for both Australia and Sweden for the remainder of the year. This is not what you want. The fill forward is using unintended data to fill in the missing values. However, when you merge on country first, the table is sorted by country then date, so the forward fill is applied appropriately in this situation.”


merge_asof()
Matches on the nearest key column rather than actual values.
Useful when matching data that does not exactly align (such as matching on timestamps).
N.B. the columns you want to merge “on” must be sorted.

pd.merge_asof(df1, df2, on = “date_time”, suffixes = (“visa”, “ibm”), direction = “forward”)

The default value for the direction argument is ‘backward’.
When the direction argument is set to forward the method selects the first row in the right table whose on key argument is greater than or equal to the left key column. 

Can also set direction = ‘nearest’ for the closest match below or above the on key argument.


.query() method
Alternative to subsetting but more intuitive.
Similar to the statement after the WHERE clause in SQL.
The method accepts an input string to determine what rows are selected

stocks.query(‘nike > 90 and disney < 140’) - use single quotes when using .query()
stocks.query(‘stock == “disney” or (stock == “nike” and close < 90’) - use double quotes when checking text

gdp_pivot.query(‘date >= “1991-01-01”’)


.melt() method
Useful when you have a very wide pivot table that is not easy to work with.
This method changes a pivot table from wide to long format:
-	Wide format is usually easier for humans to read;
-	But long format is more accessible for computers to work with

social_fin.melt(id_vars = [‘financial’, ‘company’], value_vars = [‘2019’,’2018’], var_name = [‘year’], value_name = ‘dollars’) - the id_vars argument specifies the columns we DO NOT want to change. The value_vars argument specifies columns to un-pivot, with any columns not specified being dropped from the dataset. The var_name argument changes the name of the column variable to year and the value_name argument changes the name of the column value to dollars