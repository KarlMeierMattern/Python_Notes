# Reading and writing files  

## Reading and writing CSVs  

	new_dogs = pd.read_csv(‚Äúnew_dogs.csv‚Äù)  

> Imports csv file 

	new_dogs.to_csv(‚Äúnew_dogs_with_bmi.csv‚Äù)  

> Writes to a new csv file  

---

## Introduction and Flat Files  

	file_name = ‚Äòhuck_finn.txt‚Äô  
	file = open(file_name, mode = ‚Äòr‚Äô)  
	text = file.read()  
	file.close()  
	print(text)  

	f = open("savedFile.txt", "w")  
	f.write("Hello there")  
	f.close()  

> If `mode` is set to `w` this means that if the file doesn't already exist, the program will create a new blank file with that file name. However, if it does already exist it will be overwritten with a blank file.  
> We close the file as this data is still in the RAM. Nothing gets saved until we close the file using the `.close()` command.  
> `a` means append (add to the end of the file).  However, if the file doen't exist, then it will crash.  
> `a+` means 'add to the end of the file, or create a new one if it doesn't exist'.  

üëâ Here's the amended code with the change on line 1:

### Using a Context Manager  

	with open (‚Äòhuck_finn.txt‚Äô, ‚Äòr‚Äô) as file:  
		print(file.read())  

> No need to close this file as the context manager closes it automatically

	print(file.closed)  

> Checks whether the file is closed by returning a Boolean

	print(file.readline())  

> Reads the first line, if called again it reads the second line and so on

---  

## Importing flatfiles using Numpy  
- Good to use if all data is numerical.  
- `loadtext()` vs `genfromtext()`.  

	import numpy as np  
	np.loadtext(‚Äòmnist.txt‚Äô, delimiter = ‚Äò,‚Äô)  

### Additional arguments  
`delimiter = ‚Äò\t‚Äô` tab delimited.  
`skiprows = 1` skips the first row i.e. if there is text in it.  
`usecols = [0,2]` selects only the 1st and 3rd columns.  
`dtype = str` imports all the data as strings.  

**Note**: if there are multiple data types in a column i.e. floats and strings, then loadtext() struggles.  

	data = np.genfromtext('titanic.csv', delimiter = ',', names = True, dtype = None)  

> `dtype = None` means the function will figure out what data type each column should be.  
> `names = True` tells us that there is a header.  

Because the data are of different types, data is an object called a structured array. Because numpy arrays have to contain elements that are all the same type, the structured array solves this by being a 1D array, where each element of the array is a row of the flat file imported.  

	data = np.recfromcsv(‚Äòfile.txt‚Äô, delimiter=',')  

> Only need to pass the file name to it because it has the defaults `delimiter=','` and `names=True` in addition to `dtype=None`.

---  

## Importing flat files using Pandas  
- Work with time series data.  
- Great for data analysis, wrangling, pre-processing, modeling and visualising.  

	import pandas as pd  
	df = pd.read_csv(‚Äòfile.csv‚Äô)  

### Additional arguments  
`nrows = 5` imports the first 5 rows into the DataFrame.  
`header = None` tells Python that there are no column headers for the imported data.  
`sep = ‚Äò\t‚Äô` the Pandas version of delimiter.  
`comment = ‚Äò#‚Äô` takes away comments that occur after the # sign.  
`na_values='Nothing'` takes a list of strings to recognize as NA/NaN, in this case the string 'Nothing'.  

	data_array = df.values  

> will convert the DataFrame to a Numpy array.  

---  

## Importing Data from Other File Types  

Pickled files
There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. If you want to be able to import them into Python, you can serialize them. All this means is converting the object into a sequence of bytes, or a bytestream.
‚óè	Native to Python
‚óè	Used to store data structures such as dictionaries

import pickle
with open(‚Äòpickled_fruit.pkl‚Äô, ‚Äòrb‚Äô) as file:
# rb means read-only and binary
	data = pickle.load(file)
print(data)


Excel files

import pandas as pd
file = ‚Äòurban_pop.xlsx‚Äô
data = pd.ExcelFile(file)
print(data.sheet_names)
# this will print the sheet names

Output
[‚Äò1960-1966‚Äô, ‚Äò1967-1970‚Äô]

data.parse(‚Äò1960-1966‚Äô)
# selects the first sheet using the sheet name

OR

data.parse(0)
# selects the first sheet using the sheet index

Additional arguments
data.parse(‚Äò1960-1966‚Äô, usecols=[0], skiprows=[0], names=['Country'])
# usecols[0] includes only the first column, skiprows[0] skips the first row, and names renames the column. All arguments passed need to be of type list.


Importing SAS/Stata files using Pandas

SAS files
Business analytics

import pandas as pd
from sas7bdat import SAS7BDAT
with SAS7BDAT(‚Äòurbanpop.sas7bdat‚Äô) as file:
	df_sas = file.to_data_frame()

Stata files
Academic social sciences research

import pandas as pd
data = pd.read_stata(‚Äòurbanpop.dta‚Äô)

HDF5 files
Hierarchical Data Format version 5.
Storing large quantities of numerical data.

import h5py
file_name = ‚Äòhlc.hdf5‚Äô
data = h5py.File(file_name, ‚Äòr‚Äô)


MATLAB files
Matrix Laboratory. Numerical computing environment.

scipy.io.savemat() - write .mat files

import scipy.io
filename = ‚Äòworkspace.mat‚Äô
scipy.io.loadmat(filename)
# read the file

keys = MATLAB variable names
values = objects assigned to variables



3. Working with Relational Databases in Python

Intro to relational databases
PostgreSQL
MySQL
SQLite

Creating a database engine in Python
We will use the SQLite database.
We then use SQLAlchemy to access the SQLite database

from sqlalchemy import create_engine
engine = create_engine(‚Äòsqlite:///Northwind.sqlite‚Äô)
# sqlite:///Northwind.sqlite' is called the connection string to the SQLite database Northwind.sqlite
# creates an engine for the SQLite database Northwind.sqlite

table_names = engine.table_names()
print(table_names)
# print the table names in the Northwind database


Querying relational databases in Python
SELECT * FROM table_name
# returns all columns of all rows from the table_name table

Workflow:
1.	Import packages and functions
2.	Create the database engine
3.	Connect the engine
4.	Query the database
5.	Save query results to a DataFrame
6.	Close the connection

1. 	import sqlalchemy from create_engine
	import pandas as pd
2.	engine = create_engine(‚Äòsqlite:///Northwind.sqlite‚Äô)
3.	con = engine.connect()
4.	rs = con.execute(‚ÄúSELECT * FROM Orders‚Äù)
	# pass in the relevant SQL query
	# this creates a SQLAlchemy results object which we assign to the variable rs
5.	df = pd.DataFrame(rs.fetchall())
	# converts the results object into a DataFrame
	# fetchall fetches all rows
	df.columns = rs.keys()
	# sets the column names in the DataFrame equal to the keys from the database
6.	con.close()


Using a context manager
Means you don‚Äôt have to close the connection as it closes automatically
Step 3 above becomes:

with engine.connect() as con:
	rs = con.execute(‚ÄúSELECT * FROM Orders‚Äù)
	df = pd.DataFrame(rs.fetchmany(size = 5))
	# imports 5 rows
	df.columns = rs.keys()


Querying relational databases directly with Pandas

Using Pandas, we can distill the above code (from Step 3 onwards) into a single line:

engine = create_engine('sqlite:///Chinook.sqlite')
df = pd.read_sql_query(‚ÄúSELECT * FROM Orders‚Äù, engine)


INTERMEDIATE IMPORTING DATA IN PYTHON

1. Importing data from the Internet
URL = Universal Resource Locator

Importing flat files from the Internet

Example 1

from urllib.request import urlretrieve
import pandas as pd

# Assign url of file: url
url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Save file locally
urlretrieve(url, 'winequality-red.csv')

# Read file into a DataFrame
df = pd.read_csv('winequality-red.csv', sep=';')


Example 2
If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using pandas

import pandas as pd

url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Read file into a DataFrame: df
df = pd.read_csv(url, sep = ';')


Example 3
Importing excel files from the web

import pandas as pd

url = 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'

# Read in all sheets of Excel file
# The output is a dictionary with sheet names as keys and corresponding DataFrames as values
xls = pd.read_excel(url, sheet_name = None)

# Print the sheet names to the shell
print(xls.keys())


HTTP requests to import files from the Web
‚óè	We will focus on web addresses.
‚óè	Two elements that uniquely specify web addresses:
1.	Protocol identifier: http
2.	Resource name: datacamp.com

‚óè	http = hypertext transfer protocol
‚óè	Https is a more secure form of http.
‚óè	Every time you go to a website you send an http GET request to the site
‚óè	urlretrieve() performs a GET request and saves the data locally
‚óè	HTML = hypertext markup language is the standard language for the web

Example 1
from urllib.request import urlopen, Request

url = "https://campus.datacamp.com/courses/1606/4135?ex=2"

# This packages the request
request = Request(url)

# Sends the request and catches the response
response = urlopen(request)

# Extracts the HTML
html = response.read()

# Prints the HTML
print(html)

# Close the response
response.close()

Example 2
‚óè	Using the requests library
‚óè	Quicker alternative to example 1
‚óè	No need to close the connection (unlike urllib)

import requests

url = "http://www.datacamp.com/teach/documentation"

# Packages the request, send the request and catch the response
r = requests.get(url)

# Extract the response
text = r.text

# Print the html
print(text)




Scraping the web with Python
‚óè	HTML is a mix of structured and unstructured data
‚óè	BeautifulSoup: parse and extract structured data from HTML

Example 
import requests
from bs4 import BeautifulSoup

url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response
r = requests.get(url)

# Extracts the response as html
html_doc = r.text

# Create a BeautifulSoup object from the HTML
soup = BeautifulSoup(html_doc)

# Print the title of the webpage
print(soup.title)

# Print the text
print(soup.get_text())

# Prettify the BeautifulSoup object
pretty_soup = soup.prettify()

# Print the response
print(pretty_soup)

# extract the URLs of the hyperlinks from the webpage
# Find all 'a' tags (which define hyperlinks)
# Print the URLs to the shell
for link in soup.find_all('a'):
    print(link.get('href'))



2. Interacting with APIs to import data from the Web

Intro to APIs and JSON
‚óè	APIs are protocols for building and interacting with software applications
‚óè	JSON: JavaScript Object Notation
‚óã	File format for real time server to browser communication

Example

import json
with open("a_movie.json") as json_file:
    json_data = json.load(json_file)

# Print each key-value pair in json_data
for k, v in json_data.items():
    print(k + ': ', v)

APIs
‚óè	Much of the data you get from APIs is packaged as JSONs
‚óè	APIs allow software programs to communicate with each other

URLs
‚óè	http - making an http request
‚óè	www.omdbapi.com - makes a query to the API
‚óè	?t=hackers - query string, which in this case is querying the data for the movie with the title (t) Hackers

Example
import requests

url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'

# Package the request, send the request and catch the response
r = requests.get(url)

# Decode the JSON data into a dictionary: json_data
json_data = r.json()

# Print each key-value pair in json_data
for k in json_data.keys():
    print(k + ': ', json_data[k])



3. Diving deep into the twitter API

Example 1

import tweepy

# Store credentials in relevant variables
consumer_key = "nZ6EA0FxZ293SxGNg8g8aP0HM"
consumer_secret = "fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i"
access_token = "1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy"
access_token_secret = "X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx"

# Create your Stream object with credentials
stream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)

# Filter your Stream variable
stream.filter(["clinton", "trump", "sanders", "cruz"])


Example 2
import json

tweets_data_path = 'tweets.txt'

# Initialize empty list to store tweets
tweets_data = []

# Open connection to file
tweets_file = open(tweets_data_path, mode = "r")

# Read in tweets and store in list
for line in tweets_file:
    tweet = json.loads(line)
    tweets_data.append(tweet)

# Close connection to file
tweets_file.close()

# Print the keys of the first tweet dict
print(tweets_data[0].keys())
