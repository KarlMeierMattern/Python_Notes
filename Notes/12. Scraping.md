# Scraping the web with Python  
- HTML is a mix of structured and unstructured data.  
- BeautifulSoup: parse and extract structured data from HTML.  

## Syntax   

    import requests
    from bs4 import BeautifulSoup
    #package

    url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response
    r = requests.get(url)

# Extracts the response as html
    html_doc = r.text

# Create a BeautifulSoup object from the HTML
    soup = BeautifulSoup(html_doc)

# Print the title of the webpage
    print(soup.title)

# Print the text
    print(soup.get_text())

# Prettify the BeautifulSoup object
    pretty_soup = soup.prettify()

# Print the response
    print(pretty_soup)

# extract the URLs of the hyperlinks from the webpage
# Find all 'a' tags (which define hyperlinks)
# Print the URLs to the shell
    for link in soup.find_all('a'):
        print(link.get('href'))



2. Interacting with APIs to import data from the Web

Intro to APIs and JSON
●	APIs are protocols for building and interacting with software applications
●	JSON: JavaScript Object Notation
○	File format for real time server to browser communication

Example

import json
with open("a_movie.json") as json_file:
    json_data = json.load(json_file)

# Print each key-value pair in json_data
for k, v in json_data.items():
    print(k + ': ', v)

APIs
●	Much of the data you get from APIs is packaged as JSONs
●	APIs allow software programs to communicate with each other

URLs
●	http - making an http request
●	www.omdbapi.com - makes a query to the API
●	?t=hackers - query string, which in this case is querying the data for the movie with the title (t) Hackers

Example
import requests

url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'

# Package the request, send the request and catch the response
r = requests.get(url)

# Decode the JSON data into a dictionary: json_data
json_data = r.json()

# Print each key-value pair in json_data
for k in json_data.keys():
    print(k + ': ', json_data[k])



3. Diving deep into the twitter API

Example 1

import tweepy

# Store credentials in relevant variables
consumer_key = "nZ6EA0FxZ293SxGNg8g8aP0HM"
consumer_secret = "fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i"
access_token = "1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy"
access_token_secret = "X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx"

# Create your Stream object with credentials
stream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)

# Filter your Stream variable
stream.filter(["clinton", "trump", "sanders", "cruz"])


Example 2
import json

tweets_data_path = 'tweets.txt'

# Initialize empty list to store tweets
tweets_data = []

# Open connection to file
tweets_file = open(tweets_data_path, mode = "r")

# Read in tweets and store in list
for line in tweets_file:
    tweet = json.loads(line)
    tweets_data.append(tweet)

# Close connection to file
tweets_file.close()

# Print the keys of the first tweet dict
print(tweets_data[0].keys())
