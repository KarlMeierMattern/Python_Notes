## ***While Loop**  
        while True:  
        print("You are in a corridor, do you go left or right?")  
        direction = input("> ")  
        if direction == "left":  
            print("You have fallen to your death")  
            break  
        elif direction == "right":  
            continue  
        else:  
            print("Ahh! You're a genius, you've won")  
            exit()  
        print("The game is over, you've failed!")  

---

## **Project: The Github History of the Scala Language**  
- When a csv file is imported into python as a DataFrame any date time objects are read as string objects rather than date time objects.  
- It is hard to perform operations on such string data. As such we use the following method to covert the data to a date time object:  
    .to_datetime()  
> converts string date time into python date time object  

### Example  
    pulls['date'] = pd.to_datetime(pulls['date'], utc = True)  

---

### Example  
        %matplotlib inline  
        data['month'] = data['date'].dt.month  
        data['year'] = data['date'].dt.year  
        counts = data.groupby(['year','month']).count()
        counts.plot(kind='bar', figsize = (12,4))
> `data['month']` stores the month.  
> `dt.month` attribute returns the month from the rows in the date column.  
> `data['year']` stores the year.  
> `dt.year` attribute returns the year from the rows in the date column.  
> `groupby` the month and year and count the pull requests.  


### Example (using `groupby`)  
    by_user = data.groupby('user')['pid'].count() - option 1 using .count function  
    by_user = data.groupby('user').agg({'pid':'count'}) - option 2 using .agg method  
> The above two lines of code produce the same result  

### Example (identifying unique files)  
    files = joined_pr.drop_duplicates(subset='file')  
    files = set(joined_pr['file'])  
> The above two lines of code produce the same result.  
> `set()` creates an unordered collection of unique elements.  

### Example (print the top 3 developers)  
    author_counts.nlargest(3, 'file') - prints the 3 largest rows according to values in the “file” column  

### Subsetting on specific columns  
    authors = ['xeno-by', 'soc']  
    by_author = pulls[pulls['user'].isin(authors)]  
    counts = by_author.groupby([by_author['user'], by_author['date'].dt.year]).agg({'pid': 'count'}).reset_index()  
    counts_wide = counts.pivot_table(index='date', columns='user', values='pid', fill_value=0)  
    counts_wide.plot(kind='bar')  
> `pivot_table` converts the table to a wide format.  

---

### **PROJECT: A VISUAL HISTORY OF NOBEL PRIZE WINNERS**

Part 2
# Display the number of prizes won by male and female recipients.
#display(nobel['sex'].value_counts)
display(nobel['sex'].value_counts())
# the display() formal looks much better than print()


Part 3
# Calculating the proportion of USA born winners per decade
nobel['decade'] = (np.floor(nobel['year'] / 10) * 10).astype(int)


Part 4
# setting the size of all plots.
plt.rcParams['figure.figsize'] = [11, 7]


Part 5
# Adding %-formatting to the y-axis
from matplotlib.ticker import PercentFormatter
ax.yaxis.set_major_formatter(PercentFormatter(1.0))


Part 6
# Picking out the first woman to win a Nobel Prize
(nobel[nobel['sex'] == 'Female']).nsmallest(1,'year')


Part 7
# Selecting the laureates that have received 2 or more prizes.
# ... YOUR CODE FOR TASK 5 ...
nobel.groupby('full_name').filter(lambda x: len(x) >= 2)


Part 8
# Converting birth_date from String to datetime
nobel['birth_date'] = pd.to_datetime(nobel['birth_date'])

# Calculating the age of Nobel Prize winners
nobel['age'] = nobel['year'] - nobel['birth_date'].dt.year


ANALYZING POLICE ACTIVITY WITH PANDAS


1. Preparing the data for analysis

1.1 Stanford open policing project dataset
●	.dtypes - attribute for checking data types
●	Dropping columns:
○	.drop(columns = ‘column name’, inplace = True)
●	Dropping rows:
○	.dropna(subset = [‘column/s to subset’], inplace = True)
●	Analyze the sum of missing values in each column:
○	df.isnull().sum()


1.2 Using proper data types
●	DataType impacts what operations you can perform
●	Avoid storing data as strings where possible
●	.astype() - pass the new data type you want as an argument

Note: Dot vs bracket notation - both mean the same thing
●	Dot notation: df.column
○	However, you must use bracket notation on the left side of an assignment statement i.e. to create a new series or overwrite an existing series
●	Bracket notation: df[‘column’] 


1.3 Creating a DateTimeIndex
●	df[‘column’].str.replace(‘string to replace’, ‘replacement string’)
●	pd.to_datetime(‘column’) - converts column to a DateTime object
●	df.set_index(‘column’, inplace = True’) - sets column as the index
○	N.B. - the index is not considered to be one of the DataFrame columns
○	df.index - attribute to examine the data in the index
○	df.columns - attribute to examine the data in the columns
●	df[‘column1’].str.cat(df[‘column2’], sep = ‘ ’) - concatenates column1 and column2, separating the data with a space/empty string



2. Exploring the relationship between gender and policing
●	df[‘column’].value_counts() - counts unique values in a series
○	Best suited for categorical data
●	df[‘column’].value_counts().sum() - known as ‘method chaining’
○	This should always be equal to the number of rows in the DataFrame - provided there is no missing values
●	df[‘column’].value_counts(normalize = True) - calculates the percentage/proportions of the total for each variable


2.1 Do genders commit different violations
●	df[(df[‘column1’] == ‘condition1’) & (df[‘column2’] == ‘condition2’)]
○	Ampersand (&) represents the logical ‘and’ operator
○	Pipe (|) represents the logical ‘or’ operator


2.2 Does gender affect who gets a speeding ticket

Example (filter the DataFrame first and then subset the filtered DateFrame)
# Create a DataFrame of female drivers stopped for speeding
female_and_speeding = ri[(ri[‘driver_gender’] == 'F') & (ri['violation'] == 'Speeding')]

# Compute the stop outcomes for female drivers (as proportions)
print(female_and_speeding['stop_outcome'].value_counts(normalize = True))


2.3 Does gender affect whose vehicle is searched

Example
# Calculate the search rate for each combination of gender and violation
print(ri.groupby(['driver_gender','violation'])['search_conducted'].mean())


2.4 Does gender affect who is frisked during a search
●	df[‘column’].value_counts(dropna=False)
○	.value_counts() excludes missing values by default
○	Specify the argument dropna = False to include the count of missing values
●	df[‘column’].str.contains(‘string to look for’, na = False) - this string method checks whether a defined string is present in each row of a given column
○	Setting na = False means it returns False if not found and True if found 



3. Visual exploratory data analysis

3.1 Does time of day affect arrest rate?

Example
# Calculate the hourly arrest rate, save as hourly_arrest_rate
# remember that the index contains the date and time, so we can call the .hour attribute of index
hourly_arrest_rate = ri.groupby(ri.index.hour).is_arrested.mean()


3.2 Are drug-related stops on the rise?
●	df[‘column’].resample(‘M’) - is the same as groupby month but allows you to groupby time i.e. ‘M’ is month, ‘D’ is days etc.
○	Can be called after .groupby()

Example (calculate mean price by month)
apple.groupby(apple.index.month).price.mean()
VS
apple[‘price’].resample('M').mean()

Example
# Concatenate 'annual_drug_rate' and 'annual_search_rate'
# axis can also be set to 1 to signify columns
# creates a new DateFrame with index date and two columns
annual = pd.concat([annual_drug_rate, annual_search_rate], axis='columns')

# Create subplots from 'annual'
# plots date vs drug rate and date vs search rate
# Setting subplots = True means both plots will share the x-axis but have separate y-axes
annual.plot(subplots=True)
plt.show()

3.3 What violations are caught in each district?
●	pd.crosstab(df[‘column1’], df[‘column2’])
○	Pass two Pandas series, that represent categories, to the crosstab function
○	Outputs a frequency table for how many times each combination of categories appears in the dataset

Example
# Create a stacked bar plot of 'k_zones'
k_zones.plot(kind='bar', stacked =True)
plt.show()

3.4 How long might you be stopped for a violation?

Example
# Create a dictionary that maps strings to integers
mapping = {'0-15 Min':8, '16-30 Min':23, '30+ Min':45}

# Convert the 'stop_duration' strings to integers using the 'mapping'
# mapping the dictionary above to the DataFrame
ri['stop_minutes'] = ri.stop_duration.map(mapping)

Example
# Save the resulting Series as 'stop_length'
stop_length = ri.groupby('violation_raw').stop_minutes.mean()

# Sort 'stop_length' by its values and create a horizontal bar plot
stop_length.sort_values().plot(kind='barh')
plt.show()

4. Analyzing the effect of weather on policing

4.1 Exploring the weather data set

Example
# Create a histogram with 20 bins to visualize 'TDIFF'
weather.TDIFF.plot(kind='hist',bins=20)
plt.show()

4.2 Categorizing the weather

Example
# Changing data type from object(string) to category
cats = ['short', 'medium', 'long']
ri['stop_length'] = ri.stop_length.astype('category', ordered=True, categories=cats)

# Show the memory usage
# memory usage is less for categories than objects
ri.stop_length.memory_usage(deep=True)
3400602

Note how categories are now ordered in the bottom line of code

4.3 Merging the datasets

Example
# Merge 'ri' and 'weather_rating' using a left join
ri_weather = ri.merge(weather_rating, left_on='stop_date', right_on='DATE', how='left')

# Set 'stop_datetime' as the index of 'ri_weather'
ri_weather.set_index('stop_datetime', inplace=True)


4.4 Does weather affect the arrest rate
●	The output of a single .groupby() operation on multiple columns is a Series with a MultiIndex i.e. multi indexed series (as shown in the screenshot)
●	The outer index ‘Violation’ is like the DataFrame rows and the inner index ‘driver_gender’ like the columns
●	You can call search_rate.loc[‘Equipment’,’M’] to select the row and column

●	To convert the multi indexed series into a DataFrame use .unstack()
●	If you want to avoid having to use .groupby() and then an .unstack() to create this DataFrame you can use .pivot_table()
○	Remember that mean is the default aggregation function of a pivot table


