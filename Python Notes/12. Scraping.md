# Scraping the web with Python  
- HTML is a mix of structured and unstructured data.  
- BeautifulSoup: parse and extract structured data from HTML.  

### Syntax   

    import requests  
    from bs4 import BeautifulSoup  

    url = 'https://www.python.org/~guido/'  
    r = requests.get(url)  
    html_doc = r.text  
    soup = BeautifulSoup(html_doc)  
    print(soup.title)  
    print(soup.get_text())  
    pretty_soup = soup.prettify()  
    print(pretty_soup)  

    for link in soup.find_all('a'):  
        print(link.get('href'))  

> Package the request, send the request and catch the response.  
> Extract the response as html.  
> Create a BeautifulSoup object from the HTML.  
> Print the title of the webpage.  
> Print the text.  
> Prettify the BeautifulSoup object.  
> Print the response.  
> extract the URLs of the hyperlinks from the webpage.  
> Find all 'a' tags (which define hyperlinks).  
> Print the URLs to the shell.  

---  

## Interacting with APIs to import data from the Web  
- APIs are protocols for building and interacting with software applications.  
- JSON: JavaScript Object Notation.  
- File format for real time server to browser communication.  

### Syntax  

    import json  
    with open("a_movie.json") as json_file:  
        json_data = json.load(json_file)  

    for k, v in json_data.items():
        print(k + ': ', v)

> Print each key-value pair in json_data.  


## APIs  
- Much of the data you get from APIs is packaged as JSONs.  
- APIs allow software programs to communicate with each other.  

## URLs  
- http - making an http request.  
- www.omdbapi.com - makes a query to the API.  
- ?t=hackers - query string, which in this case is querying the data for the movie with the title (t) Hackers.  

### Example  

    import requests  
    
    url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'  
    r = requests.get(url)  
    json_data = r.json()  

    for k in json_data.keys():  
        print(k + ': ', json_data[k])  

> Package the request, send the request and catch the response.  
> Decode the JSON data into a dictionary: json_data.  
> Print each key-value pair in json_data.  

---  

## Diving deep into the twitter API  

### Example  

    import tweepy  

    consumer_key = "nZ6EA0FxZ293SxGNg8g8aP0HM"  
    consumer_secret = "fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i"  
    access_token = "1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy"  
    access_token_secret = "X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx"  

    stream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)  

    stream.filter(["clinton", "trump", "sanders", "cruz"])  

> Store credentials in relevant variables.  
> Create your Stream object with credentials.  
> Filter your Stream variable.  


### Example   

    import json  

    tweets_data_path = 'tweets.txt'  
    tweets_data = []  
    tweets_file = open(tweets_data_path, mode = "r")  

    for line in tweets_file:  
        tweet = json.loads(line)  
        tweets_data.append(tweet)  

    tweets_file.close()  
    print(tweets_data[0].keys())  

> Initialize empty list to store tweets.  
> Open connection to file.  
> Read in tweets and store in list.  
> Close connection to file.  
> Print the keys of the first tweet dict.  
