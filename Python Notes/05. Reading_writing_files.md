# Reading and writing files  

## Reading a CSV  

	import csv  
	with open("January.csv") as file:  
  		reader = csv.DictReader(file)  

  	for row in reader:   
    	print (", ".join(row))  
		
> adds a comma and space and then joins data.  

## Reading a CSV, selecting a numeric column, and creating a total  

	import csv  
	with open("January.csv") as file:  
		reader = csv.DictReader(file)   

	total = 0  
	for row in reader:   
		print (row["Net Total"])  
		total += float(row["Net Total"])  
	print(f"Total: {total}")  

> To treat the CSV like a dictionary, use the `csv.DictReader()` function.  
> `Net Total` is a column in the CSV file that contains numeric data.  
> Initialise an empty variable called `total` in order to keep a running total.  

## Reading a CSV into a DataFrame  

	new_dogs = pd.read_csv(“new_dogs.csv”)  

> Imports csv file 

	new_dogs.to_csv(“new_dogs_with_bmi.csv”)  

> Writes to a new csv file  

### Additional arguments  
`data = pd.read_csv('file.csv', sep='\t')` - tab separated file  
`data = pd.read_csv('file.csv', delim_whitespace=True)` - space separated file (every time there is a white space this represents a new value)  
`pandas.read_csv('data.txt', header=None, sep=' ')` - used for separating a text file into columns in a DataFrame  
`data = pd.read_csv('file.csv', header=none)` - don't use first row for column names  
`data = pd.read_csv('file.csv', names=['Name1', 'Name2'])` - specify column names  
`data = pd.read_csv('file.csv', na_values=['NA', 99])` - custom missing values (sets NA and 99 to NaN values)  

## JSON files  
`data = pd.read_json('file.json')` - read JSON file as a database  
`data.to_json('outputfile.json')` - write database file to JSON



## Listing files, renaming files, and creating folders  

### Example  
	
	import os 

	print(os.listdir())  

	files = os.listdir()  
	if "quickSave.txt" not in files:  
		print("Error: Quick Save not found.")  

	os.mkdir("Hello")  

	os.rename("myname.txt", "NEW.o")  

> `listdir()` will allow you to list all the files.  
> `os.mkdir()` will create a folder.  
> `os.rename()` takes 2 arguments: the file to rename and the new name.  

---

## Introduction and Flat Files  

	file_name = ‘huck_finn.txt’  
	file = open(file_name, mode = ‘r’)  
	text = file.read()  
	file.close()  
	print(text)  

	f = open("savedFile.txt", "w")  
	f.write("Hello there")  
	f.close()  

> If `mode` is set to `w` this means that if the file doesn't already exist, the program will create a new blank file with that file name. However, if it does already exist it will be overwritten with a blank file.  
> You don't have to specify `mode = "r"`, you can just type `"r"`.  
> We close the file as this data is still in the RAM. Nothing gets saved until we close the file using the `.close()` command.  
> `a` means append (add to the end of the file).  However, if the file doen't exist, then it will crash.  
> `a+` means 'add to the end of the file, or create a new one if it doesn't exist'.  

---  

## Preventing data loss  
- To solve this, we set up the program to load any pre-existing data from the file.    
- Use the `eval()` function.  

### Example  
	myEvents = []  

	f=open("calendar.txt","r")  
	myEvents = eval(f.read())  
	f.close()  

> The program loads any pre-existing data from the file into the myEvents list at the very start of the code.  


## Using a Context Manager  

	with open (‘huck_finn.txt’, ‘r’) as file:  
		print(file.read())  

> No need to close this file as the context manager closes it automatically

	print(file.closed)  

> Checks whether the file is closed by returning a Boolean

	print(file.readline())  

> Reads the first line, if called again it reads the second line and so on

---  

## Importing flatfiles using Numpy  
- Good to use if all data is numerical.  
- `loadtext()` vs `genfromtext()`.  

	import numpy as np  
	np.loadtext(‘mnist.txt’, delimiter = ‘,’)  

### Additional arguments  
`delimiter = ‘\t’` tab delimited.  
`skiprows = 1` skips the first row i.e. if there is text in it.  
`usecols = [0,2]` selects only the 1st and 3rd columns.  
`dtype = str` imports all the data as strings.  

**Note**: if there are multiple data types in a column i.e. floats and strings, then loadtext() struggles.  

	data = np.genfromtext('titanic.csv', delimiter = ',', names = True, dtype = None)  

> `dtype = None` means the function will figure out what data type each column should be.  
> `names = True` tells us that there is a header.  

Because the data are of different types, data is an object called a structured array. Because numpy arrays have to contain elements that are all the same type, the structured array solves this by being a 1D array, where each element of the array is a row of the flat file imported.  

	data = np.recfromcsv(‘file.txt’, delimiter=',')  

> Only need to pass the file name to it because it has the defaults `delimiter=','` and `names=True` in addition to `dtype=None`.

---  

## Importing flat files using Pandas  
- Work with time series data.  
- Great for data analysis, wrangling, pre-processing, modeling and visualising.  

	import pandas as pd  
	df = pd.read_csv(‘file.csv’)  

### Additional arguments  
`nrows = 5` imports the first 5 rows into the DataFrame.  
`header = None` tells Python that there are no column headers for the imported data.  
`sep = ‘\t’` the Pandas version of delimiter.  
`comment = ‘#’` takes away comments that occur after the # sign.  
`na_values='Nothing'` takes a list of strings to recognize as NA/NaN, in this case the string 'Nothing'.  

	data_array = df.values  

> will convert the DataFrame to a Numpy array.  

---  

## Importing Data from Other File Types  

Pickled files
There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. If you want to be able to import them into Python, you can serialize them. All this means is converting the object into a sequence of bytes, or a bytestream.
●	Native to Python
●	Used to store data structures such as dictionaries

import pickle
with open(‘pickled_fruit.pkl’, ‘rb’) as file:
# rb means read-only and binary
	data = pickle.load(file)
print(data)


Excel files

import pandas as pd
file = ‘urban_pop.xlsx’
data = pd.ExcelFile(file)
print(data.sheet_names)
# this will print the sheet names

Output
[‘1960-1966’, ‘1967-1970’]

data.parse(‘1960-1966’)
# selects the first sheet using the sheet name

OR

data.parse(0)
# selects the first sheet using the sheet index

Additional arguments
data.parse(‘1960-1966’, usecols=[0], skiprows=[0], names=['Country'])
# usecols[0] includes only the first column, skiprows[0] skips the first row, and names renames the column. All arguments passed need to be of type list.


Importing SAS/Stata files using Pandas

SAS files
Business analytics

import pandas as pd
from sas7bdat import SAS7BDAT
with SAS7BDAT(‘urbanpop.sas7bdat’) as file:
	df_sas = file.to_data_frame()

Stata files
Academic social sciences research

import pandas as pd
data = pd.read_stata(‘urbanpop.dta’)

HDF5 files
Hierarchical Data Format version 5.
Storing large quantities of numerical data.

import h5py
file_name = ‘hlc.hdf5’
data = h5py.File(file_name, ‘r’)


MATLAB files
Matrix Laboratory. Numerical computing environment.

scipy.io.savemat() - write .mat files

import scipy.io
filename = ‘workspace.mat’
scipy.io.loadmat(filename)
# read the file

keys = MATLAB variable names
values = objects assigned to variables



3. Working with Relational Databases in Python

Intro to relational databases
PostgreSQL
MySQL
SQLite

Creating a database engine in Python
We will use the SQLite database.
We then use SQLAlchemy to access the SQLite database

from sqlalchemy import create_engine
engine = create_engine(‘sqlite:///Northwind.sqlite’)
# sqlite:///Northwind.sqlite' is called the connection string to the SQLite database Northwind.sqlite
# creates an engine for the SQLite database Northwind.sqlite

table_names = engine.table_names()
print(table_names)
# print the table names in the Northwind database


Querying relational databases in Python
SELECT * FROM table_name
# returns all columns of all rows from the table_name table

Workflow:
1.	Import packages and functions
2.	Create the database engine
3.	Connect the engine
4.	Query the database
5.	Save query results to a DataFrame
6.	Close the connection

1. 	import sqlalchemy from create_engine
	import pandas as pd
2.	engine = create_engine(‘sqlite:///Northwind.sqlite’)
3.	con = engine.connect()
4.	rs = con.execute(“SELECT * FROM Orders”)
	# pass in the relevant SQL query
	# this creates a SQLAlchemy results object which we assign to the variable rs
5.	df = pd.DataFrame(rs.fetchall())
	# converts the results object into a DataFrame
	# fetchall fetches all rows
	df.columns = rs.keys()
	# sets the column names in the DataFrame equal to the keys from the database
6.	con.close()


Using a context manager
Means you don’t have to close the connection as it closes automatically
Step 3 above becomes:

with engine.connect() as con:
	rs = con.execute(“SELECT * FROM Orders”)
	df = pd.DataFrame(rs.fetchmany(size = 5))
	# imports 5 rows
	df.columns = rs.keys()


Querying relational databases directly with Pandas

Using Pandas, we can distill the above code (from Step 3 onwards) into a single line:

engine = create_engine('sqlite:///Chinook.sqlite')
df = pd.read_sql_query(“SELECT * FROM Orders”, engine)


INTERMEDIATE IMPORTING DATA IN PYTHON

1. Importing data from the Internet
URL = Universal Resource Locator

Importing flat files from the Internet

Example 1

from urllib.request import urlretrieve
import pandas as pd

# Assign url of file: url
url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Save file locally
urlretrieve(url, 'winequality-red.csv')

# Read file into a DataFrame
df = pd.read_csv('winequality-red.csv', sep=';')


Example 2
If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using pandas

import pandas as pd

url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Read file into a DataFrame: df
df = pd.read_csv(url, sep = ';')


Example 3
Importing excel files from the web

import pandas as pd

url = 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'

# Read in all sheets of Excel file
# The output is a dictionary with sheet names as keys and corresponding DataFrames as values
xls = pd.read_excel(url, sheet_name = None)

# Print the sheet names to the shell
print(xls.keys())


## HTTP requests to import files from the Web  
- We will focus on web addresses.  
- Two elements that uniquely specify web addresses:  
1. Protocol identifier: http  
2. Resource name: datacamp.com  

- http = hypertext transfer protocol.  
- Https is a more secure form of http.  
- Every time you go to a website you send an http GET request to the site.  
- urlretrieve() performs a GET request and saves the data locally.  
- HTML = hypertext markup language is the standard language for the web.  

### Example  

	from urllib.request import urlopen, Request  

	url = "https://campus.datacamp.com/courses/1606/4135?ex=2"  
	request = Request(url)  
	response = urlopen(request)  
	html = response.read()  
	print(html)  
	response.close()  

> This packages the request.  
> Sends the request and catches the response.  
> Extracts the HTML.  
> Prints the HTML.  
> Close the response.  

### Example  
- Using the requests library.  
- Quicker alternative to previous example.  
- No need to close the connection (unlike urllib).  

	import requests  

	url = "http://www.datacamp.com/teach/documentation"  
	r = requests.get(url)  
	text = r.text  
	print(text)  

> Packages the request, send the request and catch the response.  
> Extract the response.  
> Print the html.  