# ML syntax  

    !pip install scikit-learn

## Polynomial transformation  

    from sklearn.preprocessing import PolynomialFeatures

    poly_features = PolynomialFeatures(degree=2, include_bias=False)
    X_train_poly = poly_features.fit_transform(X_train)
    X_test_poly = poly_features.transform(X_test)

## Min-max scaling  

    from sklearn.preprocessing import MinMaxScaler

    scaler = MinMaxScaler()
    X = scaler.fit_transform(X_raw)

## Standard scaling  

    from sklearn.preprocessing import StandardScaler

    ss = StandardScaler()
    X_train = ss.fit_transform(X_train)
    X_test = ss.transform(X_test)

## Label encoding  

    from sklearn.preprocessing import LabelEncoder

    le = LabelEncoder()
    data['Activity'] = le.fit_transform(data.Activity)

## Train, validation, test split  

    from sklearn.model_selection import train_test_split

    X_train, temp_data, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(temp_data, y_temp, test_size=0.5, random_state=42)

## Pipelines  

    pipe = Pipeline([('ss',StandardScaler()),('lr', LinearRegression())])
    pipe.fit(X_train,y_train)
    predicted = pipe.predict(X_test)

## Stratified sampling  

    from sklearn.model_selection import StratifiedShuffleSplit

    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)

    for train_index, test_index in sss.split(X, y):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

## K-fold cross validation  

    s = StandardScaler()
    lr = LinearRegression()
    estimator = Pipeline([("scaler", s), ("regression", lr)])
    kf = KFold(shuffle=True, random_state=72018, n_splits=3)
    predictions = cross_val_predict(estimator, X, y, cv=kf)
    r2_score(y, predictions)

## GridSearchCV  

    from sklearn.model_selection import GridSearchCV
 
    pipe = Pipeline([('polynomial', PolynomialFeatures(include_bias=False,degree=2)), ('model',LinearRegression())])
    param_grid = {
        "polynomial__degree": [1, 2, 3],
        "model__normalize":[True, False]
    }

    search = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=2)
    search.fit(X_train, y_train)
    best = search.best_estimator_
    best_params = search.best_params_
    best.score(X_test,y_test)
    predicted = best.predict(X_test)

## Linear regression  

    from sklearn.linear_model import LinearRegression  
    LR = LinearRegression()  
    LR = LR.fit(X_train, y_train)  
    y_predict = LR.predict(X_test) 

## Lasso regression  

    from sklearn.linear_model import Lasso

    las = Lasso(alpha = 0.1)
    las.fit(X_train, y_train)
    y_pred = las.predict(X_test)

## Ridge regression  

    from sklearn.linear_model import Ridge

    r = Ridge(alpha = 0.001)
    r.fit(X_train, y_train)
    y_pred = r.predict(X_test)

## Logistic regression  

    from sklearn.linear_model import LogisticRegressionCV

    lr = LogisticRegressionCV(max_iter=1000, random_state=0, Cs=10, cv=4, penalty='l1', solver='liblinear')
    lr.fit(X_train, y_train)

## KNN  

    from sklearn.neighbors import KNeighborsClassifier

    knn = KNeighborsClassifier(n_neighbors=3)
    knn = knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)

## Support vector machines  with RBF

    from sklearn.svm import SVC

    SVC_Gaussian = SVC(kernel='rbf', gamma=1, c=1)
    SVC_Gaussian.fit(X, y)

## Decision trees  

    from sklearn.tree import DecisionTreeClassifier

    dt = DecisionTreeClassifier(criterion='gini', max_depth=10, max_features=10, random_state=42)
    dt = dt.fit(X_train, y_train)

    dt.tree_.node_count
    dt.tree_.max_depth

## Bagging  

    from sklearn.ensemble import BaggingClassifier

    Bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion="entropy", max_depth = 4,random_state=2),n_estimators=30,random_state=0,bootstrap=True)
    Bag.fit(X_train,y_train)
    Bag.predict(X_test)


## Random forests  

    from sklearn.ensemble import RandomForestClassifier

    model = RandomForestClassifier(n_estimators=best_n_estimators, max_depth=best_max_depth, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

## Gradient boosting  


## AdaBoost  

    from sklearn.ensemble import AdaBoostClassifier

    model = AdaBoostClassifier(n_estimators=5, random_state=0, learning_rate=1)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

## Voting  

    from sklearn.ensemble import VotingClassifier

    VC = VotingClassifier(estimator_list)
    VC.fit(X_train, y_train)
    y_pred = VC.predict(X_test)

## Stacking  

    from sklearn.ensemble import StackingClassifier

    SC = StackingClassifier(estimator_list, final_estimator=LogisticRegression())
    SC.fit(X_train, y_train)
    y_pred = SC.predict(X_test)

## Classification error metrics  

    from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, precision_score, recall_score

    cf1 = confusion_matrix(y_test, y_pred, normalize='true')
    accuracy_score(y_test, y_pred)
    precision, recall, f_1, _ = precision_recall_fscore_support(y_test, y_pred)
    classification_report(y_test, y_pred)

## XGBoost  

!pip install xgboost
from xgboost import XGBClassifier

model =XGBClassifier(objective='binary:logistic', learning_rate=0.1, n_estimators=5, eval_metric='mlogloss', max_depth=5)
model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)], verbose=True, early_stopping_rounds=5)
