# Pandas DataFrames  

## General  
- Pandas is built on top of Numpy and Matplotlib.  
- Better than Numpy array as DataFrames can contain multiple data types.  
- You can think of DataFrame columns as single-dimension arrays called Series.
-	We can convert a dictionary to a dataframe   

### Creating DataFrames  
DataFrames can be constructed in two ways:  
1.	A list of dictionaries i.e. row by row  
a.	list_of_dics = [{‘name’: ‘Karl’, ‘age’: ‘24’}, {‘name’: Jack, ‘age’: ‘26’}]  
2.	A dictionary of lists i.e. column by column  
a.	dict_of_lists = {‘name’: [‘Karl’, ‘Jack’], ‘age’: [‘24’, ‘26’]}  
Option 2 is more efficient  
    
### Creating a data frame   

**1. First create a list/s**  
years = [2011,2012,2013,2014,2015,2016,2017,2018,2019,2020]  
durations = [103,101,99,100,100,95,95,96,93,90]  

**2. Then create a dictionary of those lists**  
movie_dict = {'years':years,'durations':durations}  

**3. Then convert that dictionary to a DataFrame**  
durations_df = pd.DataFrame(movie_dict)  

**Import and read a csv file into a DataFrame**  

    import pandas as pd  
    brics = pd.read_csv(“brics.csv”, index_col = 0)  
    index_col = 0 shifts the DataFrame a column left  

**Select column with type DataFrame**  

brics[[‘country’]]  
brics.loc[ :, [‘country’, ‘capital’]] - selects the country and capital columns
brice.loc[ :, ‘country’: ‘capital’] - selects columns from country to capital
brics.iloc[ :, [0,1]]

Row access (and all columns):
brics[1:4] - can only get row access through slicing if you don’t use loc
brics.loc[[‘RU’, ‘IN’, ‘CH’]]
brics.iloc[[1]]
brics.iloc[[1,2,3]]

Row and column access:
brics.loc[[‘RU’, ‘IN’, ‘CH’], [‘country’, ‘capital’]]
brics.iloc[[1,2,3], [0,1]]

loc - label based
iloc - integer based location

- To change the index numbers in the far left column:  

    brics = [‘BR’, ‘CH’, ‘DU’]  

---

Filtering Pandas DataFrames

brics[brics[“area”] > 8] - first checks the column “area” for values that exceed 8 and then outputs all the rows where that condition is met

brics[np.logical_and(brics[“area”] > 8, brics[“area”] < 10)] - searches the “area” column for values exceeding 8 and less than 10 and outputs the rows in the DataFrame which meet these conditions

Calling the .iterrows() method when iterating over a Pandas DataFrame:
Used to iterate over a pandas Data frame rows in the form of (index, series) pair. This function iterates over the data frame column, it will return a tuple with the column name and content in the form of a series.   

for lab, row in brics.iterrows():
	print(lab +  “: “ + row[“capital”])

On each iteration this for loop generates two pieces of data: the ‘label’ in the row and then the actual data in the row as a series.

row is a bit of a misnomer as it refers to the row data in the specified column name.

Pandas
.head() - prints first 5 rows of a DataFrame
.tail() - prints last 5 rows of a DataFrame
.info() - displays info about the data such as type and non-null items
.dtypes - returns the data types in the DataFrame
.shape - this attribute displays the number of rows and columns in a DataFrame
.describe() - computes summary stats for columns with numeric values
.values - prints the data in the DataFrame
.columns - prints column names
.index - prints the number of rows

Sorting and subsetting DataFrames

.sort_values(“weight_kg”) - sorts the column “weight-kg” in ascending order
.sort_values(“weight_kg”, ascending = False) - sorts by descending order
.sort_values([“weight_kg”, “height_cm”], ascending = [True, False]) - sorts “weight_kg” by ascending order and then sorts “height_cm” by descending order


dogs[[“breed”, “height_cm”]] - inner brackets create a list of column names to subset and outer brackets subset the DataFrame
dogs[dogs[“date_of_birth”] > “2015 - 10 - 25”] - outputs all rows in the DataFrame which have a “date_of_birth” that exceeds “2015 - 10 - 25”
dogs[dogs[“color”].isin([“Black”, “Brown”])] - outputs all rows which have a “color” equal to black or brown
dogs[“height_m”] = [45,52,61,39] - creates a new column with the heading “height_m” with the data in the list

DATA MANIPULATION WITH PANDAS


Summary statistics in DataFrames

Methods in DataFrames

dogs[“height_cm”].mean() - calculates the mean of the “height_cm” column
.median()
.mode()
.min()
.max()
.var()
.std()
.sum()
.quantile()
.agg(function) - allows you to pass a defined function into the .agg method. This method allows for non-standard methods to be run on DataFrames based on what the function is
.cumsum() - calculates the cumulative sum
.cummax() - shows the maximum value seen so far
.cummin()
.cumprod()


Counting in DataFrames

.drop_duplicates(subset = ‘name’) - drop duplicates in the name column
.drop_duplicates(subset = [‘name’, ‘breed’]) - drop elements where name and breed are duplicated
dogs[‘breed’].value_counts(sort = True, normalize = True) - counts the number of dogs of each type of breed. Normalize argument turns count into proportion of the total.


Grouped summary stats in DataFrames

dogs.groupby(‘color’)[“weight_kg”].mean() - group by color, select the weight column and calculate the mean for each colour group.
dogs.groupby(‘color’)[“weight_kg”].agg([min, max, sum]) - groups data by color and calculates the min, max and sum of the weight for each group.
dogs.groupby([‘color’, ‘breed’])[“weight_kg”].mean() - groups data by color and then by breed (for each colour) and calculates the mean of the weight for each group.



Pivot tables in DataFrames
A pivot table is a DataFrame with sorted indexes

dogs.groupby(‘color’)[“weight_kg”].mean()  
OR
dogs.pivot_table(values = ‘weight_kg’, index = ‘color’) - groups data by color and calculates the mean of the weight for each colour group. By default the pivot table calculates the mean value for each group.

dogs.pivot_table(values = ‘weight_kg’, index = ‘color’, aggfunc = [np.mean, np.median]) - groups data by color and calculates the mean and median weight for each colour group

dogs.groupby([‘color’, ‘breed’])[“weight_kg”].mean() - groups data by color and then by breed (for each colour) and calculates the mean of the weight for each group. 
OR
dogs.pivot_table(values = ‘weight_kg’, index = ‘color’, columns = ‘breed’) - produces the same result although it looks a bit different. The index argument is the column to group by and display in rows. The column argument is the column to group by and display in columns. 

dogs.pivot_table(values = ‘weight_kg’, index = ‘color’, columns = ‘breed’, fill_value = 0, margins = True) - fill_value fills all missing values with 0. Margins calculates the mean for all rows and columns

You can slice pivot tables in the same way you normally would a DataFrame

.mean(axis = ‘index’) - calling the mean function on a pivot table calculates the mean on the index values i.e. by row
.mean(axis = ‘columns’) - calling the mean function on a pivot table calculates the mean on the column values i.e. by column

Explicit indexes
Setting indexes violates the ‘tidy’ data principle of Python as there is separate syntax.
DataFrames are composed of three parts:
1.	Numpy array for data
2.	Index to store row details
3.	Index to store column details

dogs.set_index(‘name’) - sets the ‘name’ column as the index i.e. the far left column. This column is indented to the left, unlike the date in the DataFrame which is indented to the right.
dogs.reset_index(inplace=True) - resets the DataFrame to its original
dogs.reset_index(drop = True) - deletes the data in the index column

dogs[dogs[‘name’].isin([‘Bella’, ‘Stella’])
OR if you subsetted the ‘name’ column and called this new object dogs_ind
dogs_ind.loc[[‘Bella’, ‘Stella’]]

REMEMBER: when subsetting multiple columns you need to use double square brackets because the inner square brackets generate a list and the outer brackets do the subsetting.

dogs.set_index([‘breed’, ‘color’]] - the inner index ‘color’ is nested inside the outer index ‘breed’

dogs.loc[[‘Labrador’,’Chihuahua’]] - subset outer index
dogs.loc[[(‘Labrador’, ‘brown’),(‘Chihuahua’, ‘black’)]] - subset inner index with tuples. The result must match both conditions

dogs.sort_index() - sorts from outer to inner (in ascending order by default)
dogs.sort_index(level = [‘color’, ‘breed’], ascending = [‘True’, ‘False’]) - controlling the sort index

dogs.set_index(‘name’).sort_index() - sets and sorts the index

dogs.loc[(‘Labrador’, ‘brown’):(‘Schnauzer’, ‘grey)] - slicing the inner index using tuples. Only use one set of square brackets when slicing (as opposed to subsetting).

dogs.iloc[2:5, 1:4] - unlike .loc the end value is exclusive



JOINING DATA WITH PANDAS

Inner Joins

Data merging basics

ward_licenses = ward.merge(licenses, on = ‘ward’, suffixes = (‘_ward’, ‘_lic’) - matching the ward table and the licenses table based on values in the ward column in each table. The new table ward_licenses will include column names with suffixes where there are multiple columns with the same name

Code
-	The agg function counts the number of accounts by title
# Merge the licenses and biz_owners table on account
licenses_owners = licenses.merge(biz_owners, on = 'account')

# Group the results by title then count the number of accounts
counted_df = licenses_owners.groupby('title').agg({'account':'count'})

# Sort the counted_df in desending order
sorted_df = counted_df.sort_values('account', ascending = False)

# Use .head() method to print the first few rows of sorted_df
print(sorted_df.head())


Task 10
# Load user_reviews.csv
reviews_df = pd.read_csv('datasets/user_reviews.csv')

# Join the two dataframes
merged_df = pd.merge(apps, reviews_df, on = "App")

# Drop NA values from Sentiment and Review columns
merged_df = merged_df.dropna(subset = ['Sentiment', 'Review'])



Output
                     			account
    title                   
    PRESIDENT           	6259
    SECRETARY          	5205
    SOLE PROPRIETOR     	1658
    OTHER               		1200
    VICE PRESIDENT       	970


Merging multiple DataFrames

Total riders in a month
-	Notice the loc call in the print statement - we filter row data by calling the variable filter_criteria (that specifies specific row data) and then we call the ‘rides’ column. We then sum this column
# Merge the ridership, cal, and stations tables
ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \
                           .merge(stations, on='station_id')

# Create a filter to filter ridership_cal_stations
filter_criteria = ((ridership_cal_stations['month'] == 7)
                 & (ridership_cal_stations['day_type'] == 'Weekday')
                  & (ridership_cal_stations['station_name'] == 'Wilson'))

# Use .loc and the filter to select for rides
print(ridership_cal_stations.loc[filter_criteria,'rides'].sum())


Three table merge
-	Notice the agg function finds the median income by alderman 
# Merge licenses and zip_demo, on zip; and merge the wards on ward
licenses_zip_ward = licenses.merge(zip_demo, on='zip') \
                       .merge(wards, on='ward')

# Print the results by alderman and show median income
print(licenses_zip_ward.groupby('alderman').agg({'income':'median'}))


alderman                            	income                           
    
Ameya Pawar                 	66246.0
Anthony A. Beale            	38206.0
Anthony V. Napolitano       	82226.0
Ariel E. Reyboras           	41307.0
Brendan Reilly             	110215.0
Brian Hopkins               	87143.0
Carlos Ramirez-Rosa         	66246.0


Left Join
If your goal is to enhance or enrich a dataset, then you do not want to lose any of your original data. A left join will do that by returning all of the rows of your left table, while using an inner join may result in lost data if it does not exist in both tables.

movies_financials = movies.merge(financials, on = ‘id’, how = ‘left’) - the default value for how is inner. As such for an inner join we don’t have to specify this argument, but for a left join we must specify the argument. Match the movies and financials DataFrames/tables on id

If those rows in the left table match multiple rows in the right table, then all of those rows will be returned. The returned rows must be equal to if not greater than the left table.

Right Join

tv_movies = movies.merge(tv_genre, how = ‘right’, left_on = ‘id’, right_on = ‘movie_id’) - if we want to merge data on id but the column names, where the id values are held, have different names in each table, then we must specify the arguments left_on and right_on.

Outer Join
Useful to find rows that do not have a match in the other table

how = ‘outer’

Self Join
This merger is an inner join with a table on itself where a row in the table relates to another row in the same table

The how argument is an inner join, therefore no need to specify the argument inner as it is the default argument when calling the merge function

Merging on indexes
Works the same as merging on columns i.e. can inner, self, left or right join. Therefore, if you can merge on indexes there is no need to turn the indexes into columns, just merge on the index.

movies_genres = movies.merge(movies_to_genres, left_on = ‘id’, left_index = True, right_on = ‘movie_id’, right_index = True) - when merging on indexes, if you specify a left_on and right_on argument then you must also specify the left_index and right_index arguments in order to tell the merge function to use the separate indexes

Do sequels earn more?
-	Notice the second block of code is a self-join on an index, which we haven’t seen before. As such we only set the right_index = True and not the left_index
# Merge sequels and financials on index id
sequels_fin = sequels.merge(financials, on='id', how='left')

# Self merge with suffixes as inner join with left on sequel and right on id
orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel',
                            right_on='id', right_index=True,
                            suffixes=('_org','_seq'))

# Add calculation to subtract revenue_org from revenue_seq
orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']

# Select the title_org, title_seq, and diff
titles_diff = orig_seq[['title_org','title_seq','diff']]

# Print the first rows of the sorted titles_diff
print(titles_diff.sort_values('diff',ascending = False).head())

sequels_fin

             		title 		sequel     	budget    		revenue
id                                              
19995        		Avatar   	<NA>  		2.370e+08  		2.788e+09
862       		Toy Story    	863  		3.000e+07  		3.736e+08
863     			Toy Story 2  	10193  	9.000e+07 		4.974e+08
597         		Titanic   	<NA>  		2.000e+08  		1.845e+09
24428  		The Avengers   <NA>  	2.200e+08  		1.520e+09

If we are merging sequels_fin on itself, when setting left_on and right_on - if we look just at Toy Story:
-	Our left table would be the row highlighted above. We want to find a row (which is itself) that has an id of 863, which in our left table is the sequel column. As such we set our left_on = ‘sequel’ and search for a row where id = 863 - we therefore set right_on = ‘id’ i.e. our right side table


Filtering Joins

Semi-join
We want to output only the left table based on a characteristic that exists in both tables.
Using the .isin() method

Steps
1.	Merge the left and right table
2.	Search the left table for the results in the merged table using .isin() creating a Boolean Series
3.	Subset step 2 to output the actual results

Anti-join

indicator = True - adds a column called _merge which outputs both (exists in both tables) or left_only (exists only in the left table). We then filter the _merge column == ‘left_only’ as this excludes rows that exist in both tables i.e. anti-join


Summary of .merge() arguments
on = [‘country’, ‘date’]
how = ‘left’/’right’/’inner’/’outer’
indicator = True adds a column called _merge, useful for anti-joins
left_on = when left and right table are matched on data that have different column names
right_on = when left and right table are matched on data that have different column names
suffixes = (‘_ward’, ‘_lic’)

For indexes
left_index = specify this argument when left_on is used
right_index = specify this argument when right_on is used


Concatenate tables vertically (as opposed to horizontally)
Used to combine tables vertically

pd.concat([df1, df2])

pd.concat() method and set axis = 0 to concatenate vertically. The axis argument is set to 0 by default for the .concat() method and so we don’t need to explicitly write this.

Tables are combined in the order they are passed into the .concat() method.

Tables maintain their original index numbers once combined. If we want to reset the index numbers we pass the argument ignore_index = True into the method.

If we want to set keys for each table (basically a second index with names for each table in the combined table) then we pass the argument keys = [‘key1’, ‘key2’, ‘key3’] into the method. Be sure to set ignore_index = False.

The sort = True argument alphabetically sorts the column names

Vertically concatenate tables with different columns
Set the join = ‘inner’ argument to concatenate only columns that are common to all tables. By default the argument is set as join = ‘outer’ meaning all columns from all tables are included. When you specify the join = ‘inner’ argument the sort = True argument has no effect as the order of the columns will be the same as in the original tables.

Append method
.append() is a simplified .concat() method and supports both the ignore_index and sort arguments but does not support the keys or join arguments (join is always set to outer) 

df1.append(df2)

Verifying Integrity
Duplicated data can skew the data set e.g. impact the mean, alter the line plot etc.

When concatenating or merging tables there may be duplicate values which will create an unintentional one-to-many or many-to-many relationship

Merge
Provide one of the following arguments based on what you expect the output relationship to be:

validate = ‘one_to_one’ - no duplicate keys
validate = ‘one_to_many’ - if there is a duplicate key in the right table
validate = ‘many_to_one’ - if there is a duplicate key in the left table
validate = ‘many_to_many’ - if there is a duplicate key in one of the tables

Python will output/validate whether this is true or not

Concatenating
Pass in the argument verify_integrity = True. This checks for duplicate values in the index and NOT the columns. By default this is set to False i.e. it does not check for duplicate values.


Ordered merge
Useful when working with ordered or time-series data where order matters. The method orders data based on the on = argument. You can still use all the same arguments as .merge()

Use the pd.merge_ordered() method

Difference between .merge() and pd.merge_ordered():
-	df1.mege(df2) vs pd.merge_ordered(df1, df2)
-	Default for .merge() is how = ‘inner’ but for merge_ordered() it is how = ‘outer’

Forward fill
Fills in missing data in the table by filling missing values with the previous value by using the fill_method = ‘ffill’ argument

.corr() function in python calculates the correlation

Be careful with forward fill
“When you merge on date first, the table is sorted by date then country. When forward fill is applied, Sweden's population value in January is used to fill in the missing values for both Australia and Sweden for the remainder of the year. This is not what you want. The fill forward is using unintended data to fill in the missing values. However, when you merge on country first, the table is sorted by country then date, so the forward fill is applied appropriately in this situation.”


merge_asof()
Matches on the nearest key column rather than actual values.
Useful when matching data that does not exactly align (such as matching on timestamps).
N.B. the columns you want to merge “on” must be sorted.

pd.merge_asof(df1, df2, on = “date_time”, suffixes = (“visa”, “ibm”), direction = “forward”)

The default value for the direction argument is ‘backward’.
When the direction argument is set to forward the method selects the first row in the right table whose on key argument is greater than or equal to the left key column. 

Can also set direction = ‘nearest’ for the closest match below or above the on key argument.


.query() method
Alternative to subsetting but more intuitive.
Similar to the statement after the WHERE clause in SQL.
The method accepts an input string to determine what rows are selected

stocks.query(‘nike > 90 and disney < 140’) - use single quotes when using .query()
stocks.query(‘stock == “disney” or (stock == “nike” and close < 90’) - use double quotes when checking text

gdp_pivot.query(‘date >= “1991-01-01”’)


.melt() method
Useful when you have a very wide pivot table that is not easy to work with.
This method changes a pivot table from wide to long format:
-	Wide format is usually easier for humans to read;
-	But long format is more accessible for computers to work with

social_fin.melt(id_vars = [‘financial’, ‘company’], value_vars = [‘2019’,’2018’], var_name = [‘year’], value_name = ‘dollars’) - the id_vars argument specifies the columns we DO NOT want to change. The value_vars argument specifies columns to un-pivot, with any columns not specified being dropped from the dataset. The var_name argument changes the name of the column variable to year and the value_name argument changes the name of the column value to dollars


## CLEANING DATA IN PYTHON

1. Common data problems

Data type constraints

sales[‘revenue’] = sales[‘revenue’].str.strip(‘$’) - takes out the dollar sign from all revenue figures in the revenue column.
sales[‘revenue’] = sales[‘revenue’].astype(‘int’) - converts all data in the revenue column into integers.
assert sales[‘revenue’].dtype == ‘int’ - the assert statement returns nothing if the condition is true i.e. the revenue column contains only integers. Returns an error if it is not met.

Converting to a categorical data type
df[‘marriage status’] = df[‘marriage status’].astype(‘category’) - converts all data to type category. This is useful when we use numbers to describe married vs not married vs divorced, but we don’t want these numbers to behave as integers, but rather as categorical data types. 


Data range constraints (dealing with out of range data)
●	Dropping data
●	Setting custom minimums and maximums to your columns
●	Treat as missing and impute
●	Setting custom value depending on the business scenario

Example 1
movies.drop(movies[movies['avg_rating'] > 5].index, inplace = True) - drops all rows in the avg_rating column with a rating exceeding 5. inplace means all values are dropped in their place and no new column is created.

Example 2
movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5 - the first argument of .loc method finds all rows in the avg_rating column which exceed 5 and the second argument specifies that it is the avg_rating column we are working in. It then sets all rows in this column that exceed 5, to 5.

Example 3
import datetime as dt
user_signups[‘date'] = pd.to_datetime(user_signups[‘date']).dt.date - converting the date column into a Pandas datetime object. dt.date converts the DateTime object into a date i.e. it is now an object (otherwise known as a string).

Example 4
today_date = dt.date.today() - sets today's date to the variable today_date
user_signups.loc[user_signups[‘date'] > today_date, 'date'] = today_date - for all rows in the ‘date’ column that are greater than today’s date, set them to today’s date
assert user_signups[date].max().date() <= today_date - confirms that the maximum date is less than or equal to today’s date. Make sure to chain this with the .date() method to return a date instead of a time stamp.


Note
Data type ‘object’ represents strings in Pandas.


Uniqueness constraints (duplicate values)
●	Use the .duplicated() method to find duplicates in a dataset
○	Returns booleans showing True for duplicate values
■	Note: the entire row of data has to be duplicated for it to return True. This is why the subset argument is useful, as it filters the search.
○	Arguments:
■	subset - list of column names to check for duplication
●	If you don’t use subset then only complete duplicates are dropped
■	keep - whether to keep (‘first’), (‘last’) or all (False) duplicate values
●	I.e. (‘first’) means the first instance of the duplicated data is kept
●	Use .drop_duplicates() to drop the duplicate values
○	Argument:
■	subset - list of column names to check for duplication
■	Keep - whether to keep (‘first’) or (‘last’) duplicate values
■	inplace = True
●	Complete duplicates means the entire row is duplicated. Incomplete means only some information is duplicated

Example 1
# Find duplicates
duplicates = ride_sharing.duplicated(subset = 'ride_id', keep = False)

# Sort your duplicated rides
# subset the ride_sharing DataFrame by the duplicates variable created above
duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')

# Print relevant columns of duplicated_rides
print(duplicated_rides[['ride_id','duration','user_birth_year']])

Output
 

Example 2
# Drop complete duplicates from ride_sharing
ride_dup = ride_sharing.drop_duplicates()

# Create statistics dictionary for aggregation function
statistics = {'user_birth_year': 'min', 'duration': 'mean'}

# Drops incomplete duplicates by grouping by ride_id and computing the aggregation above
# e.g. looking at the output above for ride_id 33 - both rows will be grouped together using the minimum of birth_year (1979 vs 1979) in the combined entry and the average duration ((10+2)/2)
ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()



2. Text and categorical data problems

Predefined finite set of categories
●	In machine learning we often will represent categories with numbers as shown in the examples below:
 

●	However, there may be inconsistencies in categorical data due to:
○	Data entry issues: such as free text vs drop downs
○	Parsing errors
○	Other errors
●	Solutions to treating these problems:
○	Dropping data
○	Remapping categories
○	Inferring categories


Membership constraints

Example
# Sets are used to store multiple items in a single variable.
# Set is one of 4 built-in data types in Python used to store collections of data, the other 3 are List, Tuple, and Dictionary
# The difference method finds all differences between the column ‘blood_type’ in the ‘study_data’ table and the column ‘blood_type’ in the ‘categories’ table
inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])

# Use the isin method to find all all the inconsistent data in the first table ‘study_data’
# This returns a boolean (True for inconsistent rows and False for consistent rows)
inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)

# Subset the DataFrame for the inconsistent rows
inconsistent_data = study_data[inconsistent_rows]
# To drop inconsistent categories and get consistent data only
# Use the tilde symbol (~) while subsetting to get the opposite of inconsistent rows i.e. consistent rows
consistent_data = study_data[~inconsistent_rows]


Categorical variables
●	Inconsistent capitalization: e.g. “Married” vs “married”
○	Use .str.upper() or .str.lower() functions to make columns consistent
●	Leading or trailing spaces: e.g. “Married” vs “ Married”
○	Use .str.strip() method - when given no input it removes all leading and trailing white spaces
●	Note: to count different data use the following methods
○	Use the .value_counts() method on a series
○	Use the .count() method on a DataFrame

Series example
marriage_status = demographics['marriage_status']
marriage_status.value_counts()
 

DataFrame example
marriage_status.groupby('marriage_status').count()
 


Collapsing data into categories
●	Use pd.cut() to group data into categories

Remapping categories
●	Use .replace()

Example (inconsistent capitalization and remapping)
# Lower dest_region column and then replace "eur" with "europe"
airlines['dest_region'] = airlines['dest_region'].str.lower() 

# Pass a dictionary to the replace method ({item to be replaced:replacement item})
airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})

# Remove white spaces from `dest_size`
airlines['dest_size'] = airlines['dest_size'].str.strip()

Example (remapping categories)
# Create ranges for categories
# np.inf is a Numpy function specifying infinity
# This means 0 - 60 = short, 60 - 180 = medium and 180 - infinity = long
label_ranges = [0, 60, 180, np.inf]
label_names = ['short', 'medium', 'long']

# Create wait_type column
# pd.cut disaggregates the data based on the bins that we define in the argument. We thus define the bins by the numeric range provided in label_ranges above and assign each portion of the range to the label_names defined. This new data is placed into a new column called wait_type
airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, 
                               labels = label_names)

# Create mappings and replace
mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', 
            'Thursday': 'weekday', 'Friday': 'weekday', 
            'Saturday': 'weekend', 'Sunday': 'weekend'}

# Create a new column, day_week, which replaces the old mappings (Monday, Tuesday etc.) with the new mapping (weekday, weekend)
airlines['day_week'] = airlines['day'].replace(mappings)


Text data problems
●	Data inconsistency: e.g. “+9671” vs “09671”
●	Fixed length problems: e.g. phone numbers/passwords need to be a certain length
●	Typos:

Example
# Use .str.replace() to replace unwanted characters with empty strings
phones["number"] = phones["number"].str.replace("+", "")

# Use an assert statement to check that the length of the minimum phone numbers is equal to or exceeds 10 digits
assert phones["number"].str.len().min() >= 10 
# Replaces all phone numbers with less than 10 digits with Numpy’s NaN object
phones.loc[phones['number'].str.len() < 10, ‘number’] = np.nan

# Use an assert statement with the .contains method, where the bar pipe ‘|’ means ‘or’ and the .any() method searches for any of those arguments (and will return a True Boolean if our output contains either + or -) As we have omitted these the output should return False.
assert phone['number'].str.contains("+|-").any() == False


Regular expressions
Regular expressions use the backslash character ('\') to indicate special forms or to allow special characters to be used without invoking their special meaning
●	phones['number'] = phones[‘number'].str.replace(r'\D+', '')



3. Advanced data problems

Uniformity
●	E.g. showing temperature as celsius vs fahrenheit or showing weight as kg vs pounds
●	Verifying unit uniformity is imperative
●	Date time inconsistencies such as 01/03/2021 vs 1 March 2021:
○	Use pd.to_datetime() as for most cases can pick up formats automatically and convert to a standard format
○	Datetime formats (refer to dt.strftime below)
■	%d-%m-%Y
■	%m-%d-%Y
■	%c (e.g. December 25 2019)
○	Sometimes fails with unrecognizable formats

Example 1
# Attempt to infer format of each date
# ‘coerce’ returns NA for rows where conversion failed
birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'], infer_datetime_format = True, errors = 'coerce')

Example 2
# dt.strftime accepts a DateTime format of your choice
# This method converts all dates to the format specified
birthdays['Birthday'] = birthdays['Birthday'].dt.strftime("%d-%m-%Y")


Cross field validation
●	The use of multiple fields in a dataset to sanity check data integrity
●	E.g. where you have a total column for each row - summing the individual columns to make sure they do in fact equal to the value in the total column

Example
# axis = 1 indicates that we are summing by row
sum_classes = flights[['economy_class', 'business_class', 'first_class']].sum(axis = 1)

# Find rows in the total_passengers column that equal to the sum_classes variable defined above and set this equal to passenger_equ 
passenger_equ = sum_classes == flights['total_passengers']

# Use the tilde symbol to find inconsistent passenger totals
inconsistent_pass = flights[~passenger_equ]

# Find consistent passenger totals
consistent_pass = flights[passenger_equ]

Example 2
# Store today's date and find ages
today = dt.date.today()
ages_manual = today.year - banking['birth_date'].dt.year

# Find rows where age column == ages_manual
age_equ = banking['age'] == ages_manual

# Store consistent and inconsistent data
consistent_ages = banking[age_equ]
inconsistent_ages = banking[~age_equ]


Completeness
●	I.e. missing data - no variable is stored for a variable in an observation
●	Takes on forms such as NA, NaN, 0, . etc.
●	Use the .isna() method to find rows with missing values - returns True for missing values
●	Chain .isna().sum() to obtain a breakdown of missing values by column


Example
# A useful package for visualising missing data
import missingno as msno

# Use the msno.matrix() function 
banking_sorted = banking.sort_values('age')
msno.matrix(banking_sorted)
plt.show()

 
Missing data types
●	Missing completely at random: no relationship between missing data and other values (i.e. due to human error)
●	Missing at random: relationship between missing data and other observed data (i.e. missing ozone data for high temperatures)
○	I.e. values missing due to another variable
●	Missing not an random: relationship between missingness and its values (missing or non-missing). (i.e. missing temperatures for high temperatures - where a thermometer stops working when it gets very hot)
○	I.e. values missing due to the missingness of the same variable
○	There is a relationship/correlation between the missing variable and another missing variable

Dealing with missing data
●	Dropping data
●	Imputing with statistical measures (mean, median, mode)
●	Using machine learning

Example 1 (dropping rows containing ‘na’ values)
# Use the subset argument to specify which columns to drop missing values
airquality_dropped = airquality.dropna(subset = ['CO2'])

Example 2 (filling missing rows using imputed values - instead of dropping them)
# Calculate the mean values for each row in the CO2 column
co2_mean = airquality['CO2'].mean()

# Assign the mean values to the CO2 column using the fillna method
# fillna takes in a dictionary with columns as keys and imputed values as values
airquality_imputed = airquality.fillna({'CO2': co2_mean})



4. Record linkage

Minimum edit distance
●	Least possible amount of steps needed to get from one string to another
●	Can be done by:
○	Inserting new characters
○	Deleting characters
○	Substituting characters
○	Transposing characters

Example 1
# The fuzzywuzzy package allows us to compare single strings
from fuzzywuzzy import fuzz

# Use WRatio function to compare two strings
fuzz.WRatio(“Reeding”, “Reading)

Output
# Returns a string between 0 - 100 with 100 being an exact match
86



Comparison with arrays
●	Using process.extract
●	The output is a list of tuples where each is formatted like: (closest match, similarity score, index of match)

Example 2
# Import the process module from fuzzywuzzy
from fuzzywuzzy import process

# Define string and array of possible matches
string = "Houston Rockets vs Los Angeles Lakers"
choices = pd.Series(['Rockets vs Lakers', 'Lakers vs Rockets',
                     'Houson vs Los Angeles', 'Heat vs Bulls'])

# Extract is used to compare a string with an area of strings
# Takes in a string, an area of strings, and a number of possible matches to return ranked from highest to lowest
process.extract(string, choices, limit = 2)

Example 3 (collapsing categories with string matching)

 

# For each correct category
for state in categories['state']:

# Find potential matches in states with typos
# Set the limit argument to the length of the survey DataFrame i.e. shape[0] will output just the number of rows
matches = process.extract(state, survey['state'], limit = survey.shape[0])

# For each potential match
for potential_match in matches:

# If high similarity score
# Remember: potential_match[1] is the similarity score for that match
if potential_match[1] >= 80:

# Replace typo with correct category
# Remember: potential_match[0] is the actual match
survey.loc[survey['state'] == potential_match[0], 'state'] = state


Generating pairs (using record linkage)
●	Linking data from different sources regarding the same entity
●	Generate different pairs of potentially matching rows by searching for exact matches or similar strings
○	This can be useful in removing duplicated data between two DataFrames and then merging the unique data in each DataFrame
●	Similar to joins:
○	But unlike joins, record linkage does not require exact matches between different pairs of data, and instead can find close matches using string similarity
○	Effective when there are no common unique keys between the data sources

Example
import recordLinkage

# Create an indexer and object and find possible pairs
indexer = recordlinkage.Index()

# Block pairing on cuisine_type
indexer.block('cuisine_type')

# Generate pairs between the two DataFrames
pairs = indexer.index(restaurants, restaurants_new)

# Create a comparison object
comp_cl = recordlinkage.Compare()

# Find exact matches on city, cuisine_types
comp_cl.exact('city', 'city', label='city')
comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')

# Find similar matches of rest_name
comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) 

# Get potential matches and print
potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)
print(potential_matches)

Output
●	Values being 1 for a match, and 0 for not a match for each pair of rows in your DataFrames
●	To find potential matches, you need to find rows with more than matching value in a column. You can find them with
○	potential_matches[potential_matches.sum(axis = 1) >= n]
 

# Find matches in all three columns
matches = potential_matches[potential_matches.sum(axis = 1) >= 3]


Linking DataFrames
●	We now have row indices between restaurants and new_restaurants that are most likely duplicates
●	We now need to remove duplicate values and append the restaurants DataFrame to the restaurants_new DataFrame 

Example (continued)

# Get values of second column index of matches
# Using index of 1 searches in the restaurants_new data, which is the 2nd column (index of 0 would be the restaurants data)
matching_indices = matches.index.get_level_values(1)

# Subset restaurants_new based on non-duplicate values using the tilde symbol
# All of the values in the 2nd column are data from restaurants_new that has matching data in the restaurants DataFrame - as such we want to remove this duplicated data from restaurants_new as shown below
non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]

# Append non_dup to restaurants
full_restaurants = restaurants.append(non_dup)
print(full_restaurants)