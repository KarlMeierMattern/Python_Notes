# IBM ML  

## Intro to AI and ML  
- *AI*: a program that can sense, reason, act, and adapt.  
- *ML*: algorithms whose performance improve as they are exposed to more data over time.  
- *Deep learning*: subset of ML in which multilayered neural networks learn from vast amounts of data.  

### Machine learning  
- Programs learn from repeatedly seeing data, rather than being explicitly programmed by humans.  
- *Features/Predictor variables*: use to predict the target (explanatory variable).  
- *Target/Response*: category or value to be predicted.  
- *Label*: the value of the target for a single data point.  
- *Example*: an observation or single data point within the data.  
- Feature detection --> feed through classifier algorithm --> predict target  

### Supervised vs Unsupervised learning  
- **Supervised**: has a target column. The goal is to predict the target. E.g. fraud detection.   
- **Unsupervised**: does not have a target column. The goal is to find an underlying structure in the data. E.g. customer segmentation (find similar groupings of customers in order to be targeted).  

### Deep learning  
- Involves using models called neural networks.  
- The model determines the best representation of the original data (as opposed to classical ML where humans do this).  
- Deep neural networks with more layers.  
- TensorFlow is a library built for deep learning.  

### ML workflow    
1. Problem statement: what problem are you trying to solve?  
2. Data collection: what data do you need?  
3. Data exploration & preprocessing: how should you clean your data so your model can use it?  
4. Modelling: build a model to solve the problem  
5. Validation: did you solve the problem?  
6. Decision making and deployment: communicate to stakeholders or put into production.  

---

## Handling missing values, duplicates, and outliers  
- Data issues: duplicates or unnecessary data, inconsistent data types and typos, missing data, outliers, and data source issues.  
- *Remove*  
- *Impute*: with substituted values (such as mean, median etc.)  
- *Mask*: the data by creating a separate category for the   values. This is under the assumption that missing values are indicative of useful information.  

### Approaches to calculating   s  
- **Residuals**: differences between actual and predicted values, represent model failure.  
- *Standardized*: residual divided by standard error.  
- *Deleted*: residual from fitting model on all data excluding current observation. You then compare model predictions with and without this observation to assess the impact.  
- *Studentized*: deleted residuals divided by residual standard error (based on all data, or all data excluding current observation). You compare model predictions with and without this observation to assess the impact on the model. You then standardize the impact according to the range of the model.  

### Dealing with outliers  
- Remove them.  
- Impute them by assigning the mean or median value.  
- Transform the variable. E.g. log transformation to the column containing the outlier/s. Given the new range that observation may no longer be an outlier.  
- Predict what the value should be (using similar observations, or regression).  
- Keep them, but use a model that is resistant to outliers.  
- There are many ways to discover outliers in our data. We can do Uni-variate analysis (using one variable analysis) or Multi-variate analysis (using two or more variables). One of the simplest ways to detect an outlier is to inspect the data visually, by making box plots or scatter plots.  
- Use box plot for univariate analysis.  
- Use scatter plot for bivariate analysis.  

### Statistics  
- The assumption of the normal distribution must be met in order to perform any type of regression analysis.  
- **Skewness** is a measure of asymmetry of the distribution. A longer tail to the right implies positive skew.  
- **Kurtosis** refers to the pointedness of a peak in the distribution curve.  
- Both skewness and kurtosis are frequently used together to characterize the distribution of data.  
- Skewness for a symmetrical bell curve distribution is between -0.5 to 0.5.  
- Moderate skewness is between -0.5 to -1.0 and 0.5 to 1.0.  
- Highly skewed distribution is < -1.0 and > 1.0.  

### Z-score analysis  
- A way to identify outliers mathematically.  
- **Z-score** is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.   
- Data points which are too far from zero will be treated as the outliers.  
- In most of the cases, a threshold of 3 or -3 is used.  
- For example, if the Z-score value is greater than or less than 3 or -3 standard deviations respectively, that data point will be identified as a outlier.  

---

## Variable transformation  
- Models used in ML make assumptions about the data, such as asssuming the data follows a linear relationship.  
- Variables often need to be transformed prior to inclusion in the model.  
- Predictions from linear regression models assume residuals/errors are *normally distributed*.  
- Data transformation can help solve skewness in the data.  
- Three types of transformations:  
1. Feature engineering  
2. Feature encoding  
3. Feature scaling  
4. Discretization (the process of transforming continuous variables into discrete form, by creating bins or intervals).  

### Feature engineering  
- **Log** transformations can be used for feature enginerring. The algorithm will still be a linear regression as we have just transformed the feature/s to log.  
- **Polynomial** features allow us to use the linear model (e.g. if a feature is budget, you can add buget^2 to the linear regression). This can help draw out the underlying relationship.  
- **Box-cox**: generalization of the square root function, whereas the square root function uses the exponent of 0.5, box cox lets its exponent vary so it can find the best one.  
- Useful when the relationship between two variables is, for example, upward-curved rather than linear. Polynomial engineering will allow us to express that non-linear relationship for those features while still using linear regression as our model.  
- **Feature interaction**: describes how features interact with one another. It could make sense to multiply two features together to create a new feature or divide one feature by the other. For example, there may be a higher premium for increasing 'Overall Qual' for houses that were built more recently. If such a premium or a similar effect exists, a feature that multiplies 'Overall Qual' by 'Year Built' can help us capture it.  
- **Feature deviation**: create features that capture where a feature value lies relative to the members of a category it belongs to e.g. how expensive a house is relative to other houses in its neighborhood.  

### Feature encoding (categorical features)  
- **Encoding**: converting non-numeric features to numeric features.  
- Two types:  
1. **Nominal**: unordered (e.g. green, red, blue).  
2. **Ordinal**: ordered (e.g. high, medium, low).  
- Approaches:  
1. *Binary*: converts features to zero or one based on two outcomes (e.g. true or false, male or female).  
2. *One-hot encoding/dummy variables*: uses binary encoding where there are multiple outcomes by splitting each outcome into a separate column, which can then be assessed using binary encoding. This is useful for unordered categoricals, but it creates features that are highly correlated with each other.  
3. *Ordinal encoding*: converting ordered categories to numerical values. Downside is that it implies a set distance between categories, which impacts the model (e.g. low=1, med=2, high=3 - the distance impacts the model). Can revert to using one-hot encoding.  

### Feature scaling (continuous numerical data)  
- **Scaling**: converting the scale of numeric data so they are comparable.  
- Three common ways to get all attributes to have the same scale:  
1. **Min-max scaling** (or normalization) is the simplest: values are shifted and rescaled so they end up ranging from 0 to 1. This is done by subtracting the min value and dividing by the max minus min.  
2. **Standardization/standard scaling/Z-score standardization**: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation, so that the resulting distribution has unit variance (meaning a standard deviation of 1).  
- Scikit-learn library provides `MinMaxScaler` for normalization and `StandardScaler` for standardization needs.  
3. **Robust scaling**: similar to min-max scaling but focuses on interquartile range. This is done by subtracting the median value and dividing by the IQR.  

### Syntax  

    !pip install scikit-learn
    from sklearn.preprocessing import MinMaxScaler

    scaler = MinMaxScaler()
    X = scaler.fit_transform(X_raw)

## Principal Component Analysis (PCA)  
- **Dimentionality reduction** is part of the feature extraction process that combines the existing features to produce more useful ones.  
- The goal is to simplify the data without losing too much information.  
- **PCA** is one of the most popular dimensionality reduction algorithms.  
- First, it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.  
- In this way, a few multidimensional features are merged into one.  
- Use *scikit-learn* library to perform PCA on our data.  
- First we must scale our data using the `StandardScaler()` function.  
- Once the data is scaled, apply the `fit_transform()` function to reduce the dimensionality of the dataset down to two dimensions.  
- The `explained_variance_ratio_` function allows us to see how many dimensions our dataset could be reduced to in order to explain most of the variability between the features.  

---

## Estimation and inference  
- **Estimation** is the application of an algorithm, like taking the average.  
- **Inference** involves putting an accuracy on the estimate (e.g. standard error of an average, or the statistical significance/confidence of an estimate).  
- ML & statistical inference are similar --> using data to infer qualities about the distribution.  

### Parametric vs non-parametric  
- **Statistical model** (of the data) is a set of possible distributions/regressions.  
1. **Parametric model** is a type of statistical model with a finite number of parameters. E.g. normal distribution.  
- *Maximum likelihood estimation (MLE)* is a common way of estimating parameters.  
- *Likelihood function/ratio* takes all data and outputs most likely value for parameters (like mean & standard deviation). Can be used as a test statistic to decide whether to accept or reject the null hypothesis.  
2. **Non-parametric** we make fewer assumptions. We don't assume that the data belongs to a particular distribution. E.g. creating a distribution of the data using a histogram (we don't assume normal or exponential distribution but rather one defined by the data itself).  

### Commonly used distributions  
- **Uniform** every single value is equally likely.  
- **Normal/Gaussian** most likely value is closest to the mean. Based on the *Central Limit Theorem*, which states that the sum or average of many independent random events tends to follow a normal distribution.  
- **Log normal** applying log transformation to the data to create a normal distribution.  
- **Exponential distribution** is a continuous probability distribution with the x-axis expressing distance or time between events. The y-axis is represesnted by the *probability density function (PDF)*.  
- **Poisson distribution** is a discrete probability distribution with the x-axis expressing the number of events occurring in a fixed internal of time or space. The y-axis is represented by the *probability mass function (PMF)*.  
- **Binomial distribution** is a discrete probability distribution of the number of successes of *n* trials where succcess is represented by *p* and failure by *(1-p)*.  
- **Cumulative distribution function (CDF)** derived from the PDF and gives you the probability that a random variable takes on a value less than or equal to a specific value.  
- **Percent point function (PPF)** is the inverse of a CDF and takes a probability and returns the corresponding value of the random variable.  
- **Probability density function (PDF)** provides the likelihood of a continuous random variable taking on different values within a specific range. The area under the curve within a given interval represents the proability of the random variable falling within that interval.  
- **Probability mass function (PMF)** provides the probability that a discrete random variable takes on a specific value. The sum of the probabilities for all possible values of the discrete variable should equal 1.  

### Frequentist vs Bayesian statistics  
- **Frequentist** is concerned with repeated observations in the limit. We assume no prior knowledge of the true frequencies.  
1. Derive the probabilistic property of a procedure.  
2. Apply the probability directly to the observed data.  
- **Queueing theory** is the study of waiting in queues/lines.  
- **Bayesian** describes parameters by probability distributions. A prior distribution, based on the experimenters' belief, is formulated and then updated after seeing the data. This is now a *posterior distribution*.  

---

## Hypothesis testing  
- A hypothesis is a statement about about a population parameter.  
- We create two hypotheses:  
1. Null hypothesis (H0); and  
2. Alternative hypothesis (H1 or HA)  
- We decide which one to call the null hypothesis depending on how the problem is set up.  
- The null is generally a specific value with the altenative being >, or < a certain value.  
- The procedure gives us a rule to decide:  
1. Which values of the test statistic do we accept H0;  
2. Which values of the test statistic do we reject H0 and accept H1.  
- The *likelihood ratio/test statistic* describes which of our null or alternative hypotheses is more likely.  
- You cannot accept the null hypothesis; we can only reject it or fail to reject it.  

### Errors  
- **Type 1 error** (false positive) mistakingly reject a true H0.  
- **Type 2 error** (false negative) fail to reject a false H0.  
- **Rejection region** is the set of values of the test statistic that lead to rejection of H0.  
- **Acceptance region** is the set of valies of the test statistic that lead to acceptance of H0.  
- **Null distribution** is the test statistic's distribution when the null is true.  

### Significance level  
- **Alpha/significance** is a probability threshhold below which the null hypothesis will be rejected. 
- Choose this before calculating the test statistic (usually 0.05 or 0.01).  
- Represents the probability of making a Type I error.  
- A result has statistical significance (p<=alpha) when a result at least as "extreme" would be very infrequent if the null hypothesis were true.  
- Alpha is the threshold for p below which the null hypothesis is rejected even though by assumption it were true, and something else is going on.  
- When you have a low alpha value it means you're requiring stronger evidence before you're willing to reject the null hypothesis (and avoiding a type 1 error).  

### P-value (probability value)  
- **P-value** describes how likely it is that your data would have occurred by random chance (i.e. that the null hypothesis is true and that results aren't statistically significant).  
- The smaller the p-value the less likely the results occurred by random chance, and the stronger the evidence that you should reject the null hypothesis.  
- It is the smallest significance level at which the null hypothesis would be rejected.  
- It is the probability of obtaining a result at least as extreme, given that the null hypothesis is true.  
- If the p-value of your test statistic is lower than your alpha level, you would reject the null hypothesis. This indicates that you have found evidence to suggest that the effect you're investigating is statistically significant.  
- A high p-value (greater than your alpha level) would lead you to fail to reject the null hypothesis. This suggests that you don't have strong enough evidence to conclude that the effect you're studying is statistically significant.  
- **Confidence interval** contains the values of the test statistic for which we accept the null.  
- The p-value indicates the strength of evidence against a specific null hypothesis, while the confidence interval gives you a range of plausible values for a population parameter.  
- A statistically significant result cannot prove that a research hypothesis is correct (which implies 100% certainty). Instead, we may state our results “provide support for” or “give evidence for” our research hypothesis (as there is still a slight probability that the results occurred by chance and the null hypothesis was correct – e.g., less than 5%).  

### Power and sample size  
- Probabilitiy of at least one type 1 error for a 5% significance test is `1-(1-0.05)^#tests`.  
- **Bonferroni correction**: choose a *p-threshold* so that the probability of making a type 1 error is 5%.  
- *P-threshold* = 0.05/(#tests)  
- Sample size can impact the interpretation of p-values. A larger sample size provides more reliable and precise estimates of the population, leading to narrower confidence intervals.  
- With a larger sample, even small differences between groups or effects can become statistically significant, yielding lower p-values. In contrast, smaller sample sizes may not have enough statistical power to detect smaller effects, resulting in higher p-values.  
- Therefore, a larger sample size increases the chances of finding statistically significant results when there is a genuine effect, making the findings more trustworthy and robust.  

---

## Correlation vs causation  
- **Spurious correlation** two variables appear to be correlated when, in reality, they have no causal connection. These apparent correlations are often the result of random chance or the influence of a third variable.  
- **Confounding correlation** observed correlation between two variables is influenced or distorted by a third variable (the confounding variable) that is related to both of the variables being studied.  

---

## Tests  
- A **t-test** is used for testing the *mean* of one population against a standard or comparing the means of two populations if you *do not* know standard deviation of the the population and when you have a limited sample (n < 30). If you know the standard deviation of the populations , you may use a z-test.  
- A **z-test** is used for testing the *mean* of a population versus a standard, or comparing the means of two populations, with *large* (n ≥ 30) samples, whether you know the population standard deviation or not. It is also used for testing the proportion of some characteristic versus a standard proportion, or comparing the proportions of two populations.  
- An **f-test** is used to compare variances between 2 populations. The samples can be any size. It is the basis of ANOVA.  
- **chi-squared** test is used to determine whether there is a statistically significant difference between the expected and the observed frequencies in one or more *categories* of a contingency table. A contingency table is a tabular representation of categorical data. It shows the frequency distribution of the variables.  

---

## Supervised ML  
- A model is a learning algorithm, a small thing that captures a larger thing.  
- A good model omits unimportant details while retaining what's important.  

### Parameters vs hyperparameters  
- **Parameters** are aspects of the model we estimate (fit) using the data (the model learns these parameters).  
- **Hyperparameters** are not explicit components of the model, we tune these parameters ourselves (e.g. degree of the polynomial, learning rate, or the number of hidden layers in a neural network).  

### ML framework  
- Two main types of supervised ML:  
1. **Regression**: y is numeric  
- E.g. stock price, box office revenue etc.  
2. **Classification** y is categorical  
- E.g. customer churn, face recognition, which word comes next etc.  
- **Update rule** using features x and outcome y, choose parameters omega to minimise loss J.  
- The loss function is the difference between true y and our predicted yp.  

### Interpretation and prediction  
- Models face a tradeoff between interpretability and prediction. The model you choose will be defined by your business objective.  
- Interpretation can provide insight into improvements in predication, and vice-versa.  
- **Feature importance**: how important a given feature is to our prediction.  

#### Interpretation  
- The primary objective is to train a model to find insights from the data.  
- This approach uses omega to give insight into a system.  
- We want to find what features are providing the most value in predicting the outcome variable.  
**Omega**: represents the parameters. We find the best parameters by looking at past experience. Parameters define the relationship between the features (x) and the outcome (y).  
**Parameters/weights** represent coefficients relating to features (x) with expected target values.  
- Usually, we can interpret lager coefficients as having more importance on the prediction, but this is not always the case as larger coefficients can correspond to overfiting.  
- Weights are learned during the training process to minimize the error between the predicted values and the true values in the training data.  
- Workflow:  
1. Gather x and y to train a model by finding the omega that gives the best prediction yp (and minimises the loss).  
2. Focus on omega (rather than yp) to generate insights.  
- Optimise for a model with high interpretability rather than prediction score.  
- Example: x=customer_demographics, y=sales_data, examine omega to understand loyalty by segment i.e. what drives sales, as opposed to predicting sales.  
- Example: x=marketing_budget, y=movie_revenue, examine omega to understand marketing effectiveness i.e. how much should we spend on marketing, as opposed to predicting movie revenue.  

#### Prediction  
- The primary objective is to make the best prediction.  
- This approach compares y (true values) and yp (predicted values).  
- Because this approach doesn't focus on interpretability, we risk having a Black-box model where we don't fully understand what is happening (like in deep learning).  
- Example: x=customer_purchase_history, y=customer_churn, focus on predicting customer churn.  
- Example: x=financial_info, y=customer_default, focus on predicting loan default.  

---

## Supervised ML: regression vs classification  
- **Regression**: outcome is continuous (numeric).  
- **Classification**: outcome is categorical.  
- *Supervised learning overview*:  
1. Step 1 (fit): Data with outcomes (training) + model (parameters have not been learnt yet) --> fit model by tuning parameters to optimise predictions  
2. Step 2 (predict): Data without outcomes/unlabelled data (validation) + model (tuned) --> predict outcomes  
- To build a classification model you need:  
1. Features than can be quantified (e.g. using feature encoding);  
2. Labels that are known (for training data); and  
3. Method to measure similarity (between labelled and unlabelled dataset).  

## Supervised ML: Regression  
- **Assumptions of linear regression**:  
1. *Linearity*:  
- The relationship between the independent variables and the dependent variable should be approximately linear.  
2. *Independence of errors*:  
- Residuals should be independent of each other.  
- Errors/residuals are random fluctuations around the true line i.e. the variability in the dependent variable doesn't increase as the value of the independent variable increases.  
3. *Homoscedasticity*:  
- The noise/random disturbance in the relationship between the independent variables and the dependent variable is the same across all values of the independent variable.  
- If the noise is not the same across the values of an independent variable, we call it *heteroscedasticity*.  
4. *Normality of residuals*  
- This assumption can be aided by transforming the y variable.  
- This is less critical if you have a large sample size (Central Limit Theorem can help) but can still be useful for hypothesis testing, confidence intervals, and certain statistical tests.  

### Multicollinearity  
- Occurs when there is a strong correlation between the independent variables.  
- Linear regression or multilinear regression requires independent variables to have little or no similar features.  
- Reduces the interpretability of the model. We can no longer interpret a coefficient on a variable because there is no scenario in which one variable can change without a conditional change in another variable.  
- In linear regression, when you one-hot encode, this means if you don't drop a column there is an infinte number of options for what the intercept and coefficients could be. This is because all columns are completely dependent on one another.  
- Using `heatmap()` function is a good way to identify whether there is *multicollinearity* present or not.  
- The best way to solve for *multicollinearity* is to use the regularization methods like *Ridge* or *Lasso*.  

### Modelling best practice  
1. Use cost function to fit model;  
2. Develop multiple models; and  
3. Compare results and choose the best one.  
- **Cost function**:  
- Cost functions include: MAE (Mean Absolute Error), MSE (Mean Squared Error), and RMSE (Root Mean Squared Error).  
- Line of best fit/least squares regression line is found by *minimizing the sum of squared distance between the true and predicted values*.  
- This line minimises the cost function.  

---

### Syntax  

    from sklearn.linear_model import LinearRegression  
    LR = LinearRegression()  
    LR = LR.fit(X_train, y_train)  
    y_predict = LR.predict(X_test)  

### Three common measures of error for linear regression  
1. Sum of squared Error (SSE)  
- Known as L2 norm/loss.  
- Sum of squared differences between predicted value yb and true y.  
- Used in L2 regularization (Ridge), which helps prevent overfitting by adding the L2 norm of the model weights/parameters to the loss function.  
- *Note*: L1 norm/loss is the sum of the absolute differences between predicted yb and true y. This is used in L1 regularisation (Lasso), which encourages sparsity in ML models by adding the L1 norm of the model weights to the loss function.  
2. Total Sum of Squares (TSS)  
- Sum of squared differences between true y and the mean of true y. Measures total variability in the outcome variable without considering the effects of the independent variables.  
3. Coefficient of Determination (R2)  
- (R2): 1-(SSE/TSS).  
- One minus the unexplained variation divided by the total variation.  
- Measures how well the model explains variation from the mean.  
- Indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  
- We want this to be as close to 1 as possible.  

---

## Data splits and polynomianl regression  
**Overfitting** is when the model is too complex and does well on the training data but not on the test data.  
- Overfitting is simple to deal with, using methods like regularization.  
**Underfitting** is when the model is too simple and performs poorly on the training and testing data sets.  
- To deal with underfitting, we can build a more complex model using methods like polynomial regression.  
- If making a more complex model does not work, this may involve using more data to train the model on or obtaining new features.  
- As this process is complex, it's better to determine if the model can overfit the data first, using methods like polynomial regression to overfit the data to determine if we have an adequate amount of data.  
Best practices to create a model that generalises well and is not overfitted:  
- Splitting data into training and test sets;  
- Using cross validation; and 
- Using polynomial features.  

Recognize the trade off between model complexity and prediction error  

## Training and test splits  
- **Data leakage**: testing data leaking into your training data set.  
- **Training data**: used to fit the model and learn the parameters.  
- **Test data**: used to measure error and performance of the model.  
1. Predict label using the fitted model.  
2. Compare with actual value.  
3. Measure the error.  
- We fit the model to learn the parameters given the dataset.  
- The ShuffleSplit will ensure there is no bias in your outcome variable.  

### Syntax  

    model.fit(X_train, y_train)  
    y_predict = model.predict(X_test)  
    error_metric(y_test, y_predict)  

### Syntax for splitting date between training, validation, and testing data  

    from sklearn.model_selection import train_test_split

    X_train, temp_data, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(temp_data, y_temp, test_size=0.5, random_state=42)

---

## Polynomial regression  
- Use polynomial features to capture nonlinear relationships.  
- The resulting outcome is still using linear regression as the outcome is still a linear combination of the features, even if some of those features are being, for example, squared.  
- By default polynomial regression performs both power (squaring, cubing etc.) and interaction (between features) calculations.  
- Adding polynomial features helps deal with two fundamental problems: prediction, and interpretation.  
- Helps increase complexity of the model, but can result in overfitting.  

### Variants of standard models  
- Trade off between complexity and generalisation (overfitting vs underfitting, bias-variance tradeoff).  
- Other algorithms that help extend your linear models:  
1. Logistic regression;  
2. K-nearest neighbours;  
3. Decision trees;  
4. Support vector machines;  
5. Random forests;  
6. Ensemble methods; and  
7. Deep learning approaches.  
- Many can be used for both regression and classification.  

---

## Data pipelines  
- Simplify the steps of processing the data.  
- Allows us to bypass steps of applying fit_transform to X_train and then transform to X_test.  

### Syntax  

    pipe = Pipeline([('ss',StandardScaler()),('lr', LinearRegression())])
    pipe.fit(X_train,y_train)
    predicted = pipe.predict(X_test)

---

## Cross validation  
- In supervised ML it is common to withhold a portion of the data to test the final model's performance.  
- This model testing is performed on the 'unseen' data, which the model was not trained on.  
- This withholding of a portion of the dataset for testing is called cross-validation.  
- Cross-validation can also be used to select hyper-parameters/fine-tune the model.  
- Cross-validation helps avoid over-fitting; a complex model could repeat the labels of the samples that it has just seen and, therefore, would have a perfect score but would fail to predict anything useful on the 'unseen' data.  
The three most common cross validation approaches are:  
1. K-fold cross validation;  
2. Leave one out cross validation; and  
3. Stratified cross validation.  
Cross validation involves dividing the dataset into 3 parts:  
1. **training set** - is a portion of the data used for training the model;  
2. **validation set** - is a portion of the data used to optimize the hyperparameters of the model; and  
3. **test set** - is a portion of the data used to evaluate the model.  

### Stratified cross validation  
- `StratifiedShuffleSplit` is a cross-validation technique used in machine learning to split a dataset into training and testing sets while preserving the same class distribution in both the training and testing sets.  
- `StratifiedShuffleSplit` is especially useful in scenarios where you want to evaluate your machine learning model's performance multiple times, and you want to ensure that each evaluation (split) maintains the class balance, making the results more robust and representative of the overall dataset.  
- Useful when we have a skewed distribution of outcome variables and we want to maintain the skew.  

    from sklearn.model_selection import StratifiedShuffleSplit

    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)

    for train_index, test_index in sss.split(X, y):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

## Cross validation syntax  
`Scikit Learn` library contains many methods that can perform the splitting of the data into training, testing and validation sets.  
The most popular methods that we covered in this module are:  
- `train_test_split` - creates a single split into train and test sets.  
- `KFold` - creates number of k-fold splits, allowing cross validation.  
- `cross_val_score` - evaluates model's score through cross validation.  
- `cross_val_predict` – produces the out-of-bag prediction for each row.  
- `GridSearchCV` – scans over parameters to select the best hyperparameter set with the best out-of-sample score.  

## K-fold cross validation  
- Split data into multiple pairs of train and test splits and average the error across all pairs.  
- Results in more statistically signifcant results.  
- Cross-validation helps in assessing how well a model will generalize to unseen data.  
- In k-fold cross-validation, the dataset is split into "k" equally-sized parts or folds.  
- Depending on the number of folds you choose to split up the data, the first fold is always the test set and the remaining folds are the training set.  
- Remember the training data may overlap but not the test data.  
- Smaller folds/k can lead to higher bias but lower variance because the model is trained on less data, which may result in underfitting.  
- Larger folds/k can reduce bias but may increase variance because the model is trained on more data, potentially leading to overfitting if the model is too complex.  
- `cross_val_predict` is a function that does K-fold cross validation for us, appropriately fitting and transforming at every step of the way.  

### Syntax (using `cross_val_predict`)  

    X = df.drop('Price', axis=1)
    y = df.Price
    s = StandardScaler()
    lr = LinearRegression()
    estimator = Pipeline([("scaler", s), ("regression", lr)])
    kf = KFold(shuffle=True, random_state=72018, n_splits=3)
    predictions = cross_val_predict(estimator, X, y, cv=kf)
    r2_score(y, predictions)

> `cross_val_predict` splits the data for you for each fold of cross-validation, you do not need to explicitly define X_train and y_train.  
> The `cv` parameter specifies the number of folds or partitions into which the dataset will be divided for cross-validation.  
> `cv` could be set to a number but we set it to kf, the function defined previously, which ensures the data is shuffled.  
> `cross_val_predict` applies the estimator to each of the test sets, makes predictions on the test data for each fold, and then it aggregates the predictions.  

## Hyperparameter tuning  
- Using cross validation (or train-test splits) to determine which hyperparameters that are most likely to generate a model that generalizes well outside of your sample.  

### Syntax (tuning alpha for Lasso regression)  

    scores = []
    alphas = np.geomspace(0.001, 10, 5)
    for alpha in alphas:
        
        las = Lasso(alpha=alpha, max_iter=100000)
        estimator = Pipeline([("polynomial", PolynomialFeatures(degree=3)), ("scaler", s), ("lasso_regression", las)])
        predictions = cross_val_predict(estimator, X, y, cv = kf)
        score = r2_score(y, predictions)
        scores.append(score)

    list(zip(alphas,scores))

    best_estimator = Pipeline([("polynomial", PolynomialFeatures(degree=2)),("scaler", s),("lasso_regression", Lasso(alpha=0.01, max_iter=10000))])
    best_estimator.fit(X, y)
    best_estimator.score(X, y)

    best_estimator.named_steps["lasso_regression"].coef_

## Grid Search CV/Grid Search with cross validation  
To do cross-validation, we used two techniques:  
1. `KFold` and manually create a loop to do cross-validation.  
2. `cross_val_predict` and `score` to get a cross-valiated score in a couple of lines.  
To do hyper-parameter tuning, we see a general pattern:  
1. `cross_val_predict` and `score` in a manually written loop over hyperparemeters, then select the best one.  
GridSearchCV does all of this in one step.  

### Syntax  

    pipe = Pipeline([('polynomial', PolynomialFeatures(include_bias=False,degree=2)),('ss',StandardScaler()), ('model',Ridge(alpha=1))])  

    param_grid = {
        "polynomial__degree": [1,2,3,4],
        "model__alpha":[0.0001,0.001,0.01,0.1,1,10]
    }

    search = GridSearchCV(pipe, param_grid, n_jobs=2)
    search.fit(X_train, y_train)
    pd.DataFrame(search.cv_results_).head()

    print("best_score_: ",search.best_score_)
    print("best_params_: ",search.best_params_)

    best = search.best_estimator_
    predict = best.predict(X_test)
    best.score(X_test, y_test)
    best.fit(X,y)

> The parameters of pipelines can be set by using the name of the key, separated by "__", then the parameter. Here, we look for different polynomial degrees and different values of alpha.  
> We create a `GridSearchCV` object and fit it. The method trains the model and the hyperparameters are selected via exhaustive search over the specified values.  
> We can input the results into a pandas `DataFrame()` as a dictionary with keys as column headers and values as columns and display the results.  
> `best_score_ `: mean cross-validated score of the best_estimator.  
> `best_params_`: parameter setting that gives the best results (alpha and polynomial degree) on the hold-out data.  
> We can find the best model using `best = search.best_estimator_`.  
> We can call `predict()` on the estimator with the best found parameters.  
> We can calculate the R^2 on the test data using `best.score(X_test, y_test)`.  
> We can train our model on the entire data set using `best.fit(X,y)`.  

---

## Parameters (biases & weights)  
- **Bias**: in linear models refers to the intercept term. It represents the expected value of the dependent variable when all predictor variables are set to zero.  
- **Weights**: in linear models represent the coefficients/slopes associated with each feature.  
- In the context of a linear model the intercept (bias) and the coefficients (weights) are the parameters of the linear model. These are estimated during the model training process to find the best-fitting line that minimizes the sum of squared errors.  

## Bias & variance tradeoff  
Three sources of model error:  
1. Bias: being wrong;  
2. Variance: being unstable; and  
3. Irreducible error: unavoidable randomness/intrinsic & unavoidable error.  
To reduce bias and increase model complexity, you can use more features or higher-degree polynomial terms in the model.  
To reduce variance and simplify the model, you can use fewer features or employ regularization techniques like Lasso or Ridge regression.  
**Bias** (Underfitting):  
- When a model has high bias, it means that it is too simple to capture the underlying patterns or relationships in the data.  
- Models with high bias tend to underfit the data, which means they perform poorly both on the training data and unseen data (test data).  
- High bias models are characterized by their inability to learn from the data, leading to poor model performance. This is why they are associated with low model complexity.  
**Variance** (Overfitting):  
- When a model has high variance, it means that it is too complex and can fit the noise in the data as well as the underlying patterns. It essentially memorizes the training data rather than generalizing from it.  
- Models with high variance tend to overfit the data, which means they perform extremely well on the training data but poorly on unseen data (test data). They capture the noise in the data, resulting in poor generalization to new data.  
- Characterised by sensitivity to small changes in input data.  

## Regularisation & model selection  
- To manage complexity & error we can use a simpler model or employ regularisation techniques.  
- We can select the best regularisation strength via cross-validation.  
- It's best practice to scale features before regularisation (i.e using StandardScaler) so penalties aren't impacted by variable scales.  
- **Complexity tradeoff**: variance reduction (less complex model), through regularisation, may outpace the increase in bias, leading to a better fit model.  
- Regularization aims to sacrifice some training performance for better generalization. The key to regularisation is to see how well the model performs on unseen data.  

## Lasso regression (L1) & ridge regression (L2)  
Main aim is to reduce complexity of the model.  
- Model complexity is associated with having too many features (variables) that may not be essential for making accurate predictions.  
- Achieves model simplicity by encouraging the model to reduce the impact of less important features.   
- The biggest difference between ridge and lasso regression is that in ridge regression, while model coefficients can shrink towards zero, they never actually become zero.  
- Due to this, lasso regression can also be used as a `feature selection technique`, since variables with low importance can have coefficients that reach zero and will be removed entirely from the model.  
- Lambda is the hyperparameter that controls the amount of L1/L2 regularization applied to the linear regression model. A larger lambda value leads to stronger regularization.  
- Lambda shrinks the magnitude of all weights/coefficients (via adding a penalty to the loss funtion).  
- As lambda increases coefficients/weights decrease.  
- Penalty term added to cost function: in Ridge regression the weights are squared while in Lasso regression the absolute value of the weigths are used.  
- Increasing lambda for Ridge adds more penalty for larger weights/coefficients because of the squarring of coefficients when calculating the new loss function.  
- We can select the best regularization strength lambda via cross-validation.  
**Feature Selection**:  
- Lasso/Ridge adds a penalty term to the model's loss function based on the absolute/squared values of the model's coefficients.  
- This penalty encourages smaller coefficients, with some becoming exactly zero in Lasso.  
- When a coefficient becomes zero, it means that the corresponding feature is effectively removed from the model.  
**Sparse Models**:  
- Because Lasso can force some coefficients to be exactly zero, it produces sparse models.  
- Sparse models have fewer nonzero coefficients, which means they rely on a smaller subset of features for making predictions.  
**Regularization**:  
- Regularization prevents the model from fitting the training data too closely, which can help prevent overfitting.  
- Overfitting occurs when a model becomes overly complex by fitting noise or random fluctuations in the training data.  
- By constraining the coefficients with the L1/L2 penalty, regularization helps in achieving a simpler model that generalizes better to unseen data.  
**Hyperparameter tuning**:  
- The hyperparameter being tuned is the regularization hyperparameter `lambda` (called alpha in scikit learn).  
- It determines how much the model penalizes the coefficients.  
- This hyperparameter helps control the complexity of the model and performs feature selection (for Lasso) by shrinking some coefficients to zero.  

## Ridge vs Lasso  
- Ridge is more computationally efficient.  
- Lasso is better for interpretability as it can perform feature selection by reducing some weights to zero.  
- Ridge may be better if the target depends on many of the features (as Lasso may reudce these features to zero).  
**Interpretations**:  
- In Analytic View, increasing L2/L1 penalties force coefficients to be smaller, restricting their plausible range.  
- Under the Geometric formulation, the cost function minimum is found at the intersection of the penalty boundry and a contour of the traditional OLS cost function surface.  
- Under the Probabilistic formulation, L2 (Ridge) regularization imposes Gaussian prior on the coefficients, while L1 (Lasso) regularization imposes Laplacian prior.  

## Elastic net  
- Combines Ridge and Lasso.  
- Introduces the alpha (p) hyperparameter (in scikit-learn the parameter is called `l1_ratio`) that determines a weighted average of L1 & L2 penalties.  
- Alpha parameter is set between 0 and 1.  
- Where p=0 the penalty is completely L2 regularization.  
- Where p=1 the penalty is completely L1 regularization.  
- Otherwise, it is a combination of L1 and L2.  
- Unlike the Ridge Regression, Elastic Net finds zero coefficients.  
- In many cases Elastic Net performs better than Lasso, as it includes features that are correlated with one another.  

## Recursive feature elimination (RFE)  
- Provided by SKlearn
- RFE combines a model/estimation approach with a desired number of features.  
- RFE repeatedly applies the model, measures feature importance, and recursively removes less important features.  

---

## Gradient descent  
The goal is to find the optimal set of parameters that lead to the best model performance by iteratively refining the parameter values based on the gradient of the cost function.  
It is an optimization algorithm used to minimize the cost or loss function during the training of a model.  
**Objective**:  
- In ML we often want to find the model parameters (weights and biases) that result in the best performance on a given task.  
- This is typically done by minimizing a cost or loss function, which quantifies how far off our model's predictions are from the actual target values.  
**Gradient**:  
- The gradient of the cost function at a specific point represents the direction and magnitude of the steepest ascent.  
- In other words, it tells us how the cost will change if we make small adjustments to our model parameters.  
**Gradient Descent Steps**:  
1. *Initialization*:  
- We start with an initial guess for the model parameters (weights and biases).  
2. *Compute Gradient*:  
- At each step, we compute the gradient of the cost function with respect to the parameters.  
3. *Update Parameters*:  
- We adjust the parameters in the opposite direction of the gradient to reduce the cost.  
- This is done by multiplying the gradient by a learning rate (a small positive value) and subtracting it from the current parameter values.  
- The learning rate is a hyperparameter that determines the step size in the parameter space. It's essential to choose an appropriate learning rate as too large a value may cause overshooting, and too small a value may result in slow convergence.  
4. *Repeat*:  
- We repeat the process iteratively for a predefined number of iterations or until the cost function converges to a minimum.  
**Convergence**: Gradient descent continues to update the parameters until the cost function reaches a minimum or a predefined stopping criterion is met. The algorithm stops when further adjustments do not significantly reduce the cost.  

> [!IMPORTANT]  
> - Gradient descent calculates the gradient of the relationship between each model parameter and the cost. The parameters are then altered to move down this slope.  
> - The two main sources of error are local minima and instability.  
> 1. Gradient descent is prone to finding local minima: parameter estimates that aren't the best solution, but the gradient is zero.  
> ![image](https://github.com/KarlMeierMattern/Python_Notes/assets/99612323/907cae04-c000-447b-8c0e-9ae99cd268fd)  
> 2. Instability occurs when the step size / learning rate (the amount that each parameter is adjusted by each iteration) is too large. The parameters are adjusted too far on each step, and the model gets worse with each iteration.  
> ![image](https://github.com/KarlMeierMattern/Python_Notes/assets/99612323/e03f3afd-2480-4652-b076-f1e1e361cac3)  
> - Having a slower learning rate can solve this problem, but might also introduce issues. First, slower learning rates can mean training takes a long time, because more steps are required. Second, taking smaller steps makes it more likely that training settles on a local minima.  
> - By contrast, a faster learning rate can make it easier to avoid hitting local minima, because larger steps can skip over local maxima.  
> - The ability to skip over local maxima is crucial to avoid getting stuck in a local minima.  
> ![image](https://github.com/KarlMeierMattern/Python_Notes/assets/99612323/254812df-6065-4daf-a1ff-f54e5341fa1a)  

---

## Supervised ML (classification) Logistic regression  
Logistic regression is a type of regression that models the probability of a sample being in a certain class. Classifaction problems can be treated as regression problems using encoding. Scikit learn classifiers won't accept strings for the prediction column - use `LabelEncoder` to convert labels to integers. Uses Sigmoid function y = 1/(1+e^-x), which creates a decision boundary used to classify data. One way to interpret logistic regression models is by analysing feature coefficients. Although it may not be as effective as the regular linear regression models because the logistic regression model has a sigmoid function, we can still get a sense for the importance or impact of each feature.  
- Binary & multiple logistic regression are the same thing and mean there is a single binary outcome.  
- Multivariate means there are multiple binary outcomes.  
- Multiclass (OvR or multinomial) logistic regression means the dependent variable has more than two categories i.e. not binary.  

> [!Note]  
> One-vs-Rest algorithms train a binary classification function for each class, each calculating the probability that the observation is an example of the target class.  
> Multinomial algorithm, which creates a single function that returns a multi-valued output. The output is a vector (an array of values) that contains the probability distribution for all possible classes.  

    from sklearn.linear_model import LogisticRegressionCV

    lr = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear')
    lr.fit(X_train, y_train)

> The `LogisticRegressionCV` function performs cross-validated logistic regression with L1 (Lasso) or L2 (Ridge) regularization.  
> `Cs` represents the regularization strength for the logistic regression model.  
> For each `C` value, the model will be trained and evaluated on the training data using k-fold cross-validation (specified by the `cv` parameter) to estimate the model's performance.  
> In this case, Cs=10 means that the LogisticRegressionCV function will perform cross-validation with 10 different regularization strengths for L1-penalty logistic regression.  
> The `C` parameter controls the inverse of the regularization strength, where smaller values of `C` correspond to stronger regularization, and larger values of `C` correspond to weaker regularization.  

### Multinomial/multiclass logistic regression vs OvA binary logistic regression  
- Both approaches are used when you have an outcome variable with more than two categories (multiclass/multinomial regression).  
**Multinomial logistic regression**: models all the categories simultaneously, predicting the probabilities for each category directly in a single step.  
- The predicted probabilities sum to 1 for each observation.  
- This is achieved using a softmax function that converts raw scores (logits) into class probabilities.  
- Set `solver = 'lbfgs'` for L2.  
- Set `solver = 'saga'` for L1.  
**One-vs-All/Rest (OvA/R) binary logistic regression**: treats each category as a separate binary classification problem, training multiple binary models (one for each class) and then combining their outputs during prediction.  
- It involves training a separate binary logistic regression model for each class, treating that class as the positive class and the rest as the negative class.  
- During classification, you apply all binary models to the input data, and the class associated with the model that gives the highest probability is predicted.  
- Set `solver='liblinear'` for L1 & L2.  

### Syntax multinomial logistic regression  

    l1_model = LogisticRegression(random_state=0, penalty='l1', multi_class='multinomial', solver='saga', max_iter = 1000)
    l1_model.fit(X_train, y_train)
    l1_preds = l1_model.predict(X_test)

    l1_model.predict_proba(X_test)[0]
    l1_model.predict(X_test)[0]

> We can check the class probability distribution using the `predict_proba` function for the first row in the test dataset.  
> Note: the probabilities for each row will sum to 1.  
> The model prediction for the first row is then based on the class with the highest probability.  

## Classification error metrics  
Accuracy is a useful metric for many classification problems, but in cases where the class distribution is imbalanced or the cost of false positives and false negatives is significantly different, it may be necessary to consider other metrics like precision, recall, F1-score, or the area under the ROC curve, depending on the specific requirements of the task.  

### Classification error metrics (Confusion matrix)    
**Accuracy** measures the overall correctness of a classification model.  
- Accuracy = (TP+TN)/(TP+TN+FP+FN)  
- It is a measure related to predicting correctly positive and negative instances.  
- Doesn't differentiate between FPs and FNs, so it doesn't specifically measure the quality of positive predictions (recall & precision) or the quality of negative predictions (specificity).  
**Recall/sensitivity** measures the quality of positive predictions (specifically, the ability to avoid false negatives).  
- Recall = TP/(TP+FN)  
- Out of all our actual positives, how many did we get correct. Remember that a FN means we predicted negative but it was actually positive.  
**Precision** measures the quality of positive predictions (specifically, the ability to avoid false positives).  
- Precision = TP/(TP+FP)    
* Out of all our positive predictions, how many did we get correct.  
- The closer this value is to 1.0, the better job this model does at identifying only positive instances.  
**Specificity** measures the quality of negative predictions (specifically, the ability to avoid false positives in the negative class).  
- TN/(TN+FP)  
- Out of all our actual negatives, how many did we get correct. Remember that a FP means we predicted positive but it was actually negative.  
**F1 score** attempts to capture tradeoff between precision & recall.  
- 2 * (Precision * Recall)/(Precision + Recall)  

#### Syntax  

    from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score
    le =  LogisticRegression(random_state=1, penalty='elasticnet', multi_class='multinomial', solver='saga', max_iter=1000, l1_ratio=0.1)
    le.fit(X_train, y_train)
    y_pred = le.predict(X_test)
    cf1 = confusion_matrix(y_test, y_pred, normalize='true')
    print(cf1)
    print(accuracy_score(y_test, y_pred))
    precision, recall, f_1, _ = precision_recall_fscore_support(y_test, y_pred)
    print(precision)
    print(recall)
    print(f_1)

> `classification_report(y_test, y_pred)` can also be used to provide preciision, recall, & f-score.  

### Example  
- Assume the positive class refers to the rare, fatal disease class.  
- You can achieve 100% recall by predicting everything to be positive.  
- This approach is often called "sensitivity" or "true positive rate."  
- By predicting everything as positive, you would capture all actual positive cases, which means you would have no false negatives, leading to 100% recall.  
- However, this approach would likely result in a very low precision because you would have a high number of false positives - predicting healthy individuals as diseased.  
- In medical diagnosis, achieving high recall (sensitivity) is important to ensure that you don't miss any diseased individuals, but it should be done without sacrificing precision, as false positives can lead to unnecessary stress and costs for patients.  

### Receiver operating characteristic (ROC)  
- Plots true positive rate vs false positive rate.  
- This is the same as recall vs [1-specificity]  
- False positive rate = FP/(FP+TN)  
* It measures the proportion of actual total negative cases that are incorrectly identified as positive by a classification model.  
**Receiver operating characteristic area under the curve** (ROCAUC):  
- Quantifies the model's ability to distinguish between the positive and negative classes at different classification thresholds.  
- Higher AUC value indicates that the model is better at separating the two classes, meaning it has a higher capacity to correctly classify positive instances as positive and negative instances as negative across a range of threshold values.  
- In other words, a higher AUC suggests that the model is more effective at discriminating between the classes and that it has a better overall discriminative power.  
* AUC = 1: Perfect classifier that can perfectly distinguish between the positive and negative classes at all thresholds.  
* AUC > 0.5: The model is better than random guessing. A larger AUC indicates better performance.  
* AUC = 0.5: The model performs no better than random guessing (equivalent to a diagonal line in the ROC curve).  
* AUC < 0.5: The model performs worse than random guessing, which suggests that the model's predictions are in the wrong direction.  
- ROC is better suited to data with balanced classes.  

### Precision-recall curves  
- Measures tradeoff between precision & recall.  
- Generally better suited to data with imbalanced classes.  

### Encode the target column  

    from sklearn.preprocessing import LabelEncoder

    le = LabelEncoder()
    data['Activity'] = le.fit_transform(data.Activity)

> This is encoding of the Activity target column.  

    le.classes_

> Outputs the classes.  

### Example  
- You are evaluating a binary classifier.  
- There are 50 positive outcomes in the test data, and 100 observations.  
- Using a 50% threshold, the classifier predicts 40 positive outcomes, of which 10 are incorrect.  

* Note: By setting a threshold percentage, you define a cutoff point. If the predicted probability for a data point is greater than or equal to the threshold, it is classified as the positive class; otherwise, it is classified as the negative class. Increasing the threshold reduces false positives as it becomes harder to classify a sample as positive, and thus increases precision. However, it may also increase false negatives (i.e., missing actual positive instances) and thus reduce recall.  

Total positives = TP + FN = 50  
Total negatives = TN + FP = 50  
FP = 10  
TP = 40-10 = 30  
TN = 50 -10 = 40  
FN = 50 - 30 = 20  

Precision = TP/(TP+FP) = 30/(30+10) = 0.75  
Recall = TP/(TP+FN) = 30/(30+20) = 0.6  
F1-score = 2*(Precision * Recall)/(Precision+Recall) = 2*(0.75*0.6)/(0.75+0.6) = 0.6667  

---

## Supervised ML (classification & regression) K-Nearest Neighbours (KNN)  
- The idea behind KNN is that close points in a sample are thought to be similar.  
- A simplified way to interpret KNN is by thinking of the output of this method as a decision boundary which is then used to classify new points.  
- Can be used for classification & regression.  
- `k` is a hyperparameter that we can tune (using the visual 'elbow approach').  
- Choose an odd number for `k` to ensure no tie breaker between classes when you have an even number of classes.  
- When the number of classes is odd, choose a multiple of this number and add 1.  
- Smaller values of `k` lead to overfitting and larger values to underfitting (high bias, low variance).  
* When k is large the model oversimplifies the relationships in the data, resulting in a lack of sensitivity to the underlying patterns & underfitting.  
- Decision boundary depends on our value of `k`.  
- **Elbow approach**: plots the error rate curve as a function of `k` where the low/elbow point is used to choose the right value of `k`. Beyond this point the rate of improvement slows or stops.  

### Distance measurement  
**Euclidean (L2) distance**: actual distance between two points.  
- Use Pythagorean theorem to solve for distance.  
**Manhattan (L1) distance**: represents the absolute value of the distances assuming you cannot move diagonally but must rather move along the x and then the y-axis.  
- *Feature scaling*: features with larger scales will dominate the distance calculations, biasing the algorithm and influencing the determination of the nearest neighbour.  
**Decision boundary**: dividing line in the feature space that separates different classes. It represents the region where the KNN algorithm decides which class a new, unlabeled data point belongs to.  
- The decision boundary is essentially the line (in two dimensions), surface (in three dimensions), or hyperplane (in higher dimensions) that marks the boundary between different classes.  
- Decision boundaries represent a rough estimate of where the boundaries between classes should be based on the available data. Data points near the boundary are in regions of uncertainty where the algorithm may make decisions based on the number of nearest neighbors (parameter `k`).  

### KNN regression  
- The value predicted is the mean value of its neighbours.  
E.g. if you have 20 data points and you set k=20, then a single value is predicted being the mean of all the values.  

### Pros & cons of KNN  
**Pros**  
- Simple to implement as it does not require parameter estimation.  
- Adapts well to new training data.  
- Easy to interpret (by non-technical).  
**Cons**  
- Slow to predict (not train) due to many distance calculations.  
- Does not generate a model (meaning we can't say, for instance, feature 1 or 2 are influencing our outcome).  
- Requires lots of memory.  
- KNN accuracy decreases as number of features increases.  

### KNN vs linear & logistic regression  
- KNN model fitting is fast as just need to store data, with regression we need to search for best parameters.  
- Once the model is fit regression has few parameters and is memory efficient while KNN has many parameters and is memory intensive.  
- Prediction is fast for regression, but slow for KNN.  

### Feature engineering  
**Ordinal_variables** use `OrdinalEncoder()` or `LabelEncoder()` (if columns are already ordered).  
**Binary_variables** use `LabelBinarizer()`.  
**Categorical_variables** use `pd.get_dummies()` or `ColumnTransformer()` or `OneHotEncoder()`.  
`LabelEncoder()` & `LabelBinarizer()` assign integer labels to categories in alphabetical order.  
`OrdinalEncoder()` you must specify the order of the categories to be encoded.  

#### Syntax  

    from sklearn.model_selection import train_test_split
    from sklearn.neighbors import KNeighborsClassifier

    X, y = df.drop(columns='churn_value'), df['churn_value']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

    knn = KNeighborsClassifier(n_neighbors=3)
    knn = knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)

    print(classification_report(y_test, y_pred))
    print('Accuracy score: ', round(accuracy_score(y_test, y_pred), 2))
    print('F1 Score: ', round(f1_score(y_test, y_pred), 2))

---

## Supervised ML (classification & regression) Support vector machines (SVM)  
- **Vector** refers to the data points or observations in the dataset, which are represented as points in a multi-dimensional space.  
- **Support vectors** are the data points that lie closest to the decision boundary (hyperplane) between different classes. These support vectors are crucial in defining the hyperplane.  
- *The goal of SVM*: find the hyperplane that maximizes the distance between the hyperplane and the closest data points (support vectors) from different classes. This optimal hyperplane helps classify new data points into their correct categories.  
- SVMs return labels, whereas logistic regression returns probabilities.  
- Labels are decided by which side of the decision boundary they fall on.  
- **Vector beta/coefficients**: represents the vector orthogonal/perpendicular to the hyperplane.   
- SVM models are linear.  
- Scaling data is important as distance between classes matters as we want to find the largest distance between support vectors.  

### Loss function  
- **Hinge loss**: We incur a penalty for being inside the margin between the two support vectors.  
- The cost function only penalises those labels on the wrong side of the decision boundary whereas logistic regression penalises all outcomes.  
- When comparing logistic regression and SVMs, one of the main differences is that the cost function for logistic regression has a cost function that decreases to zero, but rarely reaches zero. SVMs use the Hinge Loss function as a cost function to penalize misclassification. This tends to lead to better accuracy at the cost of having less sensitivity on the predicted probabilities.  

### Regularisation  
- If we strictly adhere to the goal of SVM we may overfit our data.  
- Regularisation adds a term to the cost function to minimise the size of the coefficients by reducing model complexity.  
- This attempts to keep the decision boundary as simple as possible to avoid overfitting to the data.  
- The cost function makes it more costly to try to fit a decision boundary that classifies all data points exactly correct i.e. outliers should not be fitted, but rather accepted as misclassified.  
- The hyperparameter being tuned is `C`. A lower `C` means higher regularisation & a simpler model.  
- The `C` argument is a regularization parameter that controls the trade-off between achieving a low training error and keeping the decision boundary as simple as possible.  
- For large values of `C`, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly, which may cause the model to overfit.  
- Conversely, a very small value of `C` will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points, but potentially better generalization to unseen data.  

### Regression vs classification  
- SVM can also be used for regression.  
- Must ensure that the outcome variable is continuous.  

### SVM kernels  
- Used to achieve non-linear decision boundaries using SVMs.  
- Any linear model can be turned into a non-linear model by applying a kernel to the model.  
- Non-linear data can be made separable by mapping it to a higher-dimensional space, where it might become linearly separable.  
- **Kernel trick**: map a non-linear decision boundary to a linear hyperplane in higher dimensional space.  

### Kernel  
The `kernel` argument specifies the kernel to be used for transforming features to higher-dimensional spaces, some commonly used non-linear kernels are:  
- `rbf`: Gaussian Radial Basis Function (RBF).  
- `poly`: Polynomial Kernel.  
- `sigmoid`: Sigmoid Kernel.  

### SVM Gaussian kernel  
**Approach 1**: create higher order features (such as polynomial features).  
**Approach 2**: transform the space to a different coordinate system. This is done by defining similarity functions and using these functions to transform our space to higher dimensions.  
- We move from our original two dimension feature space to higher dimension space using these similarity functions for each row in our data set.  
**Radial basis function (RBF) kernel**: is a function that takes the distance between two points as input and outputs a similarity score.  
- Used to transform the input space into a higher-dimensional space. This transformation allows SVMs to find a non-linear decision boundary in the higher-dimensional space, effectively separating classes that might not be linearly separable in the original feature space.  
*Hyperparameters*: higher `gamma` & `C` results in less regularisation (overfitting).  
- The kernel type also represents a hyperparameter.  
- Use `GridSearchCV` to find the optimal hyperparameters.  

### SVM workflow  
- SVMs with RBF kernels are very slow to train when we have lots of data as we have to apply the kernel to every point.  
- We can however approximate the kernel map using Nystroem or RBF sampler.  
- This will map the data into higher dimensions (using `fit_transform(X_train)` and `transform(X_test)`).  
- We can then fit a linear classifier (e.g. LinearSVC).  
- We can tune the kernel and associated parameters using cross-validation.  
- Ideally you want to use a pipeline that incorporates both the transformation step (Nystroem/RBF sampler) & the linear classifier/model.  

#### Plain linear classifier  

    from sklearn.svm import LinearSVC

    LSVC = LinearSVC()
    LSVC.fit(X, y)

#### Linear SVC  

    from sklearn.svm import SVC

    linear_svm = SVC(C=1000, kernel='linear')
    linear_svm.fit(X, y)

#### SVC with RBF  

    from sklearn.svm import SVC

    SVC_Gaussian = SVC(kernel='rbf', gamma=1, c=1)
    SVC_Gaussian.fit(X, y)


#### SVC with kernel approximation  
- Should be used when the dataset is large.  
- Nystroem maps to higher dimensions.  
- SGD represents our linear classifier.  

    from sklearn.kernel_approximation import Nystroem
    from sklearn.linear_model import SGDClassifier

    nystroem = Nystroem(kernel='rbf')
    sgd = SGDClassifier()

    X_transformed = nystroem.fit_transform(X)
    sgd.fit(X_transformed, y)

## Nystroem vs polynomial transformation  
- While both methods involve transforming data to a higher-dimensional space, the key distinction lies in how they achieve this transformation and the nature of the resulting feature space.  
- In the case of polynomial transformation, you explicitly add new features to the dataset, leading to a higher-dimensional space.  
- In contrast, the Nystroem method provides an approximation of the higher-dimensional space by using a subset of data points. This can be computationally more efficient, especially when dealing with large datasets or complex kernel functions.  

### GridSearchCV  

    from sklearn.svm import SVC
    from sklearn.model_selection import GridSearchCV

    params_grid = {
        'C': [1, 10, 100],
        'kernel': ['poly', 'rbf', 'sigmoid']
    }

    model = SVC()
    grid_search = GridSearchCV(estimator = model, param_grid = params_grid, scoring='f1', cv = 5, verbose = 1)
    grid_search.fit(X_train, y_train.values.ravel())
    best_params = grid_search.best_params_
    best_params

> We chose to optimise `GridSearchCV` to find the highest `f1` score.  
> The output will be a `C` value and a type of `kernel`.  
> We can use these outputs to train and fit a new model.  

### Syntax for evaluating metrics  

    def evaluate_metrics(y_test, y_preds):
        results_pos = {}
        results_pos['accuracy'] = accuracy_score(yt, yp)
        precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, average='binary')
        results_pos['recall'] = recall
        results_pos['precision'] = precision
        results_pos['f1score'] = f_beta
        return results_pos

### Alternative syntax for evaluating metrics  

    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

    accuracy_score(y_true, y_pred)
    precision_score(y_true, y_pred)
    recall_score(y_true, y_pred)
    f1_score(y_true, y_pred)

---

## Supervised ML (classification & regression) Decision trees  
**Nodes**: represent questions based on our features.  
- Each node has a binary split which answers a Yes/No or True/False question.  
- The top node is called the parent and the other nodes are called the children/leaves.  
**Leaves**: represent decisions.  
- Trees that predict categorical results are called decision trees.  
- Trees that predict numerical results are called regression trees.  
- Values at the leaves for regression trees are the averages of the subsets.  
- Decisions at the leaves for decision trees are the class that is most represented.  
- Increasing the depth of the tree allows for more accurate predictions, but can lead to overfitting.  

### Building a decision tree for classification  
- Select a feature and split data into a binary tree.  
- Continue splitting with available features.  
- Continue splitting until leaf nodes are pure (i.e. only one class remains in that subset) - likely results in overfitting.  
- Or continue splitting until a max depth is reached. We can then prune/cut-off some of the leaves - used to avoid overfitting.  
- Or continue until a predefined performance metric (like accuracy) is achieved - used to avoid overfitting.  

### Finding the correct splits  
- Use greedy search to find the best splits.  
- The best split is one that maximises information gain from the split.  

### Error metric (entropy)  
- Decision trees split your data using impurity measures. They are a greedy algorithm and are not based on statistical assumptions.  
- Te most common splitting impurity measures are entropy and Gini index.  
- We have information gain when using entropy.  
- Splitting based on entropy, as opposed to the standard classification error, allows further splitting to occur.  
- Where two splits have equal information gain the algorithm randomly chooses.  

### Gini index  
- Similar to entropy that allows perfect splits.  
- G(t) = [1 - sum(accuracy)^2]  
- Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the set.  
- The Gini impurity for a particular node in the decision tree is calculated based on the distribution of class labels among the data points in that node.  
- If a leaf node has a Gini impurity of zero, it means that all the data points in that leaf node belong to the same class. In other words, the leaf is "pure" in terms of the target variable, and all the instances in that leaf node have the same class label.  

### How splits are chosen  
- Decision tree classifiers compute the value assigned to a leaf by calculating the ratio: number of observations of one class divided by the number of observations in that leaf E.g. number of customers that are younger than 50 years old divided by the total number of customers.  
- For regression decision trees leaf values are calculated as the average value of the predicted variable.  

### Decision tree pros & cons  
- Decision trees tend to have high variance (overfitting).  
* Can use bagging to counter this.  
- Small changes in data greatly affect prediction (high variance).  
- Solution is to prune trees.  
- You can prune based on depth, classification error, or information gain threshold.  
- Very easy to interpret and implement.  
- Handles all data types (binary, ordinal, continuous).  
- No preprocessing or scaling required as it does not effect splits.  

### Hyperparameters  
- `criterion`: gini or entropy, which specifies which criteria to be used when splitting a tree node.  
- `max_depth`: a numeric value to specify the max depth of the tree. Larger tree depth normally means larger model complexity.  
- `min_samples_leaf`: minimal number of samples in leaf nodes. Larger samples in leaf nodes will tend to generate simpler trees.  
- `max_features` is the number of features that the model can look at each time it does a split.  

#### Syntax  

    from sklearn.tree import DecisionTreeClassifier

    dt = DecisionTreeClassifier(criterion='gini', max_depth=10, max_features=10, random_state=42)
    dt = dt.fit(X_train, y_train)

    dt.tree_.node_count
    dt.tree_.max_depth

> Outputs the node counts and depth of the tree.  

---

## Training, validaion, and test sets  
In this example, we manually iterate over different hyperparameter combinations (number of trees and maximum depth) and evaluate each combination on the validation set. The best hyperparameters are then used to train the final model on the combined training and validation sets, and the model is evaluated on the separate test set.  

    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score

    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

    base_model = RandomForestClassifier(random_state=42)

    for n_estimators in [50, 100, 150]:
        for max_depth in [None, 10, 20]:
            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
            model.fit(X_train, y_train)

            y_val_pred = model.predict(X_val)
            accuracy = accuracy_score(y_val, y_val_pred)

            print(f"n_estimators={n_estimators}, max_depth={max_depth} | Validation Accuracy: {accuracy}")

    best_n_estimators = 100
    best_max_depth = None

    final_model = RandomForestClassifier(n_estimators=best_n_estimators, max_depth=best_max_depth, random_state=42)
    final_model.fit(X_train.append(X_val), y_train.append(y_val))

    y_test_pred = final_model.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_test_pred)

    print("Final Test Accuracy:", test_accuracy)

---

## Supervised ML (classification & regression) Ensemble methods  
- Some useful and popular tree ensembles are bagging, boosting, and random forests.  

### Bootstrapping (statistical sampling technique)  
- Bootstrapping involves sampling with replacement from a dataset to create new datasets of the same size as the original.  
- Since it's with replacement, some data points may be repeated, and others may be left out.  
- This process allows for the estimation of the sampling distribution of a statistic and helps in assessing the variability and uncertainty associated with the data.  
- Bootstrap sampling is a method that involves drawing sample data repeatedly with replacement from a data source to estimate a model parameter. Scikit-learn has methods for bagging, but it is helpful to understand bootstrap sampling using `resampling`.  

#### Simple example  
- Resample the first five rows of a DataFrame.  
- The dataset is of the same size(5) as the original dataset, but some rows are repeated.  

    from sklearn.utils import resample

    resample(df[0:5])

### Bagging (ensemble ML technique)  
- Known as bootstrap aggregating.  
- Bagging is an ensemble technique where multiple models are trained on different bootstrap samples (subsets) of the original dataset.  
- Each model in the ensemble is exposed to a slightly different version of the training data.  
- After training, predictions from individual models are combined (usually through averaging for regression or voting for classification) to make the final prediction.  
- **Benefits**:
1. Reduce variance & overfitting associated with individual models;  
2. Increase stability of the model through averaging/voting mechanism; and  
3. Improve overall performance.   
- Bagging can be used for classification & regression.  
- Bootstrap aggregation: (bootstrap) get random subsets of the original training set to build classifiers, (aggregation) aggregate the classifications together using a majority vote.  

### Out-of-bag (oob) error  
- For each model in the ensemble, there is a subset of data points that were not included in the bootstrap sample used to train that particular model.  
- These "left-out" data points are called out-of-bag samples.  
- The out-of-bag error for a model is the error computed on this set of samples that were not seen during training.  
- Out-of-bag samples serve as a natural validation set for each model.  
- It is a way to estimate the performance of an ensemble model without the need for a separate validation set.  

### Bagging process  
- Random subsets (bootstrap samples) of the training data are generated by drawing with replacement.  
- A base model (e.g., decision tree) is trained on each bootstrap sample independently.  
- Predictions from each base model are combined to form the ensemble prediction.  

### Random forests (tree ensemble)  
- Random Forest is a specific bagging algorithm that uses decision trees as base models.  
- Randomness in a Random Forest comes from two main sources:  
1. **Bootstrapping** (rows): each tree is trained on a different subset of the original data, introducing diversity among the trees.  
2. **Feature subsetting** (columns): at each node of each tree, a random subset of features is considered for determining the best split. This ensures that each tree is not only trained on a different subset/bootstrap of data but also makes decisions based on a different subset of features.  
- By combining these two sources of randomness, random forests are able to decorrelate the individual trees and create an ensemble model that is more robust and less prone to overfitting compared to individual decision trees.  
- The most important parameters are the number of trees `n_estimators` and the number of features `max_features` to sample.  
- Extra Random Trees is an implementation that adds randomness by creating splits at random, instead of using a greedy search to find split variables and split points.  

#### Mechanice of random forests  
This illustrates selecting bootstrapped samples using `resample` and feature subsetting using `random.sample`.  
We first resample the first 5 rows.  
Then we select 3 columns out of the total of 7.  

    import random

    for n in range(5):
        print(f"sample {n}")
        print(resample(X[0:5]).iloc[:,random.sample(range(7), 3)])

### Random forests vs bagging  
- While bagging uses bootstrapping it does not use feature subsetting.  
- Random forests are a combination of trees such that each tree depends on a random subset of the **features** and **data**. As a result, each tree in the forest is different and usually performs better than bagging.  

## Boosting  
- Boosting methods are additive in the sense that they sequentially retrain decision trees using the observations with the highest residuals on the previous tree. To do so, observations with a high residual are assigned a higher weight.  
- Trees in boosting generally have one split i.e. a depth of 1 (1 node and two leaves - known as a decision stump/weak learners) that we aggregate together.  
- Each subsequent weak learner is rewarded more to correctly classify incorrect records. We lower the weights of the correct classifications & increase the weights for the incorrectly classified records.  
- We fit the entire dataset as opposed to bootstrapped samples (bagging).  
- Base trees are created successively and not independently (bagging).  
- Misclassified records are weighted more heavily and not weighted equally (bagging).  
- Boosting has the potential to overfit as each successive tree tries to improve on the prior errors (bagging does not given its extra trees).  
- The final result is the weighted sum of all classifiers where individual classifiers are weighted by how well their decision boundary did in correctly classifying the records i.e. better classifiers get more weight.  
- We also incorporate the hyperparameter *learning rate* (lambda) to say how much we can correct errors of prior trees.  
- Learning rate < 1.0 helps prevent overfitting meaning the errors from prior trees have less influence (increases regularisation).  
- To avoid overfitting use cross validation to determine correct number of tress & the learning rate.  
- Tends to be well suited to data sets with outliers and rare events.  

### Loss functions  
- Loss function is calculated as the distance from the decision boundary/margin to each record. The distance will be positive for correct classifications and negative for incorrect classifications.  

### Gradient boosting  
- Generalised boosting method that can use different loss functions.  
- The main loss functions are:  
1. **0-1 loss function** ignores observations that were correctly classified. Not used in practice as difficult to optimise (non-smooth and non-convex).  
2. **Adaptive boosting loss function** has an exponential nature. The shape of this function is more sensitive to outliers.  
- AdaBoost is usually performed with decision trees but we can use other base classifiers like support vector machines.  
3. **Gradient boosting loss function** is most common gradient boosting implementation uses a binomial log-likelihood loss function called deviance where loss = log(1+e^(-margin)). It tends to be more robust to outliers than AdaBoost.  
- The additive nature of gradient boosting makes it prone to overfitting. This can be addressed using cross validation or fine tuning the number of boosting iterations. - Other hyperparameters to fine tune are:  
`learning_rate` (shrinkage)
`subsample` set to < 1.0 uses a fraction of the data for each base learner that adds randomness and prevents overfitting (stochastic gradient boosting).  
`max_features` represents the number of features to consider in each base learner when splitting.  
- Used for classification & regression.  

### Boosting vs bagging/random forests  
- In both bagging and random forests, each classifier in the ensemble is powerful but prone to overfitting.  
- As bagging or random forests aggregate more and more classifiers, they reduce overfitting.  
- With AdaBoost, each classifier usually has performance slightly better than random. This is referred to as a weak learner or weak classifier.  
- AdaBoost combines these classifiers to get a strong classifier.  
- Unlike bagging and random forests, in AdaBoost, adding more learners can cause overfitting.    
- As a result, AdaBoost requires hyperparameter tuning, taking more time to train.  
- One advantage of AdaBoost is that each classifier is smaller, so predictions are faster.  

## XGBoost (extreme gradient boosting)  
- Similar to gradient boosting but allows for parallelisation, which speeds up training.  
- Not in scikit learn
- https://xgboost.readthedocs.io/en/latest/  
- Leading library for working with standard tabular data, like Pandas DataFrames, as opposed to more exotic types of data like images and videos.  
- Type of ensemble method.  
- Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble.  
- It begins by initializing the ensemble with a single model, whose predictions can be pretty naive.  
Method:  
- We use the current ensemble to generate predictions for each observation in the dataset.  
- These predictions are used to calculate a loss function (like mean squared error, for instance).  
- Then, we use the loss function to fit a new model by determining model parameters that reduce the loss (using gradient descent on the loss function).  
- Finally, we add the new model to ensemble, and repeat.  
Process: naive model -> make predications -> calculate loss -> train new model -> add new model to ensemble.  
Parameter tuning:  
- `n_estimators` specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble. Typical values range from 100-1000. Too low a value causes underfitting, too high a value causes overfitting.  
- `early_stopping_rounds` offers a way to automatically find the ideal value for `n_estimators`. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for `n_estimators`. You specify a number for how many rounds of straight deterioration to allow before stopping. Setting `early_stopping_rounds=5` is a reasonable choice. In this case, we stop after 5 straight rounds of deteriorating validation scores. When using `early_stopping_rounds`, you also need to set aside some data for calculating the validation scores - this is done by setting the `eval_set` parameter. If you later want to fit a model with all of your data, set `n_estimators` to whatever value you found to be optimal when running `early_stopping_rounds` stopping.  
- `learning rate` means that instead of using the full prediction from each tree in the ensemble, you multiply each tree's prediction by a small number (the learning rate) before adding them together. This makes each tree's contribution smaller, and it helps prevent overfitting. A small learning rate means slower learning, but it can lead to more accurate models with a larger number of trees (`n_estimators`).  
- `n_jobs` is used on larger datasets where runtime is a consideration. You can use parallelism to build your models faster. It's common to set the parameter `n_jobs` equal to the number of cores on your machine. It's useful in large datasets where you would otherwise spend a long time waiting during the fit command.  
- `max_depth` is the maximum depth of a tree. Increasing this value will make overfitting more likely, so decreasing max depth will decrease overfitting.  
- `min_child_weight` is essentially the minimum number of instances needed to be in each node, larger values decrease overfitting.  
- `gamma`, `reg_lambda` , `alpha` are other regularization parameters with default parameters of 0, 1 and 0, respectively. Increasing these parameters, will decrease overfitting.  

## Stacking & Voting classifiers  
- Stacking takes several classification models called base learners and uses their output as the input for the meta-classifier.  
- *Meta classifier*: this is the final classifier that brings together all the classifiers to output a single decision (via majority vote or weighted) or a new model.  
- Models of any kind can be combined to create a stacked model.  
- E.g. base learners can be a combination of logistic regression, SVM, random forest etc.  
- Output of base learners creates features that can be fed into the final classifier.  
- Similar to bagging but not limited to bootstrapping or decision trees.  
- Additional hold out set needed for base learners in addition to our hold out set for our final model.  
- Prone to overfitting given high complexity.  
- The main condition to use stacking is that models need to output predicted probabilities.  

### Syntax  
- The stacking classifier object requires a list of estimators/base learners.  
- To train the final model we create a stacking classifier, this combines the base estimators using the meta estimator.  
- The meta-classifier is determined by the parameter `final_estimator` in this case we use `LogisticRegression`, we also input the base classifiers using the `estimators` parameter and fit the model.  

    from sklearn.ensemble import StackingClassifier
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.linear_model import LogisticRegression

    estimators = [('SVM',SVC(random_state=42)),('KNN',KNeighborsClassifier()),('dt',DecisionTreeClassifier())]
    model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

---

## Model interpretability  

### Model interpretability methods  
**Intrinsic**: models with high interpretability   
- Linear  
- Decision trees  
- KNN  
These models can still become complex with too many features.  
Use regularisation or pruning to reduce complexity.  
**Post-hoc**: models with low interpretability, black box models  
- Random forests  
- Non-linear SVM  
- Neural networks  

### Model agnostic explanations  
You can use model agnostic explanation methods to understand different models, regardless of their structures.  
Feature importance:  
1. **Permutation feature importance**: measure the increase in prediction error by permuting/shuffling a feature.  
- Important features will increase error and less important features will generate small error increases.  
- The feature importance will be measured by calculating the difference between the prediction errors before and after permutation.  
2. **Impurity-based feature importance**  
3. **Shapley Additive exPlanations (SHAP) values**  
**Partial dependency plot (PDP)**: illustrates the relationship between a feature and the outcome.  
- Shows the marginal effects of one feature on the outcome (while holding other features constant).  
- Shows how the model outcome changes when a specific feature changes in its distribution.  
- Since an ML model may include many features it is not feasible to create PDP for every single feature. Thus, we normally first find the most important features via ranking their feature importances. Then, we can focus the PDP on those important features.  

#### Syntax  

    from sklearn.inspection import permutation_importance

    black_box_model = RandomForestClassifier(random_state=0, max_depth=25, max_features=10, n_estimators=100, bootstrap=True)
    black_box_model.fit(X_train, y_train)

    feature_importances = permutation_importance(estimator=black_box_model, X=X_train, y=y_train, n_repeats=5, random_state=123, n_jobs=2)

    feature_importances.importances

### Surrogate models  
- These are models trained to explain the relationship between inputs and outputs of black box/non-self-interpretable models.  
- Surrogate models have high interpretability.  
**Global surrogate model** tries to approximate the black box model.  
- Takes the inputs and predicted y from the black box model as inputs and outputs its own predicted y.  
- Measures the difference between predicted y of black box model versus predicted y of global surrogate model.  
- A small difference means the surrogate model approximates the black box model well.  
- We can then use the surrogate model for interpretability.  
**Local surrogate model** is built on one or a few instances of the black box model.  
- Global surrogate models may produce large inconsistencies to black box models.  
**Local Interpretable Model-Agnostic Explanations (LIME)** method used to build local surrogate model.  
- Generates artificial dataset based on selected data instance (seed), which is fed into black box model and used to make predictions y.  
- Use instance weighting to weight how closely instances in the artificial dataset are to the the seed data.  
- Use weighted dataset X and predictions y to train a local surrogate model.  

---

## Modelling unbalanced classes  
- Classification algorithms are built to optimize accuracy, which makes it challenging to create a model when there is not a balance across the number of observations of different classes. Common methods to approach balancing the classes are:  
1. Downsampling or removing observations from the most common class;  
2. Upsampling or duplicating observations from the rarest class or classes; and  
3. A mix of downsampling and upsampling.  
- Use cross-validation to find the best combination.  

### Stratified sampling  
1. Do a stratified train-test split.  
- Ensures that class balance remains consistent in both our train and test set.   
2. Up or down sample the dataset.  
- This must be done second because if for example we upsample the minority class before performing the train-test split we may get the same sample in both the training and testing data, which resuts in train-test contamination.  
3. Build the model.  

### Precision & recall  
Assuming the minority class is the positive class.  
**Downsampling/undersampling**: adds importance to the minor class, typically increasing recall and reducing precision.  
- Because we have more samples from the minority/positive class we are likely to have more false positives (reducing precision).  
- We increase the ability of the model to predict the minority class, but at the expense of losing data to predict the majority class.  
**Upsampling/oversampling**: recall, while lower, is still typically higher than precision, with the gap between the two being smaller.  
- The downside is that we are fitting the model to duplications of data in the minority class.  

### Random oversampling  
- Simplest oversampling approach.  
- Resample with replacement from minority class.  
- Good for categorical data.  

### Synthetic oversampling  
- Start with point in minority class.  
- Choose one of K-nearest neighbors.  
- Add a new point between them.  
Two main approaches:  
1. **SMOTE** (synthetic minority oversampling technique):  
- *Regular*: connect minorty points to any neighbour.  
- *Borderline SMOTE*: classify outliers (all nearest neighbours are from a different class), safe, or in-danger (at least 50% of nearest neighbours are from the same class).  
* Connect minority in-danger points only to minority points.  
* Connect minority in-danger points to whatever is nearby.  
- *SVM*: use minority support vectors to generate new points.  
2. **ADASYN** (adaptive synthetic sampling):  
- Look at classes in the neighbourbood of each minority point.  
- Number of samples generated for each point is proportional to the number of samples in the neighbourhood that are not from the same class.  
* So if the minority point is surrounded by points from the majority class, more minority point samples will be produced than if it were the inverse.  
* It thus puts more weight on values that would have previously been misclassified (because they were underrepresented in the neighbourhood).  

### Cluster centroid undersampling techniques  
**NearMiss-1**: keep points from the majority class that are closest to nearby minority points.  
**NearMiss-2**: keep points from the majority class that are closest to distant minority points.  
**NearMiss-3**: two step algorithm, find KNN for majority class, select majority samples for which the average distance to the nearest neighbours is the largest.  
**Tomek links**: mixed mutual nearest neighbors. Two samples from different classes are the nearest neighbours of one another. Either remove the majority class, or remove both classes.  
**Edited nearest neighbours**: remove points that don't agree with neighbours. Run KNN with k=1. If you miclassify a point that point is removed and so on.  

### Balanced bagging (blagging)  
1. Take bootstrapped samples from the original population.  
2. Balance each sample by downsampling.  
3. Learn a decision tree from each.  
4. Majority vote.  
- The result is a bagged decision trees learned on balanced populations.  

### Summary  
- Ensure to only use over/under sampling after the train-test split has been determined.  
- Use sensible metrics like AUC, or F1, but avoid using accuracy as it is easy to fool when the data is unbalanced.  
- Use cross validation to determine whether to use undersampling or oversampling.  


















