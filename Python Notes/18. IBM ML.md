# IBM ML  

## Intro to AI and ML  
- AI: a program that can sense, reason, act, and adapt.  
- ML: algorithms whose performance improve as they are exposed to more data over time.  
- Deep learning: subset of ML in which multilayered neural networks learn from vast amounts of data.  

### Machine learning  
- Programs learn from repeatedly seeing data, rather than being explicitly programmed by humans.  
- *Features/Predictor variables*: use to predict the target (explanatory variable).  
- *Target/Response*: category or value to be predicted.  
- *Label*: the value of the target for a single data point.  
- *Example*: an observation or single data point within the data.  
- Feature detection --> feed through classifier algorithm --> predict target  

### Supervised vs Unsupervised learning  
- Supervised: has a target column. The goal is to predict the target. E.g. fraud detection.   
- Unsupervised: does not have a target column. The goal is to find an underlying structure in the data. E.g. customer segmentation (find similar groupings of customers in order to be targeted).  

### Deep learning  
- Involves using models called neural networks.  
- The model determines the best representation of the original data (as opposed to classical ML where humans do this).  
- Deep neural networks with more layers.  
- TensorFlow is a library built for deep learning.  

### ML workflow    
1. Problem statement: what problem are you trying to solve?  
2. Data collection: what data do you need?  
3. Data exploration & preprocessing: how should you clean your data so your model can use it?  
4. Modelling: build a model to solve the problem  
5. Validation: did you solve the problem?  
6. Decision making and deployment: communicate to stakeholders or put into production.  

---

## Handling missing values, duplicates, and outliers  
- Data issues: duplicates or unnecessary data, inconsistent data types and typos, missing data, outliers, and data source issues.  
- Remove  
- Impute: with substituted values (such as mean, median etc.)  
- Mask: the data by creating a separate category for the missing values. This is under the assumption that missing values are indicative of useful information.  

### Approaches to calculating residuals  
- Residuals: differences between actual and predicted values, represent model failure.  
- Standardized: residual divided by standard error.  
- Deleted: residual from fitting model on all data excluding current observation. You then compare model predictions with and without this observation to assess the impact.  
- Studentized: deleted residuals divided by residual standard error (based on all data, or all data excluding current observation). You compare model predictions with and without this observation to assess the impact on the model. You then standardize the impact according to the range of the model.  

### Dealing with outliers  
- Remove them.  
- Impute them by assigning the mean or median value.  
- Transform the variable. E.g. log transformation to the column containing the outlier/s. Given the new range that observation may no longer be an outlier.  
- Predict what the value should be (using similar observations, or regression).  
- Keep them, but use a model that is resistant to outliers.  
- There are many ways to discover outliers in our data. We can do Uni-variate analysis (using one variable analysis) or Multi-variate analysis (using two or more variables). One of the simplest ways to detect an outlier is to inspect the data visually, by making box plots or scatter plots.  
- Use box plot for univariate analysis.  
- Use scatter plot for bivariate analysis.  

### Statistics  
- The assumption of the normal distribution must be met in order to perform any type of regression analysis.  
- Skewness is a measure of asymmetry of the distribution. A longer tail to the right implies positive skew.  
- Kurtosis refers to the pointedness of a peak in the distribution curve.  
- Both skewness and kurtosis are frequently used together to characterize the distribution of data.  
- Skewness for a symmetrical bell curve distribution is between -0.5 to 0.5.  
- Moderate skewness is between -0.5 to -1.0 and 0.5 to 1.0.  
- Highly skewed distribution is < -1.0 and > 1.0.  

### Z-score analysis  
- A way to identify outliers mathematically.  
- Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.   
- Data points which are too far from zero will be treated as the outliers.  
- In most of the cases, a threshold of 3 or -3 is used.  
- For example, if the Z-score value is greater than or less than 3 or -3 standard deviations respectively, that data point will be identified as a outlier.  

---

## Feature engineering and variable transformation  
- Models used in ML make assumptions about the data, such as asssuming the data follows a linear relationship.  

### Variable transformation  
- Variable selection: choosing the features to include in model.  
- Variables often need to be transformed prior to inclusion in the model.  
- Predictions from linear regression models assume residuals are normally distributed.  
- Data transformation can help solve skewness in the data.  
- Log transformations can be used for feature enginerring. The algorithm will still be a linear regression as we have just transformed the feature/s to log.  
- Polynomial features allow us to use the linear model (e.g. if a feature is budget, you can add buget^2 to the linear regression). This can help draw out the underlying relationship.  
- Encoding: converting non-numeric features to numeric features.  
- Scaling: converting the scale of numeric data so they are comparable.  

### Feature encoding  
- Often applied to categorical features.  
- Two types:  
1. Nominal: unordered (e.g. green, red, blue).  
2. Ordinal: ordered (e.g. high, medium, low).  
- Approaches:  
1. Binary: converts features to zero or one based on two outcomes (e.g. true or false, male or female).  
2. One-hot encoding: uses binary encoding where there are multiple outcomes by splitting each outcome into a separate column, which can then be assessed using binary encoding.  
3. Ordinal encoding: converting ordered categories to numerical values. Downside is that it implies a set distance between categories, which impacts the model (e.g. low=1, med=2, high=3 - the distance impacts the model). Can revert to using one-hot encoding.  

### Feature scaling  
- Adjusting a variable's scale allows for comparison of variables with different scales.  
- Different continious (numeric) features often have diffferent scales.  


### Feature scaling (type of transformation)  
- The objective is to make different variables or datasets comparable.  
- This is a type of transformation.  
- Two common ways to get all attributes to have the same scale:    
1. Min-max scaling (or normalization) is the simplest: values are shifted and rescaled so they end up ranging from 0 to 1. This is done by subtracting the min value and dividing by the max minus min.  
2. Standardization/standard scaling: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation, so that the resulting distribution has unit variance (meaning a standard deviation of 1).  
- Scikit-learn library provides `MinMaxScaler` for normalization and `StandardScaler` for standardization needs.  
