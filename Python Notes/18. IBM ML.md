# IBM ML  

## Intro to AI and ML  
- *AI*: a program that can sense, reason, act, and adapt.  
- *ML*: algorithms whose performance improve as they are exposed to more data over time.  
- *Deep learning*: subset of ML in which multilayered neural networks learn from vast amounts of data.  

### Machine learning  
- Programs learn from repeatedly seeing data, rather than being explicitly programmed by humans.  
- *Features/Predictor variables*: use to predict the target (explanatory variable).  
- *Target/Response*: category or value to be predicted.  
- *Label*: the value of the target for a single data point.  
- *Example*: an observation or single data point within the data.  
- Feature detection --> feed through classifier algorithm --> predict target  

### Supervised vs Unsupervised learning  
- **Supervised**: has a target column. The goal is to predict the target. E.g. fraud detection.   
- **Unsupervised**: does not have a target column. The goal is to find an underlying structure in the data. E.g. customer segmentation (find similar groupings of customers in order to be targeted).  

### Deep learning  
- Involves using models called neural networks.  
- The model determines the best representation of the original data (as opposed to classical ML where humans do this).  
- Deep neural networks with more layers.  
- TensorFlow is a library built for deep learning.  

### ML workflow    
1. Problem statement: what problem are you trying to solve?  
2. Data collection: what data do you need?  
3. Data exploration & preprocessing: how should you clean your data so your model can use it?  
4. Modelling: build a model to solve the problem  
5. Validation: did you solve the problem?  
6. Decision making and deployment: communicate to stakeholders or put into production.  

---

## Handling missing values, duplicates, and outliers  
- Data issues: duplicates or unnecessary data, inconsistent data types and typos, missing data, outliers, and data source issues.  
- *Remove*  
- *Impute*: with substituted values (such as mean, median etc.)  
- *Mask*: the data by creating a separate category for the   values. This is under the assumption that missing values are indicative of useful information.  

### Approaches to calculating residuals  
- **Residuals**: differences between actual and predicted values, represent model failure.  
- *Standardized*: residual divided by standard error.  
- *Deleted*: residual from fitting model on all data excluding current observation. You then compare model predictions with and without this observation to assess the impact.  
- *Studentized*: deleted residuals divided by residual standard error (based on all data, or all data excluding current observation). You compare model predictions with and without this observation to assess the impact on the model. You then standardize the impact according to the range of the model.  

### Dealing with outliers  
- Remove them.  
- Impute them by assigning the mean or median value.  
- Transform the variable. E.g. log transformation to the column containing the outlier/s. Given the new range that observation may no longer be an outlier.  
- Predict what the value should be (using similar observations, or regression).  
- Keep them, but use a model that is resistant to outliers.  
- There are many ways to discover outliers in our data. We can do Uni-variate analysis (using one variable analysis) or Multi-variate analysis (using two or more variables). One of the simplest ways to detect an outlier is to inspect the data visually, by making box plots or scatter plots.  
- Use box plot for univariate analysis.  
- Use scatter plot for bivariate analysis.  

### Statistics  
- The assumption of the normal distribution must be met in order to perform any type of regression analysis.  
- **Skewness** is a measure of asymmetry of the distribution. A longer tail to the right implies positive skew.  
- **Kurtosis** refers to the pointedness of a peak in the distribution curve.  
- Both skewness and kurtosis are frequently used together to characterize the distribution of data.  
- Skewness for a symmetrical bell curve distribution is between -0.5 to 0.5.  
- Moderate skewness is between -0.5 to -1.0 and 0.5 to 1.0.  
- Highly skewed distribution is < -1.0 and > 1.0.  

### Z-score analysis  
- A way to identify outliers mathematically.  
- **Z-score** is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.   
- Data points which are too far from zero will be treated as the outliers.  
- In most of the cases, a threshold of 3 or -3 is used.  
- For example, if the Z-score value is greater than or less than 3 or -3 standard deviations respectively, that data point will be identified as a outlier.  

---

## Variable transformation  
- Models used in ML make assumptions about the data, such as asssuming the data follows a linear relationship.  
- Variables often need to be transformed prior to inclusion in the model.  
- Predictions from linear regression models assume residuals are *normally distributed*.  
- Data transformation can help solve skewness in the data.  
- Three types of transformations:  
1. Feature engineering  
2. Feature encoding  
3. Feature scaling  
4. Discretization (the process of transforming continuous variables into discrete form, by creating bins or intervals).  

### Feature engineering  
- **Log** transformations can be used for feature enginerring. The algorithm will still be a linear regression as we have just transformed the feature/s to log.  
- **Polynomial** features allow us to use the linear model (e.g. if a feature is budget, you can add buget^2 to the linear regression). This can help draw out the underlying relationship.  
- **Feature interaction**: describes how features interact with one another. It could make sense to multiply two features together to create a new feature or divide one feature by the other. For example, there may be a higher premium for increasing 'Overall Qual' for houses that were built more recently. If such a premium or a similar effect exists, a feature that multiplies 'Overall Qual' by 'Year Built' can help us capture it.  
- **Feature deviation**: create features that capture where a feature value lies relative to the members of a category it belongs to e.g. how expensive a house is relative to other houses in its neighborhood.  

### Feature encoding (categorical features)  
- **Encoding**: converting non-numeric features to numeric features.  
- Two types:  
1. **Nominal**: unordered (e.g. green, red, blue).  
2. **Ordinal**: ordered (e.g. high, medium, low).  
- Approaches:  
1. *Binary*: converts features to zero or one based on two outcomes (e.g. true or false, male or female).  
2. *One-hot encoding/dummy variables*: uses binary encoding where there are multiple outcomes by splitting each outcome into a separate column, which can then be assessed using binary encoding.  
3. *Ordinal encoding*: converting ordered categories to numerical values. Downside is that it implies a set distance between categories, which impacts the model (e.g. low=1, med=2, high=3 - the distance impacts the model). Can revert to using one-hot encoding.  

### Feature scaling (continuous numerical data)  
- **Scaling**: converting the scale of numeric data so they are comparable.  
- Three common ways to get all attributes to have the same scale:  
1. **Min-max scaling** (or normalization) is the simplest: values are shifted and rescaled so they end up ranging from 0 to 1. This is done by subtracting the min value and dividing by the max minus min.  
2. **Standardization/standard scaling/Z-score standardization**: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation, so that the resulting distribution has unit variance (meaning a standard deviation of 1).  
- Scikit-learn library provides `MinMaxScaler` for normalization and `StandardScaler` for standardization needs.  
3. **Robust scaling**: similar to min-max scaling but focuses on interquartile range. This is done by subtracting the median value and dividing by the IQR.  

## Principal Component Analysis (PCA)  
- **Dimentionality reduction** is part of the feature extraction process that combines the existing features to produce more useful ones.  
- The goal is to simplify the data without losing too much information.  
- **PCA** is one of the most popular dimensionality reduction algorithms.  
- First, it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.  
- In this way, a few multidimensional features are merged into one.  
- Use *scikit-learn* library to perform PCA on our data.  
- First we must scale our data using the `StandardScaler()` function.  
- Once the data is scaled, apply the `fit_transform()` function to reduce the dimensionality of the dataset down to two dimensions.  
- The `explained_variance_ratio_` function allows us to see how many dimensions our dataset could be reduced to in order to explain most of the variability between the features.  

---

## Estimation and inference  
- **Estimation** is the application of an algorithm, like taking the average.  
- **Inference** involves putting an accuracy on the estimate (e.g. standard error of an average, or the statistical significance/confidence of an estimate).  
- ML & statistical inference are similar --> using data to infer qualities about the distribution.  

### Parametric vs non-parametric  
- **Statistical model** (of the data) is a set of possible distributions/regressions.  
1. **Parametric model** is a type of statistical model with a finite number of parameters. E.g. normal distribution.  
- *Maximum likelihood estimation (MLE)* is a common way of estimating parameters.  
- *Likelihood function/ratio* takes all data and outputs most likely value for parameters (like mean & standard deviation). Can be used as a test statistic to decide whether to accept or reject the null hypothesis.  
2. **Non-parametric** we make fewer assumptions. We don't assume that the data belongs to a particular distribution. E.g. creating a distribution of the data using a histogram (we don't assume normal or exponential distribution but rather one defined by the data itself).  

### Commonly used distributions  
- **Uniform** every single value is equally likely.  
- **Normal/Gaussian** most likely value is closest to the mean. Based on the *Central Limit Theorem*, which states that the sum or average of many independent random events tends to follow a normal distribution.  
- **Log normal** applying log transformation to the data to create a normal distribution.  
- **Exponential distribution** is a continuous probability distribution with the x-axis expressing distance or time between events. The y-axis is represesnted by the *probability density function (PDF)*.  
- **Poisson distribution** is a discrete probability distribution with the x-axis expressing the number of events occurring in a fixed internal of time or space. The y-axis is represented by the *probability mass function (PMF)*.  
- **Binomial distribution** is a discrete probability distribution of the number of successes of *n* trials where succcess is represented by *p* and failure by *(1-p)*.  
- **Cumulative distribution function (CDF)** derived from the PDF and gives you the probability that a random variable takes on a value less than or equal to a specific value.  
- **Percent point function (PPF)** is the inverse of a CDF and takes a probability and returns the corresponding value of the random variable.  
- **Probability density function (PDF)** provides the likelihood of a continuous random variable taking on different values within a specific range. The area under the curve within a given interval represents the proability of the random variable falling within that interval.  
- **Probability mass function (PMF)** provides the probability that a discrete random variable takes on a specific value. The sum of the probabilities for all possible values of the discrete variable should equal 1.  

### Frequentist vs Bayesian statistics  
- **Frequentist** is concerned with repeated observations in the limit. We assume no prior knowledge of the true frequencies.  
1. Derive the probabilistic property of a procedure.  
2. Apply the probability directly to the observed data.  
- **Queueing theory** is the study of waiting in queues/lines.  
- **Bayesian** describes parameters by probability distributions. A prior distribution, based on the experimenters' belief, is formulated and then updated after seeing the data. This is now a *posterior distribution*.  

---

## Hypothesis testing  
- A hypothesis is a statement about about a population parameter.  
- We create two hypotheses:  
1. Null hypothesis (H0); and  
2. Alternative hypothesis (H1 or HA)  
- We decide which one to call the null hypothesis depending on how the problem is set up.  
- The null is generally a specific value with the altenative being >, or < a certain value.  
- The procedure gives us a rule to decide:  
1. Which values of the test statistic do we accept H0;  
2. Which values of the test statistic do we reject H0 and accept H1.  
- The *likelihood ratio/test statistic* describes which of our null or alternative hypotheses is more likely.  
- You cannot accept the null hypothesis; we can only reject it or fail to reject it.  

### Errors  
- **Type 1 error** (false positive) mistakingly reject a true H0.  
- **Type 2 error** (false negative) fail to reject a false H0.  
- **Rejection region** is the set of values of the test statistic that lead to rejection of H0.  
- **Acceptance region** is the set of valies of the test statistic that lead to acceptance of H0.  
- **Null distribution** is the test statistic's distribution when the null is true.  

### Significance level  
- **Alpha/significance** is a probability threshhold below which the null hypothesis will be rejected. 
- Choose this before calculating the test statistic (usually 0.05 or 0.01).  
- Represents the probability of making a Type I error.  
- A result has statistical significance (p<=alpha) when a result at least as "extreme" would be very infrequent if the null hypothesis were true.  
- Alpha is the threshold for p below which the null hypothesis is rejected even though by assumption it were true, and something else is going on.  
- When you have a low alpha value it means you're requiring stronger evidence before you're willing to reject the null hypothesis (and avoiding a type 1 error).  

### P-value (probability value)  
- **P-value** describes how likely it is that your data would have occurred by random chance (i.e. that the null hypothesis is true and that results aren't statistically significant).  
- The smaller the p-value the less likely the results occurred by random chance, and the stronger the evidence that you should reject the null hypothesis.  
- It is the smallest significance level at which the null hypothesis would be rejected.  
- It is the probability of obtaining a result at least as extreme, given that the null hypothesis is true.  
- If the p-value of your test statistic is lower than your alpha level, you would reject the null hypothesis. This indicates that you have found evidence to suggest that the effect you're investigating is statistically significant.  
- A high p-value (greater than your alpha level) would lead you to fail to reject the null hypothesis. This suggests that you don't have strong enough evidence to conclude that the effect you're studying is statistically significant.  
- **Confidence interval** contains the values of the test statistic for which we accept the null.  
- The p-value indicates the strength of evidence against a specific null hypothesis, while the confidence interval gives you a range of plausible values for a population parameter.  
- A statistically significant result cannot prove that a research hypothesis is correct (which implies 100% certainty). Instead, we may state our results “provide support for” or “give evidence for” our research hypothesis (as there is still a slight probability that the results occurred by chance and the null hypothesis was correct – e.g., less than 5%).  

### Power and sample size  
- Probabilitiy of at least one type 1 error for a 5% significance test is `1-(1-0.05)^#tests`.  
- **Bonferroni correction**: choose a *p-threshold* so that the probability of making a type 1 error is 5%.  
- *P-threshold* = 0.05/(#tests)  
- Sample size can impact the interpretation of p-values. A larger sample size provides more reliable and precise estimates of the population, leading to narrower confidence intervals.  
- With a larger sample, even small differences between groups or effects can become statistically significant, yielding lower p-values. In contrast, smaller sample sizes may not have enough statistical power to detect smaller effects, resulting in higher p-values.  
- Therefore, a larger sample size increases the chances of finding statistically significant results when there is a genuine effect, making the findings more trustworthy and robust.  

---

## Correlation vs causation  
- **Spurious correlation** two variables appear to be correlated when, in reality, they have no causal connection. These apparent correlations are often the result of random chance or the influence of a third variable.  
- **Confounding correlation** observed correlation between two variables is influenced or distorted by a third variable (the confounding variable) that is related to both of the variables being studied.  

---

## Tests  
- A **t-test** is used for testing the *mean* of one population against a standard or comparing the means of two populations if you *do not* know standard deviation of the the population and when you have a limited sample (n < 30). If you know the standard deviation of the populations , you may use a z-test.  
- A **z-test** is used for testing the *mean* of a population versus a standard, or comparing the means of two populations, with *large* (n ≥ 30) samples, whether you know the population standard deviation or not. It is also used for testing the proportion of some characteristic versus a standard proportion, or comparing the proportions of two populations.  
- An **f-test** is used to compare variances between 2 populations. The samples can be any size. It is the basis of ANOVA.  
- **chi-squared** test is used to determine whether there is a statistically significant difference between the expected and the observed frequencies in one or more *categories* of a contingency table. A contingency table is a tabular representation of categorical data. It shows the frequency distribution of the variables.  

---

## Supervised ML: regression  
- A model is a learning algorithm, a small thing that captures a larger thing.  
- A good model omits unimportant details while retaining what's important.  

### Parameters vs hyperparameters  
- **Fit parameters** are aspects of the model we estimate (fit) using the data.  
- **Hyperparameters** are not explicit components of the model.  

### ML framework  
- Two main types of supervised ML:  
1. **Regression**: y is numeric  
- E.g. stock price, box office revenue etc.  
2. **Classification** y is categorical  
- E.g. customer churn, face recognition, which word comes next etc.  
    $ Y_{p} = F(\Omega,x) $  
    \[ Y_{p} = F(\Omega, x) \]  