# IBM ML  

## Intro to AI and ML  
- *AI*: a program that can sense, reason, act, and adapt.  
- *ML*: algorithms whose performance improve as they are exposed to more data over time.  
- *Deep learning*: subset of ML in which multilayered neural networks learn from vast amounts of data.  

### Machine learning  
- Programs learn from repeatedly seeing data, rather than being explicitly programmed by humans.  
- *Features/Predictor variables*: use to predict the target (explanatory variable).  
- *Target/Response*: category or value to be predicted.  
- *Label*: the value of the target for a single data point.  
- *Example*: an observation or single data point within the data.  
- Feature detection --> feed through classifier algorithm --> predict target  

### Supervised vs Unsupervised learning  
- **Supervised**: has a target column. The goal is to predict the target. E.g. fraud detection.   
- **Unsupervised**: does not have a target column. The goal is to find an underlying structure in the data. E.g. customer segmentation (find similar groupings of customers in order to be targeted).  

### Deep learning  
- Involves using models called neural networks.  
- The model determines the best representation of the original data (as opposed to classical ML where humans do this).  
- Deep neural networks with more layers.  
- TensorFlow is a library built for deep learning.  

### ML workflow    
1. Problem statement: what problem are you trying to solve?  
2. Data collection: what data do you need?  
3. Data exploration & preprocessing: how should you clean your data so your model can use it?  
4. Modelling: build a model to solve the problem  
5. Validation: did you solve the problem?  
6. Decision making and deployment: communicate to stakeholders or put into production.  

---

## Handling missing values, duplicates, and outliers  
- Data issues: duplicates or unnecessary data, inconsistent data types and typos, missing data, outliers, and data source issues.  
- *Remove*  
- *Impute*: with substituted values (such as mean, median etc.)  
- *Mask*: the data by creating a separate category for the   values. This is under the assumption that missing values are indicative of useful information.  

### Approaches to calculating residuals  
- **Residuals**: differences between actual and predicted values, represent model failure.  
- *Standardized*: residual divided by standard error.  
- *Deleted*: residual from fitting model on all data excluding current observation. You then compare model predictions with and without this observation to assess the impact.  
- *Studentized*: deleted residuals divided by residual standard error (based on all data, or all data excluding current observation). You compare model predictions with and without this observation to assess the impact on the model. You then standardize the impact according to the range of the model.  

### Dealing with outliers  
- Remove them.  
- Impute them by assigning the mean or median value.  
- Transform the variable. E.g. log transformation to the column containing the outlier/s. Given the new range that observation may no longer be an outlier.  
- Predict what the value should be (using similar observations, or regression).  
- Keep them, but use a model that is resistant to outliers.  
- There are many ways to discover outliers in our data. We can do Uni-variate analysis (using one variable analysis) or Multi-variate analysis (using two or more variables). One of the simplest ways to detect an outlier is to inspect the data visually, by making box plots or scatter plots.  
- Use box plot for univariate analysis.  
- Use scatter plot for bivariate analysis.  

### Statistics  
- The assumption of the normal distribution must be met in order to perform any type of regression analysis.  
- **Skewness** is a measure of asymmetry of the distribution. A longer tail to the right implies positive skew.  
- **Kurtosis** refers to the pointedness of a peak in the distribution curve.  
- Both skewness and kurtosis are frequently used together to characterize the distribution of data.  
- Skewness for a symmetrical bell curve distribution is between -0.5 to 0.5.  
- Moderate skewness is between -0.5 to -1.0 and 0.5 to 1.0.  
- Highly skewed distribution is < -1.0 and > 1.0.  

### Z-score analysis  
- A way to identify outliers mathematically.  
- **Z-score** is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.   
- Data points which are too far from zero will be treated as the outliers.  
- In most of the cases, a threshold of 3 or -3 is used.  
- For example, if the Z-score value is greater than or less than 3 or -3 standard deviations respectively, that data point will be identified as a outlier.  

---

## Variable transformation  
- Models used in ML make assumptions about the data, such as asssuming the data follows a linear relationship.  
- Variables often need to be transformed prior to inclusion in the model.  
- Predictions from linear regression models assume residuals/errors are *normally distributed*.  
- Data transformation can help solve skewness in the data.  
- Three types of transformations:  
1. Feature engineering  
2. Feature encoding  
3. Feature scaling  
4. Discretization (the process of transforming continuous variables into discrete form, by creating bins or intervals).  

### Feature engineering  
- **Log** transformations can be used for feature enginerring. The algorithm will still be a linear regression as we have just transformed the feature/s to log.  
- **Polynomial** features allow us to use the linear model (e.g. if a feature is budget, you can add buget^2 to the linear regression). This can help draw out the underlying relationship.  
- **Box-cox**: generalization of the square root function, whereas the square root function uses the exponent of 0.5, box cox lets its exponent vary so it can find the best one.  
- Useful when the relationship between two variables is, for example, upward-curved rather than linear. Polynomial engineering will allow us to express that non-linear relationship for those features while still using linear regression as our model.  
- **Feature interaction**: describes how features interact with one another. It could make sense to multiply two features together to create a new feature or divide one feature by the other. For example, there may be a higher premium for increasing 'Overall Qual' for houses that were built more recently. If such a premium or a similar effect exists, a feature that multiplies 'Overall Qual' by 'Year Built' can help us capture it.  
- **Feature deviation**: create features that capture where a feature value lies relative to the members of a category it belongs to e.g. how expensive a house is relative to other houses in its neighborhood.  

### Feature encoding (categorical features)  
- **Encoding**: converting non-numeric features to numeric features.  
- Two types:  
1. **Nominal**: unordered (e.g. green, red, blue).  
2. **Ordinal**: ordered (e.g. high, medium, low).  
- Approaches:  
1. *Binary*: converts features to zero or one based on two outcomes (e.g. true or false, male or female).  
2. *One-hot encoding/dummy variables*: uses binary encoding where there are multiple outcomes by splitting each outcome into a separate column, which can then be assessed using binary encoding. This is useful for unordered categoricals, but it creates features that are highly correlated with each other.  
3. *Ordinal encoding*: converting ordered categories to numerical values. Downside is that it implies a set distance between categories, which impacts the model (e.g. low=1, med=2, high=3 - the distance impacts the model). Can revert to using one-hot encoding.  

### Feature scaling (continuous numerical data)  
- **Scaling**: converting the scale of numeric data so they are comparable.  
- Three common ways to get all attributes to have the same scale:  
1. **Min-max scaling** (or normalization) is the simplest: values are shifted and rescaled so they end up ranging from 0 to 1. This is done by subtracting the min value and dividing by the max minus min.  
2. **Standardization/standard scaling/Z-score standardization**: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation, so that the resulting distribution has unit variance (meaning a standard deviation of 1).  
- Scikit-learn library provides `MinMaxScaler` for normalization and `StandardScaler` for standardization needs.  
3. **Robust scaling**: similar to min-max scaling but focuses on interquartile range. This is done by subtracting the median value and dividing by the IQR.  

## Principal Component Analysis (PCA)  
- **Dimentionality reduction** is part of the feature extraction process that combines the existing features to produce more useful ones.  
- The goal is to simplify the data without losing too much information.  
- **PCA** is one of the most popular dimensionality reduction algorithms.  
- First, it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.  
- In this way, a few multidimensional features are merged into one.  
- Use *scikit-learn* library to perform PCA on our data.  
- First we must scale our data using the `StandardScaler()` function.  
- Once the data is scaled, apply the `fit_transform()` function to reduce the dimensionality of the dataset down to two dimensions.  
- The `explained_variance_ratio_` function allows us to see how many dimensions our dataset could be reduced to in order to explain most of the variability between the features.  

---

## Estimation and inference  
- **Estimation** is the application of an algorithm, like taking the average.  
- **Inference** involves putting an accuracy on the estimate (e.g. standard error of an average, or the statistical significance/confidence of an estimate).  
- ML & statistical inference are similar --> using data to infer qualities about the distribution.  

### Parametric vs non-parametric  
- **Statistical model** (of the data) is a set of possible distributions/regressions.  
1. **Parametric model** is a type of statistical model with a finite number of parameters. E.g. normal distribution.  
- *Maximum likelihood estimation (MLE)* is a common way of estimating parameters.  
- *Likelihood function/ratio* takes all data and outputs most likely value for parameters (like mean & standard deviation). Can be used as a test statistic to decide whether to accept or reject the null hypothesis.  
2. **Non-parametric** we make fewer assumptions. We don't assume that the data belongs to a particular distribution. E.g. creating a distribution of the data using a histogram (we don't assume normal or exponential distribution but rather one defined by the data itself).  

### Commonly used distributions  
- **Uniform** every single value is equally likely.  
- **Normal/Gaussian** most likely value is closest to the mean. Based on the *Central Limit Theorem*, which states that the sum or average of many independent random events tends to follow a normal distribution.  
- **Log normal** applying log transformation to the data to create a normal distribution.  
- **Exponential distribution** is a continuous probability distribution with the x-axis expressing distance or time between events. The y-axis is represesnted by the *probability density function (PDF)*.  
- **Poisson distribution** is a discrete probability distribution with the x-axis expressing the number of events occurring in a fixed internal of time or space. The y-axis is represented by the *probability mass function (PMF)*.  
- **Binomial distribution** is a discrete probability distribution of the number of successes of *n* trials where succcess is represented by *p* and failure by *(1-p)*.  
- **Cumulative distribution function (CDF)** derived from the PDF and gives you the probability that a random variable takes on a value less than or equal to a specific value.  
- **Percent point function (PPF)** is the inverse of a CDF and takes a probability and returns the corresponding value of the random variable.  
- **Probability density function (PDF)** provides the likelihood of a continuous random variable taking on different values within a specific range. The area under the curve within a given interval represents the proability of the random variable falling within that interval.  
- **Probability mass function (PMF)** provides the probability that a discrete random variable takes on a specific value. The sum of the probabilities for all possible values of the discrete variable should equal 1.  

### Frequentist vs Bayesian statistics  
- **Frequentist** is concerned with repeated observations in the limit. We assume no prior knowledge of the true frequencies.  
1. Derive the probabilistic property of a procedure.  
2. Apply the probability directly to the observed data.  
- **Queueing theory** is the study of waiting in queues/lines.  
- **Bayesian** describes parameters by probability distributions. A prior distribution, based on the experimenters' belief, is formulated and then updated after seeing the data. This is now a *posterior distribution*.  

---

## Hypothesis testing  
- A hypothesis is a statement about about a population parameter.  
- We create two hypotheses:  
1. Null hypothesis (H0); and  
2. Alternative hypothesis (H1 or HA)  
- We decide which one to call the null hypothesis depending on how the problem is set up.  
- The null is generally a specific value with the altenative being >, or < a certain value.  
- The procedure gives us a rule to decide:  
1. Which values of the test statistic do we accept H0;  
2. Which values of the test statistic do we reject H0 and accept H1.  
- The *likelihood ratio/test statistic* describes which of our null or alternative hypotheses is more likely.  
- You cannot accept the null hypothesis; we can only reject it or fail to reject it.  

### Errors  
- **Type 1 error** (false positive) mistakingly reject a true H0.  
- **Type 2 error** (false negative) fail to reject a false H0.  
- **Rejection region** is the set of values of the test statistic that lead to rejection of H0.  
- **Acceptance region** is the set of valies of the test statistic that lead to acceptance of H0.  
- **Null distribution** is the test statistic's distribution when the null is true.  

### Significance level  
- **Alpha/significance** is a probability threshhold below which the null hypothesis will be rejected. 
- Choose this before calculating the test statistic (usually 0.05 or 0.01).  
- Represents the probability of making a Type I error.  
- A result has statistical significance (p<=alpha) when a result at least as "extreme" would be very infrequent if the null hypothesis were true.  
- Alpha is the threshold for p below which the null hypothesis is rejected even though by assumption it were true, and something else is going on.  
- When you have a low alpha value it means you're requiring stronger evidence before you're willing to reject the null hypothesis (and avoiding a type 1 error).  

### P-value (probability value)  
- **P-value** describes how likely it is that your data would have occurred by random chance (i.e. that the null hypothesis is true and that results aren't statistically significant).  
- The smaller the p-value the less likely the results occurred by random chance, and the stronger the evidence that you should reject the null hypothesis.  
- It is the smallest significance level at which the null hypothesis would be rejected.  
- It is the probability of obtaining a result at least as extreme, given that the null hypothesis is true.  
- If the p-value of your test statistic is lower than your alpha level, you would reject the null hypothesis. This indicates that you have found evidence to suggest that the effect you're investigating is statistically significant.  
- A high p-value (greater than your alpha level) would lead you to fail to reject the null hypothesis. This suggests that you don't have strong enough evidence to conclude that the effect you're studying is statistically significant.  
- **Confidence interval** contains the values of the test statistic for which we accept the null.  
- The p-value indicates the strength of evidence against a specific null hypothesis, while the confidence interval gives you a range of plausible values for a population parameter.  
- A statistically significant result cannot prove that a research hypothesis is correct (which implies 100% certainty). Instead, we may state our results “provide support for” or “give evidence for” our research hypothesis (as there is still a slight probability that the results occurred by chance and the null hypothesis was correct – e.g., less than 5%).  

### Power and sample size  
- Probabilitiy of at least one type 1 error for a 5% significance test is `1-(1-0.05)^#tests`.  
- **Bonferroni correction**: choose a *p-threshold* so that the probability of making a type 1 error is 5%.  
- *P-threshold* = 0.05/(#tests)  
- Sample size can impact the interpretation of p-values. A larger sample size provides more reliable and precise estimates of the population, leading to narrower confidence intervals.  
- With a larger sample, even small differences between groups or effects can become statistically significant, yielding lower p-values. In contrast, smaller sample sizes may not have enough statistical power to detect smaller effects, resulting in higher p-values.  
- Therefore, a larger sample size increases the chances of finding statistically significant results when there is a genuine effect, making the findings more trustworthy and robust.  

---

## Correlation vs causation  
- **Spurious correlation** two variables appear to be correlated when, in reality, they have no causal connection. These apparent correlations are often the result of random chance or the influence of a third variable.  
- **Confounding correlation** observed correlation between two variables is influenced or distorted by a third variable (the confounding variable) that is related to both of the variables being studied.  

---

## Tests  
- A **t-test** is used for testing the *mean* of one population against a standard or comparing the means of two populations if you *do not* know standard deviation of the the population and when you have a limited sample (n < 30). If you know the standard deviation of the populations , you may use a z-test.  
- A **z-test** is used for testing the *mean* of a population versus a standard, or comparing the means of two populations, with *large* (n ≥ 30) samples, whether you know the population standard deviation or not. It is also used for testing the proportion of some characteristic versus a standard proportion, or comparing the proportions of two populations.  
- An **f-test** is used to compare variances between 2 populations. The samples can be any size. It is the basis of ANOVA.  
- **chi-squared** test is used to determine whether there is a statistically significant difference between the expected and the observed frequencies in one or more *categories* of a contingency table. A contingency table is a tabular representation of categorical data. It shows the frequency distribution of the variables.  

---

## Supervised ML  
- A model is a learning algorithm, a small thing that captures a larger thing.  
- A good model omits unimportant details while retaining what's important.  

### Parameters vs hyperparameters  
- **Fit parameters** are aspects of the model we estimate (fit) using the data.  
- **Hyperparameters** are not explicit components of the model (e.g. learning rate or the number of hidden layers in a neural network).  

### ML framework  
- Two main types of supervised ML:  
1. **Regression**: y is numeric  
- E.g. stock price, box office revenue etc.  
2. **Classification** y is categorical  
- E.g. customer churn, face recognition, which word comes next etc.  
- **Update rule** using features x and outcome y, choose parameters omega to minimise loss J.  
- The loss function is the difference between true y and our predicted yp.  

### Interpretation and prediction  
- Models face a tradeoff between interpretability and prediction. The model you choose will be defined by your business objective.  
- Interpretation can provide insight into improvements in predication, and vice-versa.  
- **Feature importance**: how important a given feature is to our prediction.  

#### Interpretation  
- The primary objective is to train a model to find insights from the data.  
- This approach uses omega to give insight into a system.  
- We want to find what features are providing the most value in predicting the outcome variable.  
- **Omega**: represents the parameters. We find the best parameters by looking at past experience. Parameters define the relationship between the features (x) and the outcome (y).  
- **Parameters/weights** represent coefficients relating to features (x) with expected target values.  
- Weights are learned during the training process to minimize the error between the predicted values and the true values in the training data.  
- Workflow:  
1. Gather x and y to train a model by finding the omega that gives the best prediction yp (and minimises the loss).  
2. Focus on omega (rather than yp) to generate insights.  
- Optimise for a model with high interpretability rather than prediction score.  
- Example: x=customer_demographics, y=sales_data, examine omega to understand loyalty by segment i.e. what drives sales, as opposed to predicting sales.  
- Example: x=marketing_budget, y=movie_revenue, examine omega to understand marketing effectiveness i.e. how much should we spend on marketing, as opposed to predicting movie revenue.  

#### Prediction  
- The primary objective is to make the best prediction.  
- This approach compares y (true values) and yp (predicted values).  
- Because this approach doesn't focus on interpretability, we risk having a Black-box model where we don't fully understand what is happening (like in deep learning).  
- Example: x=customer_purchase_history, y=customer_churn, focus on predicting customer churn.  
- Example: x=financial_info, y=customer_default, focus on predicting loan default.  

---

## Supervised ML: regression vs classification  
- **Regression**: outcome is continuous (numeric).  
- **Classification**: outcome is categorical.  
- *Supervised learning overview*:  
1. Step 1 (fit): Data with outcomes (training) + model (parameters have not been learnt yet) --> fit model by tuning parameters to optimise predictions  
2. Step 2 (predict): Data without outcomes/unlabelled data (validation) + model (tuned) --> predict outcomes  
- To build a classification model you need:  
1. Features than can be quantified (e.g. using feature encoding);  
2. Labels that are known (for training data); and  
3. Method to measure similarity (between labelled and unlabelled dataset).  

## Regression  
- **Assumptions of linear regression**:  
1. *Linearity*:  
- The relationship between the independent variables and the dependent variable should be approximately linear.  
2. *Independence of errors*:  
- Residuals should be independent of each other.  
- Errors/residuals are random fluctuations around the true line i.e. the variability in the dependent variable doesn't increase as the value of the independent variable increases.  
3. *Homoscedasticity*:  
- The noise/random disturbance in the relationship between the independent variables and the dependent variable is the same across all values of the independent variable.  
- If the noise is not the same across the values of an independent variable, we call it *heteroscedasticity*.  
4. *Normality of residuals*  
- This assumption can be aided by transforming the y variable.  
- This is less critical if you have a large sample size (Central Limit Theorem can help) but can still be useful for hypothesis testing, confidence intervals, and certain statistical tests.  

### Multicollinearity  
- Occurs when there is a strong correlation between the independent variables.  
- Linear regression or multilinear regression requires independent variables to have little or no similar features.  
- Reduces the interpretability of the model. We can no longer interpret a coefficient on a variable because there is no scenario in which one variable can change without a conditional change in another variable.  
- In linear regression, when you one-hot encode, this means if you don't drop a column there is an infinte number of options for what the intercept and coefficients could be. This is because all columns are completely dependent on one another.  
- Using `heatmap()` function is a good way to identify whether there is *multicollinearity* present or not.  
- The best way to solve for *multicollinearity* is to use the regularization methods like *Ridge* or *Lasso*, which we will introduce in the **Regularization** lab.  

### Modelling best practice  
1. Use cost function to fit model;  
2. Develop multiple models; and  
3. Compare results and choose the best one.  
- **Cost function**:  
- Cost functions include: MAE (Mean Absolute Error), MSE (Mean Squared Error), and RMSE (Root Mean Squared Error).  
- Line of best fit/least squares regression line is found by *minimizing the sum of squared distance between the true and predicted values*.  
- This line minimises the cost function.  

---

### Syntax  

    from sklearn.linear_model import LinearRegression  
    LR = LinearRegression()  
    LR = LR.fit(X_train, y_train)  
    y_predict = LR.predict(X_test)  

### Three common measures of error for linear regression  
1. Sum of squared Error (SSE)  
- Known as L2 norm/loss.  
- Sum of squared differences between predicted value yb and true y.  
- Used in L2 regularization (Ridge), which helps prevent overfitting by adding the L2 norm of the model weights/parameters to the loss function.  
- *Note*: L1 norm/loss is the sum of the absolute differences between predicted yb and true y. This is used in L1 regularisation (Lasso), which encourages sparsity in ML models by adding the L1 norm of the model weights to the loss function.  
2. Total Sum of Squares (TSS)  
- Sum of squared differences between true y and the mean of true y. Measures total variability in the outcome variable without considering the effects of the independent variables.  
3. Coefficient of Determination (R2)  
- (R^2): 1-(SSE/TSS).  
- One minus the unexplained variation divided by the total variation.  
- Measures how well the model explains variation from the mean.  
- We want this to be as close to 1 as possible.  

---

## Data splits and polynomianl regression  
**Overfitting** is when the model is too complex and does well on the training data but not on the test data.  
- Overfitting is simple to deal with, using methods like regularization.  
**Underfitting** is when the model is too simple and performs poorly on the training and testing data sets.  
- To deal with underfitting, we can build a more complex model using methods like polynomial regression.  
- If making a more complex model does not work, this may involve using more data to train the model on or obtaining new features.  
- As this process is complex, it's better to determine if the model can overfit the data first, using methods like polynomial regression to overfit the data to determine if we have an adequate amount of data.  
Best practices to create a model that generalises well and is not overfitted:  
- Splitting data into training and test sets;  
- Using cross validation; and 
- Using polynomial features.  

Recognize the trade off between model complexity and prediction error  

## Training and test splits  
- **Data leakage**: testing data leaking into your training data set.  
- **Training data**: used to fit the model and learn the parameters.  
- **Test data**: used to measure error and performance of the model.  
1. Predict label using the fitted model.  
2. Compare with actual value.  
3. Measure the error.  
- We fit the model to learn the parameters given the dataset.  
- The ShuffleSplit will ensure there is no bias in your outcome variable.  

### Syntax  

    model.fit(X_train, y_train)  
    y_predict = model.predict(X_test)  
    error_metric(y_test, y_predict)  

---

## Polynomial regression  
- Use polynomial features to capture nonlinear relationships.  
- The resulting outcome is still using linear regression as the outcome is still a linear combination of the features, even if some of those features are being, for example, squared.  
- By default polynomial regression performs both power (squaring, cubing etc.) and interaction (between features) calculations.  
- Adding polynomial features helps deal with two fundamental problems: prediction, and interpretation.  

### Variants of standard models  
- Trade off between complexity and generalisation (overfitting vs underfitting, bias-variance tradeoff).  
- Other algorithms that help extend your linear models:  
1. Logistic regression;  
2. K-nearest neighbours;  
3. Decision trees;  
4. Support vector machines;  
5. Random forests;  
6. Ensemble methods; and  
7. Deep learning approaches.  
- Many can be used for both regression and classification.  

---

