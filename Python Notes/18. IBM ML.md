# IBM ML  

## Intro to AI and ML  
- *AI*: a program that can sense, reason, act, and adapt.  
- *ML*: algorithms whose performance improve as they are exposed to more data over time.  
- *Deep learning*: subset of ML in which multilayered neural networks learn from vast amounts of data.  

### Machine learning  
- Programs learn from repeatedly seeing data, rather than being explicitly programmed by humans.  
- *Features/Predictor variables*: use to predict the target (explanatory variable).  
- *Target/Response*: category or value to be predicted.  
- *Label*: the value of the target for a single data point.  
- *Example*: an observation or single data point within the data.  
- Feature detection --> feed through classifier algorithm --> predict target  

### Supervised vs Unsupervised learning  
- **Supervised**: has a target column. The goal is to predict the target. E.g. fraud detection.   
- **Unsupervised**: does not have a target column. The goal is to find an underlying structure in the data. E.g. customer segmentation (find similar groupings of customers in order to be targeted).  

### Deep learning  
- Involves using models called neural networks.  
- The model determines the best representation of the original data (as opposed to classical ML where humans do this).  
- Deep neural networks with more layers.  
- TensorFlow is a library built for deep learning.  

### ML workflow    
1. Problem statement: what problem are you trying to solve?  
2. Data collection: what data do you need?  
3. Data exploration & preprocessing: how should you clean your data so your model can use it?  
4. Modelling: build a model to solve the problem  
5. Validation: did you solve the problem?  
6. Decision making and deployment: communicate to stakeholders or put into production.  

---

## Handling missing values, duplicates, and outliers  
- Data issues: duplicates or unnecessary data, inconsistent data types and typos, missing data, outliers, and data source issues.  
- *Remove*  
- *Impute*: with substituted values (such as mean, median etc.)  
- *Mask*: the data by creating a separate category for the   values. This is under the assumption that missing values are indicative of useful information.  

### Approaches to calculating   s  
- **Residuals**: differences between actual and predicted values, represent model failure.  
- *Standardized*: residual divided by standard error.  
- *Deleted*: residual from fitting model on all data excluding current observation. You then compare model predictions with and without this observation to assess the impact.  
- *Studentized*: deleted residuals divided by residual standard error (based on all data, or all data excluding current observation). You compare model predictions with and without this observation to assess the impact on the model. You then standardize the impact according to the range of the model.  

### Dealing with outliers  
- Remove them.  
- Impute them by assigning the mean or median value.  
- Transform the variable. E.g. log transformation to the column containing the outlier/s. Given the new range that observation may no longer be an outlier.  
- Predict what the value should be (using similar observations, or regression).  
- Keep them, but use a model that is resistant to outliers.  
- There are many ways to discover outliers in our data. We can do Uni-variate analysis (using one variable analysis) or Multi-variate analysis (using two or more variables). One of the simplest ways to detect an outlier is to inspect the data visually, by making box plots or scatter plots.  
- Use box plot for univariate analysis.  
- Use scatter plot for bivariate analysis.  

### Statistics  
- The assumption of the normal distribution must be met in order to perform any type of regression analysis.  
- **Skewness** is a measure of asymmetry of the distribution. A longer tail to the right implies positive skew.  
- **Kurtosis** refers to the pointedness of a peak in the distribution curve.  
- Both skewness and kurtosis are frequently used together to characterize the distribution of data.  
- Skewness for a symmetrical bell curve distribution is between -0.5 to 0.5.  
- Moderate skewness is between -0.5 to -1.0 and 0.5 to 1.0.  
- Highly skewed distribution is < -1.0 and > 1.0.  

### Z-score analysis  
- A way to identify outliers mathematically.  
- **Z-score** is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.   
- Data points which are too far from zero will be treated as the outliers.  
- In most of the cases, a threshold of 3 or -3 is used.  
- For example, if the Z-score value is greater than or less than 3 or -3 standard deviations respectively, that data point will be identified as a outlier.  

---

## Variable transformation  
- Models used in ML make assumptions about the data, such as asssuming the data follows a linear relationship.  
- Variables often need to be transformed prior to inclusion in the model.  
- Predictions from linear regression models assume residuals/errors are *normally distributed*.  
- Data transformation can help solve skewness in the data.  
- Three types of transformations:  
1. Feature engineering  
2. Feature encoding  
3. Feature scaling  
4. Discretization (the process of transforming continuous variables into discrete form, by creating bins or intervals).  

### Feature engineering  
- **Log** transformations can be used for feature enginerring. The algorithm will still be a linear regression as we have just transformed the feature/s to log.  
- **Polynomial** features allow us to use the linear model (e.g. if a feature is budget, you can add buget^2 to the linear regression). This can help draw out the underlying relationship.  
- **Box-cox**: generalization of the square root function, whereas the square root function uses the exponent of 0.5, box cox lets its exponent vary so it can find the best one.  
- Useful when the relationship between two variables is, for example, upward-curved rather than linear. Polynomial engineering will allow us to express that non-linear relationship for those features while still using linear regression as our model.  
- **Feature interaction**: describes how features interact with one another. It could make sense to multiply two features together to create a new feature or divide one feature by the other. For example, there may be a higher premium for increasing 'Overall Qual' for houses that were built more recently. If such a premium or a similar effect exists, a feature that multiplies 'Overall Qual' by 'Year Built' can help us capture it.  
- **Feature deviation**: create features that capture where a feature value lies relative to the members of a category it belongs to e.g. how expensive a house is relative to other houses in its neighborhood.  

### Feature encoding (categorical features)  
- **Encoding**: converting non-numeric features to numeric features.  
- Two types:  
1. **Nominal**: unordered (e.g. green, red, blue).  
2. **Ordinal**: ordered (e.g. high, medium, low).  
- Approaches:  
1. *Binary*: converts features to zero or one based on two outcomes (e.g. true or false, male or female).  
2. *One-hot encoding/dummy variables*: uses binary encoding where there are multiple outcomes by splitting each outcome into a separate column, which can then be assessed using binary encoding. This is useful for unordered categoricals, but it creates features that are highly correlated with each other.  
3. *Ordinal encoding*: converting ordered categories to numerical values. Downside is that it implies a set distance between categories, which impacts the model (e.g. low=1, med=2, high=3 - the distance impacts the model). Can revert to using one-hot encoding.  

### Feature scaling (continuous numerical data)  
- **Scaling**: converting the scale of numeric data so they are comparable.  
- Three common ways to get all attributes to have the same scale:  
1. **Min-max scaling** (or normalization) is the simplest: values are shifted and rescaled so they end up ranging from 0 to 1. This is done by subtracting the min value and dividing by the max minus min.  
2. **Standardization/standard scaling/Z-score standardization**: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation, so that the resulting distribution has unit variance (meaning a standard deviation of 1).  
- Scikit-learn library provides `MinMaxScaler` for normalization and `StandardScaler` for standardization needs.  
3. **Robust scaling**: similar to min-max scaling but focuses on interquartile range. This is done by subtracting the median value and dividing by the IQR.  

## Principal Component Analysis (PCA)  
- **Dimentionality reduction** is part of the feature extraction process that combines the existing features to produce more useful ones.  
- The goal is to simplify the data without losing too much information.  
- **PCA** is one of the most popular dimensionality reduction algorithms.  
- First, it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.  
- In this way, a few multidimensional features are merged into one.  
- Use *scikit-learn* library to perform PCA on our data.  
- First we must scale our data using the `StandardScaler()` function.  
- Once the data is scaled, apply the `fit_transform()` function to reduce the dimensionality of the dataset down to two dimensions.  
- The `explained_variance_ratio_` function allows us to see how many dimensions our dataset could be reduced to in order to explain most of the variability between the features.  

---

## Estimation and inference  
- **Estimation** is the application of an algorithm, like taking the average.  
- **Inference** involves putting an accuracy on the estimate (e.g. standard error of an average, or the statistical significance/confidence of an estimate).  
- ML & statistical inference are similar --> using data to infer qualities about the distribution.  

### Parametric vs non-parametric  
- **Statistical model** (of the data) is a set of possible distributions/regressions.  
1. **Parametric model** is a type of statistical model with a finite number of parameters. E.g. normal distribution.  
- *Maximum likelihood estimation (MLE)* is a common way of estimating parameters.  
- *Likelihood function/ratio* takes all data and outputs most likely value for parameters (like mean & standard deviation). Can be used as a test statistic to decide whether to accept or reject the null hypothesis.  
2. **Non-parametric** we make fewer assumptions. We don't assume that the data belongs to a particular distribution. E.g. creating a distribution of the data using a histogram (we don't assume normal or exponential distribution but rather one defined by the data itself).  

### Commonly used distributions  
- **Uniform** every single value is equally likely.  
- **Normal/Gaussian** most likely value is closest to the mean. Based on the *Central Limit Theorem*, which states that the sum or average of many independent random events tends to follow a normal distribution.  
- **Log normal** applying log transformation to the data to create a normal distribution.  
- **Exponential distribution** is a continuous probability distribution with the x-axis expressing distance or time between events. The y-axis is represesnted by the *probability density function (PDF)*.  
- **Poisson distribution** is a discrete probability distribution with the x-axis expressing the number of events occurring in a fixed internal of time or space. The y-axis is represented by the *probability mass function (PMF)*.  
- **Binomial distribution** is a discrete probability distribution of the number of successes of *n* trials where succcess is represented by *p* and failure by *(1-p)*.  
- **Cumulative distribution function (CDF)** derived from the PDF and gives you the probability that a random variable takes on a value less than or equal to a specific value.  
- **Percent point function (PPF)** is the inverse of a CDF and takes a probability and returns the corresponding value of the random variable.  
- **Probability density function (PDF)** provides the likelihood of a continuous random variable taking on different values within a specific range. The area under the curve within a given interval represents the proability of the random variable falling within that interval.  
- **Probability mass function (PMF)** provides the probability that a discrete random variable takes on a specific value. The sum of the probabilities for all possible values of the discrete variable should equal 1.  

### Frequentist vs Bayesian statistics  
- **Frequentist** is concerned with repeated observations in the limit. We assume no prior knowledge of the true frequencies.  
1. Derive the probabilistic property of a procedure.  
2. Apply the probability directly to the observed data.  
- **Queueing theory** is the study of waiting in queues/lines.  
- **Bayesian** describes parameters by probability distributions. A prior distribution, based on the experimenters' belief, is formulated and then updated after seeing the data. This is now a *posterior distribution*.  

---

## Hypothesis testing  
- A hypothesis is a statement about about a population parameter.  
- We create two hypotheses:  
1. Null hypothesis (H0); and  
2. Alternative hypothesis (H1 or HA)  
- We decide which one to call the null hypothesis depending on how the problem is set up.  
- The null is generally a specific value with the altenative being >, or < a certain value.  
- The procedure gives us a rule to decide:  
1. Which values of the test statistic do we accept H0;  
2. Which values of the test statistic do we reject H0 and accept H1.  
- The *likelihood ratio/test statistic* describes which of our null or alternative hypotheses is more likely.  
- You cannot accept the null hypothesis; we can only reject it or fail to reject it.  

### Errors  
- **Type 1 error** (false positive) mistakingly reject a true H0.  
- **Type 2 error** (false negative) fail to reject a false H0.  
- **Rejection region** is the set of values of the test statistic that lead to rejection of H0.  
- **Acceptance region** is the set of valies of the test statistic that lead to acceptance of H0.  
- **Null distribution** is the test statistic's distribution when the null is true.  

### Significance level  
- **Alpha/significance** is a probability threshhold below which the null hypothesis will be rejected. 
- Choose this before calculating the test statistic (usually 0.05 or 0.01).  
- Represents the probability of making a Type I error.  
- A result has statistical significance (p<=alpha) when a result at least as "extreme" would be very infrequent if the null hypothesis were true.  
- Alpha is the threshold for p below which the null hypothesis is rejected even though by assumption it were true, and something else is going on.  
- When you have a low alpha value it means you're requiring stronger evidence before you're willing to reject the null hypothesis (and avoiding a type 1 error).  

### P-value (probability value)  
- **P-value** describes how likely it is that your data would have occurred by random chance (i.e. that the null hypothesis is true and that results aren't statistically significant).  
- The smaller the p-value the less likely the results occurred by random chance, and the stronger the evidence that you should reject the null hypothesis.  
- It is the smallest significance level at which the null hypothesis would be rejected.  
- It is the probability of obtaining a result at least as extreme, given that the null hypothesis is true.  
- If the p-value of your test statistic is lower than your alpha level, you would reject the null hypothesis. This indicates that you have found evidence to suggest that the effect you're investigating is statistically significant.  
- A high p-value (greater than your alpha level) would lead you to fail to reject the null hypothesis. This suggests that you don't have strong enough evidence to conclude that the effect you're studying is statistically significant.  
- **Confidence interval** contains the values of the test statistic for which we accept the null.  
- The p-value indicates the strength of evidence against a specific null hypothesis, while the confidence interval gives you a range of plausible values for a population parameter.  
- A statistically significant result cannot prove that a research hypothesis is correct (which implies 100% certainty). Instead, we may state our results “provide support for” or “give evidence for” our research hypothesis (as there is still a slight probability that the results occurred by chance and the null hypothesis was correct – e.g., less than 5%).  

### Power and sample size  
- Probabilitiy of at least one type 1 error for a 5% significance test is `1-(1-0.05)^#tests`.  
- **Bonferroni correction**: choose a *p-threshold* so that the probability of making a type 1 error is 5%.  
- *P-threshold* = 0.05/(#tests)  
- Sample size can impact the interpretation of p-values. A larger sample size provides more reliable and precise estimates of the population, leading to narrower confidence intervals.  
- With a larger sample, even small differences between groups or effects can become statistically significant, yielding lower p-values. In contrast, smaller sample sizes may not have enough statistical power to detect smaller effects, resulting in higher p-values.  
- Therefore, a larger sample size increases the chances of finding statistically significant results when there is a genuine effect, making the findings more trustworthy and robust.  

---

## Correlation vs causation  
- **Spurious correlation** two variables appear to be correlated when, in reality, they have no causal connection. These apparent correlations are often the result of random chance or the influence of a third variable.  
- **Confounding correlation** observed correlation between two variables is influenced or distorted by a third variable (the confounding variable) that is related to both of the variables being studied.  

---

## Tests  
- A **t-test** is used for testing the *mean* of one population against a standard or comparing the means of two populations if you *do not* know standard deviation of the the population and when you have a limited sample (n < 30). If you know the standard deviation of the populations , you may use a z-test.  
- A **z-test** is used for testing the *mean* of a population versus a standard, or comparing the means of two populations, with *large* (n ≥ 30) samples, whether you know the population standard deviation or not. It is also used for testing the proportion of some characteristic versus a standard proportion, or comparing the proportions of two populations.  
- An **f-test** is used to compare variances between 2 populations. The samples can be any size. It is the basis of ANOVA.  
- **chi-squared** test is used to determine whether there is a statistically significant difference between the expected and the observed frequencies in one or more *categories* of a contingency table. A contingency table is a tabular representation of categorical data. It shows the frequency distribution of the variables.  

---

## Supervised ML  
- A model is a learning algorithm, a small thing that captures a larger thing.  
- A good model omits unimportant details while retaining what's important.  

### Parameters vs hyperparameters  
- **Fit parameters** are aspects of the model we estimate (fit) using the data (the model learns these parameters).  
- **Hyperparameters** are not explicit components of the model, we tune these parameters ourselves (e.g. degree of the polynomial, learning rate, or the number of hidden layers in a neural network).  

### ML framework  
- Two main types of supervised ML:  
1. **Regression**: y is numeric  
- E.g. stock price, box office revenue etc.  
2. **Classification** y is categorical  
- E.g. customer churn, face recognition, which word comes next etc.  
- **Update rule** using features x and outcome y, choose parameters omega to minimise loss J.  
- The loss function is the difference between true y and our predicted yp.  

### Interpretation and prediction  
- Models face a tradeoff between interpretability and prediction. The model you choose will be defined by your business objective.  
- Interpretation can provide insight into improvements in predication, and vice-versa.  
- **Feature importance**: how important a given feature is to our prediction.  

#### Interpretation  
- The primary objective is to train a model to find insights from the data.  
- This approach uses omega to give insight into a system.  
- We want to find what features are providing the most value in predicting the outcome variable.  
**Omega**: represents the parameters. We find the best parameters by looking at past experience. Parameters define the relationship between the features (x) and the outcome (y).  
**Parameters/weights** represent coefficients relating to features (x) with expected target values.  
- Usually, we can interpret lager coefficients as having more importance on the prediction, but this is not always the case as larger coefficients can correspond to overfiting.  
- Weights are learned during the training process to minimize the error between the predicted values and the true values in the training data.  
- Workflow:  
1. Gather x and y to train a model by finding the omega that gives the best prediction yp (and minimises the loss).  
2. Focus on omega (rather than yp) to generate insights.  
- Optimise for a model with high interpretability rather than prediction score.  
- Example: x=customer_demographics, y=sales_data, examine omega to understand loyalty by segment i.e. what drives sales, as opposed to predicting sales.  
- Example: x=marketing_budget, y=movie_revenue, examine omega to understand marketing effectiveness i.e. how much should we spend on marketing, as opposed to predicting movie revenue.  

#### Prediction  
- The primary objective is to make the best prediction.  
- This approach compares y (true values) and yp (predicted values).  
- Because this approach doesn't focus on interpretability, we risk having a Black-box model where we don't fully understand what is happening (like in deep learning).  
- Example: x=customer_purchase_history, y=customer_churn, focus on predicting customer churn.  
- Example: x=financial_info, y=customer_default, focus on predicting loan default.  

---

## Supervised ML: regression vs classification  
- **Regression**: outcome is continuous (numeric).  
- **Classification**: outcome is categorical.  
- *Supervised learning overview*:  
1. Step 1 (fit): Data with outcomes (training) + model (parameters have not been learnt yet) --> fit model by tuning parameters to optimise predictions  
2. Step 2 (predict): Data without outcomes/unlabelled data (validation) + model (tuned) --> predict outcomes  
- To build a classification model you need:  
1. Features than can be quantified (e.g. using feature encoding);  
2. Labels that are known (for training data); and  
3. Method to measure similarity (between labelled and unlabelled dataset).  

## Regression  
- **Assumptions of linear regression**:  
1. *Linearity*:  
- The relationship between the independent variables and the dependent variable should be approximately linear.  
2. *Independence of errors*:  
- Residuals should be independent of each other.  
- Errors/residuals are random fluctuations around the true line i.e. the variability in the dependent variable doesn't increase as the value of the independent variable increases.  
3. *Homoscedasticity*:  
- The noise/random disturbance in the relationship between the independent variables and the dependent variable is the same across all values of the independent variable.  
- If the noise is not the same across the values of an independent variable, we call it *heteroscedasticity*.  
4. *Normality of residuals*  
- This assumption can be aided by transforming the y variable.  
- This is less critical if you have a large sample size (Central Limit Theorem can help) but can still be useful for hypothesis testing, confidence intervals, and certain statistical tests.  

### Multicollinearity  
- Occurs when there is a strong correlation between the independent variables.  
- Linear regression or multilinear regression requires independent variables to have little or no similar features.  
- Reduces the interpretability of the model. We can no longer interpret a coefficient on a variable because there is no scenario in which one variable can change without a conditional change in another variable.  
- In linear regression, when you one-hot encode, this means if you don't drop a column there is an infinte number of options for what the intercept and coefficients could be. This is because all columns are completely dependent on one another.  
- Using `heatmap()` function is a good way to identify whether there is *multicollinearity* present or not.  
- The best way to solve for *multicollinearity* is to use the regularization methods like *Ridge* or *Lasso*.  

### Modelling best practice  
1. Use cost function to fit model;  
2. Develop multiple models; and  
3. Compare results and choose the best one.  
- **Cost function**:  
- Cost functions include: MAE (Mean Absolute Error), MSE (Mean Squared Error), and RMSE (Root Mean Squared Error).  
- Line of best fit/least squares regression line is found by *minimizing the sum of squared distance between the true and predicted values*.  
- This line minimises the cost function.  

---

### Syntax  

    from sklearn.linear_model import LinearRegression  
    LR = LinearRegression()  
    LR = LR.fit(X_train, y_train)  
    y_predict = LR.predict(X_test)  

### Three common measures of error for linear regression  
1. Sum of squared Error (SSE)  
- Known as L2 norm/loss.  
- Sum of squared differences between predicted value yb and true y.  
- Used in L2 regularization (Ridge), which helps prevent overfitting by adding the L2 norm of the model weights/parameters to the loss function.  
- *Note*: L1 norm/loss is the sum of the absolute differences between predicted yb and true y. This is used in L1 regularisation (Lasso), which encourages sparsity in ML models by adding the L1 norm of the model weights to the loss function.  
2. Total Sum of Squares (TSS)  
- Sum of squared differences between true y and the mean of true y. Measures total variability in the outcome variable without considering the effects of the independent variables.  
3. Coefficient of Determination (R2)  
- (R2): 1-(SSE/TSS).  
- One minus the unexplained variation divided by the total variation.  
- Measures how well the model explains variation from the mean.  
- Indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  
- We want this to be as close to 1 as possible.  

---

## Data splits and polynomianl regression  
**Overfitting** is when the model is too complex and does well on the training data but not on the test data.  
- Overfitting is simple to deal with, using methods like regularization.  
**Underfitting** is when the model is too simple and performs poorly on the training and testing data sets.  
- To deal with underfitting, we can build a more complex model using methods like polynomial regression.  
- If making a more complex model does not work, this may involve using more data to train the model on or obtaining new features.  
- As this process is complex, it's better to determine if the model can overfit the data first, using methods like polynomial regression to overfit the data to determine if we have an adequate amount of data.  
Best practices to create a model that generalises well and is not overfitted:  
- Splitting data into training and test sets;  
- Using cross validation; and 
- Using polynomial features.  

Recognize the trade off between model complexity and prediction error  

## Training and test splits  
- **Data leakage**: testing data leaking into your training data set.  
- **Training data**: used to fit the model and learn the parameters.  
- **Test data**: used to measure error and performance of the model.  
1. Predict label using the fitted model.  
2. Compare with actual value.  
3. Measure the error.  
- We fit the model to learn the parameters given the dataset.  
- The ShuffleSplit will ensure there is no bias in your outcome variable.  

### Syntax  

    model.fit(X_train, y_train)  
    y_predict = model.predict(X_test)  
    error_metric(y_test, y_predict)  

---

## Polynomial regression  
- Use polynomial features to capture nonlinear relationships.  
- The resulting outcome is still using linear regression as the outcome is still a linear combination of the features, even if some of those features are being, for example, squared.  
- By default polynomial regression performs both power (squaring, cubing etc.) and interaction (between features) calculations.  
- Adding polynomial features helps deal with two fundamental problems: prediction, and interpretation.  
- Helps increase complexity of the model, but can result in overfitting.  

### Variants of standard models  
- Trade off between complexity and generalisation (overfitting vs underfitting, bias-variance tradeoff).  
- Other algorithms that help extend your linear models:  
1. Logistic regression;  
2. K-nearest neighbours;  
3. Decision trees;  
4. Support vector machines;  
5. Random forests;  
6. Ensemble methods; and  
7. Deep learning approaches.  
- Many can be used for both regression and classification.  

---

## Data pipelines  
- Simplify the steps of processing the data.  
- Allows us to bypass steps of applying fit_transform to X_train and then transform to X_test.  

### Syntax  

    pipe = Pipeline([('ss',StandardScaler()),('lr', LinearRegression())])
    pipe.fit(X_train,y_train)
    predicted = pipe.predict(X_test)

---

## Cross validation  
- In supervised ML it is common to withhold a portion of the data to test the final model's performance.  
- This model testing is performed on the 'unseen' data, which the model was not trained on.  
- This withholding of a portion of the dataset for testing is called cross-validation.  
- Cross-validation can also be used to select hyper-parameters.  
- Cross-validation helps avoid over-fitting; a complex model could repeat the labels of the samples that it has just seen and, therefore, would have a perfect score but would fail to predict anything useful on the 'unseen' data.  
The three most common cross validation approaches are:  
1. K-fold cross validation;  
2. Leave one out cross validation; and  
3. Stratified cross validation.  
Cross validation involves dividing the dataset into 3 parts:  
1. **training set** - is a portion of the data used for training the model;  
2. **validation set** - is a portion of the data used to optimize the hyperparameters of the model; and  
3. **test set** - is a portion of the data used to evaluate the model.  

## Cross validation syntax  
`Scikit Learn` library contains many methods that can perform the splitting of the data into training, testing and validation sets.  
The most popular methods that we covered in this module are:  
- `train_test_split` - creates a single split into train and test sets.  
- `KFold` - creates number of k-fold splits, allowing cross validation.  
- `cross_val_score` - evaluates model's score through cross validation.  
- `cross_val_predict` – produces the out-of-bag prediction for each row.  
- `GridSearchCV` – scans over parameters to select the best hyperparameter set with the best out-of-sample score.  

## K-fold cross validation  
- Split data into multiple pairs of train and test splits and average the error across all pairs.  
- Results in more statistically signifcant results.  
- Cross-validation helps in assessing how well a model will generalize to unseen data.  
- In k-fold cross-validation, the dataset is split into "k" equally-sized parts or folds.  
- Depending on the number of folds you choose to split up the data, the first fold is always the test set and the remaining folds are the training set.  
- Remember the training data may overlap but not the test data.  
- Smaller folds/k can lead to higher bias but lower variance because the model is trained on less data, which may result in underfitting.  
- Larger folds/k can reduce bias but may increase variance because the model is trained on more data, potentially leading to overfitting if the model is too complex.  
- `cross_val_predict` is a function that does K-fold cross validation for us, appropriately fitting and transforming at every step of the way.  

### Syntax (using `cross_val_predict`)  

    X = df.drop('Price', axis=1)
    y = df.Price
    s = StandardScaler()
    lr = LinearRegression()
    estimator = Pipeline([("scaler", s), ("regression", lr)])
    kf = KFold(shuffle=True, random_state=72018, n_splits=3)
    predictions = cross_val_predict(estimator, X, y, cv=kf)
    r2_score(y, predictions)

> `cross_val_predict` splits the data for you for each fold of cross-validation, you do not need to explicitly define X_train and y_train.  
> The `cv` parameter specifies the number of folds or partitions into which the dataset will be divided for cross-validation.  
> `cv` could be set to a number but we set it to kf, the function defined previously, which ensures the data is shuffled.  
> `cross_val_predict` applies the estimator to each of the test sets, makes predictions on the test data for each fold, and then it aggregates the predictions.  

## Hyperparameter tuning  
- Using cross validation (or train-test splits) to determine which hyperparameters that are most likely to generate a model that generalizes well outside of your sample.  

### Syntax (tuning alpha for Lasso regression)  

    pf = PolynomialFeatures(degree=3)
    scores = []
    alphas = np.geomspace(0.001, 10, 5)
    for alpha in alphas:
        
        las = Lasso(alpha=alpha, max_iter=100000)
        estimator = Pipeline([("make_higher_degree", pf), ("scaler", s), ("lasso_regression", las)])
        predictions = cross_val_predict(estimator, X, y, cv = kf)
        score = r2_score(y, predictions)
        scores.append(score)

    list(zip(alphas,scores))

    best_estimator = Pipeline([("make_higher_degree", PolynomialFeatures(degree=2)),("scaler", s),("lasso_regression", Lasso(alpha=0.01, max_iter=10000))])
    best_estimator.fit(X, y)
    best_estimator.score(X, y)

    best_estimator.named_steps["lasso_regression"].coef_

## Grid Search CV  
To do cross-validation, we used two techniques:  
1. `KFold` and manually create a loop to do cross-validation.  
2. `cross_val_predict` and `score` to get a cross-valiated score in a couple of lines.  
To do hyper-parameter tuning, we see a general pattern:  
1. `cross_val_predict` and `score` in a manually written loop over hyperparemeters, then select the best one.  
GridSearchCV does all of this in one step.  

---

## Parameters (biases & weights)  
- **Bias**: in linear models refers to the intercept term. It represents the expected value of the dependent variable when all predictor variables are set to zero.  
- **Weights**: in linear models represent the coefficients/slopes associated with each feature.  
- In the context of a linear model the intercept (bias) and the coefficients (weights) are the parameters of the linear model. These are estimated during the model training process to find the best-fitting line that minimizes the sum of squared errors.  

## Bias & variance tradeoff  
Three sources of model error:  
1. Bias: being wrong;  
2. Variance: being unstable; and  
3. Irreducible error: unavoidable randomness/intrinsic & unavoidable error.  
To reduce bias and increase model complexity, you can use more features or higher-degree polynomial terms in the model.  
To reduce variance and simplify the model, you can use fewer features or employ regularization techniques like Lasso or Ridge regression.  
**Bias** (Underfitting):  
- When a model has high bias, it means that it is too simple to capture the underlying patterns or relationships in the data.  
- Models with high bias tend to underfit the data, which means they perform poorly both on the training data and unseen data (test data).  
- High bias models are characterized by their inability to learn from the data, leading to poor model performance. This is why they are associated with low model complexity.  
**Variance** (Overfitting):  
- When a model has high variance, it means that it is too complex and can fit the noise in the data as well as the underlying patterns. It essentially memorizes the training data rather than generalizing from it.  
- Models with high variance tend to overfit the data, which means they perform extremely well on the training data but poorly on unseen data (test data). They capture the noise in the data, resulting in poor generalization to new data.  
- Characterised by sensitivity to small changes in input data.  

## Regularisation & model selection  
- To manage complexity & error we can use a simpler model or employ regularisation techniques.  
- We can select the best regularisation strength via cross-validation.  
- It's best practice to scale features before regularisation (i.e using StandardScaler) so penalties aren't impacted by variable scales.  
- **Complexity tradeoff**: variance reduction (less complex model), through regularisation, may outpace the increase in bias, leading to a better fit model.  

## Lasso regression (L1) & ridge regression (L2)  
Main aim is to reduce complexity of the model.  
- Model complexity is associated with having too many features (variables) that may not be essential for making accurate predictions.  
- Achieves model simplicity by encouraging the model to reduce the impact of less important features.   
- The biggest difference between ridge and lasso regression is that in ridge regression, while model coefficients can shrink towards zero, they never actually become zero.  
- Due to this, lasso regression can also be used as a `feature selection technique`, since variables with low importance can have coefficients that reach zero and will be removed entirely from the model.  
- Lambda is the hyperparameter that controls the amount of L1/L2 regularization applied to the linear regression model. A larger lambda value leads to stronger regularization.  
- Lambda shrinks the magnitude of all weights/coefficients (via adding a penalty to the loss funtion).  
- As lambda increases coefficients/weights decrease.  
- Penalty term added to cost function: in Ridge regression the weights are squared while in Lasso regression the absolute value of the weigths are used.  
- Increasing lambda for Ridge adds more penalty for larger weights/coefficients because of the squarring of coefficients when calculating the new loss function.  
**Feature Selection**:  
- Lasso adds a penalty term to the model's loss function based on the absolute values of the model's coefficients.  
- This penalty encourages some of the coefficient values to become exactly zero.  
- When a coefficient becomes zero, it means that the corresponding feature is effectively removed from the model.  
- This process is a form of automatic feature selection, which simplifies the model by eliminating irrelevant or redundant features.  
**Sparse Models**:  
- Because Lasso can force some coefficients to be exactly zero, it produces sparse models.  
- Sparse models have fewer nonzero coefficients, which means they rely on a smaller subset of features for making predictions.  
- This reduces the complexity of the model and can make it more interpretable.  
**Regularization**:  
- Lasso regularization prevents the model from fitting the training data too closely, which can help prevent overfitting.  
- Overfitting occurs when a model becomes overly complex by fitting noise or random fluctuations in the training data.  
- By constraining the coefficients with the L1 penalty, Lasso helps in achieving a simpler model that generalizes better to unseen data.  
**Hyperparameter tuning**:  
- The hyperparameter being tuned is the regularization hyperparameter `lambda` (called alpha in scikit learn) used in Lasso regression.  
- In Lasso regression, *lambda is the hyperparameter that controls the strength of L1 regularization*.  
- It determines how much the model penalizes the absolute values of the coefficients, encouraging some of them to become exactly zero.  
- This hyperparameter helps control the complexity of the model and performs feature selection by shrinking some coefficients to zero.  
- We are therefore tuning the lambda hyperparameter for Lasso regression.  

## Ridge vs Lasso  
- Ridge is more computationally efficient.  
- Lasso is better for interpretability as it can perform feature selection by reducing some weights to zero.  
- Ridge may be better if the target depends on many of the features (as Lasso may reudce these features to zero).  

## Elastic net  
- Combines Ridge and Lasso.  
- Introduces the alpha hyperparameter that determines a weighted average of L1 & L2 penalties.  

## Recursive feature elimination (RFE)  
- Provided by SKlearn
- RFE combines a model/estimation approach with a desired number of features.  
- RFE repeatedly applies the model, measures feature importance, and recursively removes less important features.  

---

## Gradient descent  
The goal is to find the optimal set of parameters that lead to the best model performance by iteratively refining the parameter values based on the gradient of the cost function.  
It is an optimization algorithm used to minimize the cost or loss function during the training of a model.  
**Objective**:  
- In ML we often want to find the model parameters (weights and biases) that result in the best performance on a given task.  
- This is typically done by minimizing a cost or loss function, which quantifies how far off our model's predictions are from the actual target values.  
**Gradient**:  
- The gradient of the cost function at a specific point represents the direction and magnitude of the steepest ascent.  
- In other words, it tells us how the cost will change if we make small adjustments to our model parameters.  
**Gradient Descent Steps**:  
1. *Initialization*:  
- We start with an initial guess for the model parameters (weights and biases).  
2. *Compute Gradient*:  
- At each step, we compute the gradient of the cost function with respect to the parameters.  
3. *Update Parameters*:  
- We adjust the parameters in the opposite direction of the gradient to reduce the cost.  
- This is done by multiplying the gradient by a learning rate (a small positive value) and subtracting it from the current parameter values.  
- The learning rate is a hyperparameter that determines the step size in the parameter space. It's essential to choose an appropriate learning rate as too large a value may cause overshooting, and too small a value may result in slow convergence.  
4. *Repeat*:  
- We repeat the process iteratively for a predefined number of iterations or until the cost function converges to a minimum.  
**Convergence**: Gradient descent continues to update the parameters until the cost function reaches a minimum or a predefined stopping criterion is met. The algorithm stops when further adjustments do not significantly reduce the cost.  

---




