EXPLORATORY DATA ANALYSIS IN PYTHON
Exploratory data analysis is a process for exploring datasets, answering questions, and visualizing results. Clean and validate data, to visualize distributions and relationships between variables, and use regression models to predict and explain. You'll use Pandas, NumPy, SciPy, StatsModels for regression, and Matplotlib for visualization.

1. Read, Clean & Validate

1.1 DataFrames & Series
Pandas attributes to remember:
●	.shape - returns number of rows & columns
●	.columns - returns column names as an index (which is a data structure similar to a list)

1.2 Clean & Validate
●	.value_counts().sort_index() - compare results to original dataset to validate data
●	.describe()
●	.replace([list of values to replace], np.nan) - takes a list of values we want to replace and the value we want to replace them with (in this case NaN)
●	.replace([list of values to replace], np.nan, inplace = True) - modifies the Pandas Series in place i.e. without making a copy, instead of making a new series

1.3 Filter & Visualize
●	When you compare a series to a value, the result is a boolean series
○	E.g. pre_term = nsfg[‘preg_length’] < 37
○	Boolean series treats True as 1 and False as 0
○	E.g. pre_term.sum() - sums the True values
●	Filter for weight of pre-term babies using the boolean series:
○	pre_term_weight = nsfg[‘birth_weight’][pre_term]
●	Logical operators:
○	birth_weight[A&B] - A and B
○	birth_weight[A|B] - A or B
●	plt.hist(nsfg[‘agecon’], bins=20, histtype='step') - creates an unfilled histogram

Example
# Filter full-term babies
full_term = nsfg['prglngth'] >= 37

# Filter single births
single = nsfg['nbrnaliv'] == 1

# Compute birth weight for single full-term babies
single_full_term_weight = birth_weight[full_term & single]
print('Single full-term mean:', single_full_term_weight.mean())

# Compute birth weight for multiple full-term babies
mult_full_term_weight = birth_weight[~single & full_term]
print('Multiple full-term mean:', mult_full_term_weight.mean())


2. Distributions

2.1 Probability Mass Functions (PMF)
●	Link to library
○	https://pypi.org/project/empiricaldist/

Example
# Make a PMF of age
# Pmf comes from a class created in the library above
pmf_age = Pmf(gss['age'])

# Plot the PMF
pmf_age.plot()

plt.show()


2.2 Cumulative Distribution Functions (CDF)
●	Link to library
○	https://pypi.org/project/empiricaldist/
●	Good way to visualize and compare distribution
●	PMF vs CDF
○	CDF(3) is 4/5 because 4 out of 5 values are less than or equal to 3


Example (evaluating the inverse CDF)
p = 0.25
q = cdf.inverse(p)
print(q)

Output
30 (25% of respondents are 30 years old or younger)

2.3 Comparing Distributions
●	CDFs omit most of the noise present in PMFs
●	Good for exploratory data analysis

Example
# Bachelor's degree
bach = (gss['educ'] >= 16)

# Associate degree
assc = (gss['educ'] >= 14) & (gss['educ'] < 16)

# Create a variable to extract income
income = gss['realinc']

# Plot the CDFs
# Extract incomes for associates and incomes for bachelors and then plot these
Cdf(income[assc]).plot(label='Associate')
Cdf(income[bach]).plot(label='Bachelor')

# Label the axes
plt.xlabel('Income (1986 USD)')
plt.ylabel('CDF')
plt.legend()
plt.show()


2.4 Modeling Distributions
●	Probability density function (PDF)
●	Normal/Gaussian distribution (bell curve)
○	KDE (Kernel Density Estimate)
■	Use points in the sample to estimate the PDF they came from
■	Used to move from a PMF to a PDF
■	Uses Seaborn library
●	Use CDFs for exploration
●	Use PMFs if there are a small number of unique values
●	Use KDE if there a lot of values
●	If data is said to be ‘lognormal’ then it fits a normal distribution

Example (using CDF)
Assessing whether income data is lognormal by comparing Cdf of the logarithm of the data to a normal distribution with the same mean and standard deviation.

# Extract realinc and compute its log
income = gss['realinc']
log_income = np.log10(income)

# Create and plot the Cdf of log_income
Cdf(log_income).plot()
plt.show()

# Compute mean and standard deviation
mean = log_income.mean()
std = log_income.std()

# Make a norm object
from scipy.stats import norm
dist = norm(mean,std)

# Evaluate the model CDF
xs = np.linspace(2, 5.5)
ys = dist.cdf(xs)

# Plot the model CDF
plt.clf()
plt.plot(xs, ys, color='gray')


Example (using PDF)
# Evaluate the normal PDF
xs = np.linspace(2, 5.5)
ys = dist.pdf(xs)

# Plot the model PDF
plt.clf()
plt.plot(xs, ys, color='gray')

# Plot the data KDE
sns.kdeplot(log_income)
plt.show()




3. Relationships (between variables)

3.1 Exploring Relationships
●	When a dataset has been smoothed i.e. data has been rounded, we can use a process called jittering to add randomness into the dataset and make it more realistic
○	Say the variable height in the DataFrame df has been smoothed, we can jitter it by:
■	height_jitter = df[‘height’] + np.random.normal(0, 1, size = len(df))

3.2 Visualizing Relationships (refer to Seaborn course) 
●	Violin plot: estimating the KDE function for each column of data and plotting it
●	Box plot
●	plt.yscale(‘log’) - changes the y-scale to the logarithmic scale


3.3 Correlation
●	Does not work well for non-linear relationships - will underestimate the correlation
●	In general you can conclude:
○	Correlation close to 1 means there is a linear relationship
○	Correlation close to 0 does not mean there is no relationship - but the relationship is likely non-linear

3.4 Simple/Linear Regression

Example
from scipy.stats import linregress

# Extract the variables
# Must drop NaN values as linear regression can’t handle them
subset = brfss.dropna(subset=['INCOME2', '_VEGESU1'])
xs = subset['INCOME2']
ys = subset['_VEGESU1']

# Compute the linear regression
res = linregress(xs,ys)
print(res)

Output
LinregressResult(slope=0.06988048092105019, intercept=1.5287786243363106, rvalue=0.11967005884864099, pvalue=1.378503916248713e-238, stderr=0.002110976356332333, intercept_stderr=0.013196467544093607)

Example (continued)
# Plot the line of best fit
fx = np.array([xs.min(),xs.max()])
fy = res.intercept + res.slope*fx
plt.plot(fx, fy, '-', alpha=0.7)
plt.show()


4. Multivariate relationships

4.1 Limits of Simple Regression
●	Only measures the strength of a linear relationship

4.2 Multiple Regression
●	Useful when we want to add control (constant) variables and describe non-linear relationships

Example 1

import statsmodels.formula.api as smf

# realinc is what we are trying to predict (y) and educ is the variable we are using to inform the prediction (x variable)
# Regress realinc as a function of educ
# OLS stands for ordinary least squares (another name for regression)
# the result from ols represents the model
# run .fit() to get the results
gss = pd.read_hdf('gss.hdf5', 'gss')
results = smf.ols('realinc ~ educ', data=gss).fit()
results.params

Output (shows intercept and slope)
Intercept   -11539.147837
educ          3586.523659
dtype: float64

This means each additional year of education is associated with a $3586 increase in income 


Example 2
Incorporating age

# on the right side of the formula you can add as many variables as you like
# adding age means we expect educ & age to contribute to realinc
results = smf.ols('realinc ~ educ + age', data=gss).fit()
results.params

Output (shows intercept and slope)
Intercept   -16117.275684
educ          3655.166921
age	      83.731804


Non-linear relationships
●	One option is to add a new variable that is a non-linear combination of other variables
●	E.g. adding a quadratic term

Example
import statsmodels.formula.api as smf

# Add a new column with educ squared
gss['educ2'] = gss['educ']**2

# Run a regression model with educ, educ2 and age
results = smf.ols('realinc ~ educ+educ2+age', data = gss).fit()

# Print the estimated parameters
print(results.params)


4.3 Visualizing Regression Results

Generating predictions
●	Uses the model to generate predictions
●	Easier to evaluate a model looking at its predictions rather than its parameters

Example
Using .predict(), which takes a DataFrame as a parameter and returns a series with a prediction for each row in the DataFrame

# Run a regression model with educ, educ2, age, and age2
results = smf.ols('realinc ~ educ + educ2 + age + age2', data=gss).fit()

# Make the DataFrame
df = pd.DataFrame()
df['educ'] = np.linspace(0,20)
df['age'] = 30
df['educ2'] = df['educ']**2
df['age2'] = df['age']**2

# Plot mean income in each age group
grouped = gss.groupby('educ')
mean_income_by_educ = grouped['realinc'].mean()
plt.plot(mean_income_by_educ,'o', alpha = 0.5)

# Generate and plot the predictions
pred = results.predict(df)
plt.plot(df['educ'], pred, label='Age 30')

# Label axes
plt.xlabel('Education (years)')
plt.ylabel('Income (1986 $)')
plt.legend()
plt.show()


4.4 Logistic Regression
●	Useful for exploring relationships between quantitative and categorical variables (must be binary)
●	Explaining and predicting binary variables i.e. those that can be assigned a 1 and 0 (i.e. to represent yes and no, agree and disagree etc.

Example
# Recode grass
# Only binary variables can be used (0 & 1), whereas currently the variables are (1&2)
# Replace 2 with 0
gss['grass'].replace(2, 0, inplace=True)

# Run logistic regression
# We are going to predict whether individuals are in favor of cannabis legalization based on certain factors
# Use C() to identify categorical variables
results = smf.logit('grass ~ age + age2 + educ + educ2 + C(sex)', data=gss).fit()
results.params

# Make a DataFrame with a range of ages
# We will feed this DataFrame into the regression model above
df = pd.DataFrame()
df['age'] = np.linspace(18, 89)
df['age2'] = df['age']**2

# Set the education level to 12
# We hold this constant i.e. individuals in our sample data set have 12 years of education
df['educ'] = 12
df['educ2'] = df['educ']**2

# Generate predictions for men and women
df['sex'] = 1
pred1 = results.predict(df)

df['sex'] = 2
pred2 = results.predict(df)

# Plot mean of those in favor of legalization in each age group
grouped = gss.groupby('age')
favor_by_age = grouped['grass'].mean()
plt.plot(favor_by_age, 'o', alpha=0.5)

# Plot the predictions
plt.plot(df['age'], pred1, label='Male')
plt.plot(df['age'], pred2, label = 'Female')

plt.xlabel('Age')
plt.ylabel('Probability of favoring legalization')
plt.legend()
plt.show()


DEALING WITH MISSING DATA IN PYTHON


1. The problem with missing data
●	Workflow for dealing with missing data:
1.	Convert all missing data to null values
2.	Analyze the type and amount of missing data
3.	Delete or impute missing values
4.	Evaluate the performance of the deleted/imputed dataset

Null value operations
●	None vs np.nan
●	None is of type NoneType while np.nan is of float. This allows np.nan to have both arithmetic and logical operations.

Example (Handling missing values)
# When importing a file - specify the argument na_values equal to the value you want to replace with NaN
import pandas as pd
college = pd.read_csv('college.csv', na_values = '.')


Example (Replace missing values with NaN)
# Set rows in the BMI column of the diabetes DataFrame that are equal to 0 to NaN
import numpy as np
diabetes[diabetes[‘BMI’] == 0] = np.nan

Example (unique values & sorting data)
# Store unique values of 'csat' column to 'csat_unique'
csat_unique = college.csat.unique()

# Print the sorted values of csat_unique
print(np.sort(csat_unique))

Example
# Set the 0 values of column 'BMI' to np.nan
diabetes.BMI[diabetes.BMI == 0] = np.nan

# Print the 'NaN' values in the column BMI
print(diabetes.BMI[np.isnan(diabetes.BMI)])

Analyze missingness
●	Use isnull() or isna() as both are the same
○	Returns a boolean

Example
# A useful package for visualising missing data
import missingno as msno

# Use the msno.bar() and msno.matrix() functions
# Set the frequency to month for msno.matrix()
# Call both these functions on the original DataFrame and not once isna() or isnull() is called
msno.bar(air_quality)
msno.matrix(air_quality, freq = ‘M’)
plt.show()



2. Does missingness have a pattern
●	Heatmap
○	Lighter colors signify greater correlation between variables
●	Dendrogram
○	Tree diagram of missingness
○	Describes correlation of variables by grouping them


Example
import missingno as msno
diabetes = pd.read_csv('pima-indians-diabetes data.csv')
msno.heatmap(diabetes)
msno.dendrogram(diabetes)
plt.show()

Output (for dendrogram)
 


How to delete missing data
●	Keep, impute or delete


Pairwise & Listwise deletion
●	Used when values are missing completely at random (i.e. the number of missing values should be small)
●	Listwise deletion drops the whole row whereas pairwise deletion drops individual entries

Example
# Visualize the missingness in the data
msno.matrix(diabetes)

# Visualize the correlation of missingness between variables
msno.heatmap(diabetes)

# Show heatmap
plt.show()

# Drop rows where 'BMI' has a missing value
diabetes.dropna(subset=['BMI'], how='all', inplace=True)



3. Imputation techniques

The .fillna() method
●	The method attribute can be set equal to:
○	‘ffill’ or ‘pad’: this is the forward fill and fills missing values based on the last observed value
○	‘bfill’ or ‘backwardfill’: fills missing values based on the next observed value
○	E.g. airquality.fillna(method='ffill', inplace=True)

The .interpolate() method
●	This method extends the sequence of values to the missing values
●	The method in .interpolate() can be set to:
○	‘linear’: fills in values by extrapolating a straight line between the last observed value and the next observed value after the NaNs
○	‘quadratic’: parabolic trajectory in the negative direction and gives back a positive value
○	‘nearest’: uses a combination of ‘ffill’ and ‘bfill’ to fill in the missing values
○	E.g. airquality.interpolate(method='linear', inplace=True)



4. Advanced imputation techniques

Imputing using fancyimpute (machine learning models)
●	KNN (K-nearest neighbor): selects similar data points using all the non-missing features. Takes the average of selected data points to fill in the missing feature.
●	MICE (Multiple imputation by chained equations)
○	Performs multiple regression over a random sample of data and the takes the average to impute the missing data

Example 1
from fancyimpute import KNN

# Initialize KNN
knn_imputer = KNN()

# Create a copy of the DateFrame so that we can compare to the original DataFrame later
diabetes_knn = diabetes.copy(deep=True)
diabetes_knn.iloc[:, :] = knn_imputer.fit_transform(diabetes_knn)


Example 2
from fancyimpute import IterativeImputer
MICE_imputer = IterativeImputer()
diabetes_MICE = diabetes.copy(deep=True)
diabetes_MICE.iloc[:, :] = MICE_imputer.fit_transform(diabetes_MICE)


Imputing categorical data
●	Categories usually consist of strings 
●	These need to be converted to numeric values by encoding them
●	Three steps:
1.	Convert non-missing categorical columns to ordinal values
2.	Impute the missing values in the original DataFrame
3.	Convert back from ordinal values to categorical values
●	Categorical features can be encoded using two techniques namely, one-hot encoding and ordinal encoding. In one-hot encoding, each category becomes a column and the respective category column for each row is 1 and the others 0. In ordinal encoding, the categories are mapped to integer values starting from 0 to number of categories.

Example 1
from sklearn.preprocessing import OrdinalEncoder

# Create Ordinal encoder
ambience_ord_enc = OrdinalEncoder()

# Select non-null values of ambience column in users
# OrdinalEncoder() cannot handle NaN value - therefore we must remove them
ambience = users['ambience']
ambience_not_null = ambience[ambience.notnull()]

# Reshape ambience_not_null to shape (-1, 1)
reshaped_vals = ambience_not_null.values.reshape(-1,1)

# Ordinally encode reshaped_vals
encoded_vals = ambience_ord_enc.fit_transform(reshaped_vals)

# Assign back encoded values to non-null values of ambience in users
users.loc[ambience.notnull(), 'ambience'] = np.squeeze(encoded_vals)

Example 2 (automate the encoding using a loop)
# Create an empty dictionary ordinal_enc_dict
ordinal_enc_dict = {}

for col_name in users:
    # Create Ordinal encoder for col
    ordinal_enc_dict[col_name] = OrdinalEncoder()
    col = users[col_name]
    
    # Select non-null values of col
    col_not_null = col[col.notnull()]
    reshaped_vals = col_not_null.values.reshape(-1, 1)
    encoded_vals = ordinal_enc_dict[col_name].fit_transform(reshaped_vals)
    
    # Store the values to non-null values of the column in users
    users.loc[col.notnull(), col_name] = np.squeeze(encoded_vals)

Example 3 (imputing the data)
# Create KNN imputer
KNN_imputer = KNN()

# Impute and round the users DataFrame
users.iloc[:, :] = np.round(KNN_imputer.fit_transform(users))

# Loop over the column names in users
for col_name in users:
    
    # Reshape the data
    reshaped = users[col_name].values.reshape(-1, 1)
    
    # Perform inverse transform of the ordinally encoded columns
    users[col_name] = ordinal_enc_dict[col_name].inverse_transform(reshaped)
