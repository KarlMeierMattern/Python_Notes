# ML Models  
Link to source https://www.datacamp.com/blog/machine-learning-models-explained  
1. ML regression models  
- Liner regression  
- Ridge regresssion  
- Lasso regression  
2. ML classification models  
- Logistic regression  
- K-nearest neighbours  
3. ML tree-based models  
- Decision trees  
- Random forests  

---

## ML regression models  
- Regression algorithms are used to predict a continuous outcome (y) using independent variables (x).  
- Where there are multuple input variables it is called a multivariate regression model.  
- The dependent variable (y) must be numeric.  
- For example, predicting the rent of a house based on its size, the number of bedrooms, and whether it is fully furnished.  

### Linear regression  
- `y = mx + c` there are infinite ways to draw this line based on `m` and `c`
- Line of best fit/least squares regression line is found by *minimizing the sum of squared distance between the true and predicted values*.  

### Ridge regression  
- Technique used to keep a regression model’s coefficients as low as possible.  
- One problem with a simple linear regression model is that its coefficients can become large, which makes the model more sensitive to inputs. This can lead to overfitting.  
- A perfect line on a trainining data set will likely not generalize well to test data. This phenomenon is called overfitting.  
- A model that is highly complex will pick up on unnecessary nuances of the training dataset that aren’t reflected in the real world. This model will perform extremely well on training data but will underperform on datasets outside what it was trained on.  
- A linear regression model with large coefficients is prone to overfitting.  
- Ridge regression is a *regularization technique* that will force the algorithm to choose smaller coefficients by penalizing its loss function to include an additional cost.  

### Lasso regression  
- Extension of linear regression that shrinks model coefficients by adding a penalty term to its cost function.  
- The biggest difference between ridge and lasso regression is that in ridge regression, while  model coefficients can shrink towards zero, they never actually become zero. In lasso regression, it is possible for model coefficients to become zero.  
- Due to this, lasso regression can also be used as a `feature selection technique`, since variables with low importance can have coefficients that reach zero and will be removed entirely from the model.  

### Regresssion metrics  
- A common misconception is that a regression model can be evaluated using a metric like accuracy.  
- Accuracy is a metric used to assess the performance of classification models.  
- Regression models, on the other hand, are evaluated using metrics such as MAE (Mean Absolute Error), MSE (Mean Squared Error), and RMSE (Root Mean Squared Error).  

#### Mean absolute error  
- Calculates the sum of the difference between all true and predicted values, and divides this by the total number of observations.  

#### Mean squared error  
- Calculates the sum of the difference between all true and predicted values squared, and divides this by the total number of observations.  

#### Root mean squared error  
- Calculated as the square root of the mean squared error.  
- One advantage of the RMSE over its MSE is that the error is returned in the same unit of the variable we are predicting.  

---

## ML classification models  
- We use classification algorithms to predict a discrete outcome (y) using independent variables (x).  
- The dependent variable is always a class or category.  
- For example, predicting whether a patient is likely to develop heart disease based on their risk factors is a classification problem.  
- If there are only two possible outcomes (e.g. Yes and No), this is called a binary classification problem.  
- Other examples of `binary classification` problems include classifying whether email is spam or legitimate, customer churn prediction, and deciding whether to provide someone a loan.  
- `Multiclass classification` problems incorporate three or more possible outcomes, such as weather forecasting or distinguishing between different animal species.  

## Logistic regression  
- Predicts the probability of an event taking place.  
- Unlike linear regression, logistic regression is modelled with an s-shaped curve, which is known as a logistic function.  
- The model predicts probability of an event occurring between 0 and 1. As such, there is a lower and upper bounds, unlike linear regression.  

### Example  
- Spam email: if the text contains little to no suspicious keywords, then the probability of it being spam will be low and close to 0, conversely many suspicious keywords will have a high probability of being spam, close to 1.  
- This probability is then turned into a classification outcome.  
- Data points with a probability >= 0.5 of being spam are classified as spam and the logistic regression model will return a classification outcome of 1.  
- Data points with a probability < 0.5 of being spam are classified by the model as “Not Spam” and will return a classification outcome of 0.  
- For binary classification problems, the default threshold of a logistic regression model is 0.5, which means that data points with a higher probability than 0.5 will automatically be assigned a label of 1. This threshold value can be manually changed depending on your use case to achieve better results.  
- In linear regression we found the line of best fit by minimizing the sum of squared error between the predicted and true values. In logistic regression, however, the coefficients are estimated using a technique called *maximum likelihood estimation* instead of least squares.  

## K-nearest neighbours (KNN)  
- Classification algorithm that classifies a data point based on what group the data points nearest to it belong to.  

### Example  
The K-Nearest Neighbors algorithm works like this:  
- Step 1: The model first stores all the training data.  
- Step 2: Then, it calculates the distance from the new data point to all points in the dataset.  
- Step 3: The model sorts these data points based on their distance to the new data point.  
- Step 4: The new data point is assigned to the class of its nearest neighbors depending on the value of “k.”  
- If we assign k as 1, the model looks at only one closest neighbor to the data point and assigns the data point to that class.  
- Choosing different values of k will impact what class the new point is assigned to.  

### Classification metrics  
- While accuracy is the most used metric, it is not always the most reliable.  

#### Accuracy  
- The fraction of correct predictions made by the machine learning model.  

#### Precision  
- Calculates the quality of positive predictions made by the model.  

#### Recall  
- Used to calculate the quality of negative predictions made by the model.  

#### Example  
- There is a rare, fatal disease that affects a fraction of the population. 95% of the patients in a hospital’s database do not have the disease, while only 5% do. 
- If the ML algorithm predicts that nobody has the disease, then the training accuracy of this model will be 95%. Despite the high accuracy, we know this is not a good model.  
- Precision, or specificity, tells us the ability of the model to correctly identify people without the disease.  
- Recall, or sensitivity, tells us how well the model identifies people with the disease.  
- A “good” precision and recall value is subjective and depends on your use case.  
- In this disease prediction scenario, we always want to identify people with the disease, even if this comes with the risk of a false positive. Here, we will build the model to have higher recall than precision.  
- However, if we were to build a model that prevents malicious actors from entering an e-commerce website, we might want higher precision since blocking legitimate users will lead to a decline in sales.
- We often use a metric called the `F1-Score` to find the harmonic mean of a classifier’s precision and recall. Simply put, the F1-Score combines precision and recall into a single metric by computing their average.  
- AUC, or Area Under the Curve, is another popular metric used to measure the performance of a classification model. An algorithm’s AUC tells us about its ability to distinguish between positive and negative classes.  

---

## ML tree-based models  
- Tree-based models are *supervised* machine learning algorithms that construct a tree-like structure to make predictions.  
- They can be used for both classification and regression problems.  

### Decision trees  
- This model allows us to continuously split the dataset based on specific parameters until a final decision is made.  
- The decision tree will choose a variable to split on first based on a metric called `entropy`.  
- Entropy is a measure of disorder or impurity in a node. Thus, a node with more variable composition, such as Pass and Fail would be considered to have higher entropy than a node which has only pass or only fail.  
- It will stop splitting when a `pure split` is obtained, i.e., when all the data points belong to a single class.  
- The structure is created based on a metric called `information gain`. The best possible decision tree is one with the highest information gain.  
- Decision trees are prone to overfitting if left to grow completely. This is because they are designed to split perfectly on all samples of the training dataset, which makes them unable to generalize well to external data.  

### Random forests  
- Created by combining the predictions made by multiple decision tree models and returning a single output.  
- Step 1: The rows and variables of the dataset are randomly sampled with replacement. Multiple decision trees are then created and trained on each data sample.  
- Step 2: The predictions made by all these decision trees are combined to come up with a single output. For instance, if 3 separate decision trees were trained and 2 of them predicted “Yes” while 1 predicted “No,” then the final outcome of the random forest algorithm would be “Yes.”  
- In case of a regression problem, the outcome will be the average prediction of all decision trees.  
- One of the biggest advantages of the random forest algorithm is that it generalizes well, since it combines the output of multiple decision trees that are trained on a subset of features.  

---

## ML clustering models  
- Unsupervised learning approach.  
- Clustering is the task of creating a group of objects that are similar to each other but different from others.  
- Business use cases include: recommending movies to users with similar viewing patterns on a video streaming site, anomaly detection, and customer segmentation.  

### K-means clustering  
- Used to group similar objects together in data.  
- Step 1: Initially, each observation will be assigned to a cluster at random. A centroid will then be computed for each cluster.  
- Step 2: The distance of each data point to the centroid is measured, and each point is assigned to the nearest centroid.  
- Step 3: The centroid of the new cluster is then recalculated, and data points will be reassigned accordingly.  
- Step 4: This process is repeated until data points are no longer being reassigned.  

---

## sklearn  
- These import statements allow you to use different scikit-learn functionalities, such as loading datasets, splitting data, preprocessing, building models, and evaluating model performance.  

    from sklearn import datasets  

> For loading example datasets  

    from sklearn.model_selection import train_test_split  

> For splitting data into training and testing sets  

    from sklearn.preprocessing import StandardScaler  

> For feature scaling  

    from sklearn.linear_model import LogisticRegression  

> For logistic regression model

    from sklearn.tree import DecisionTreeClassifier  

> For decision tree classifier

    from sklearn.metrics import accuracy_score  

> For evaluating model accuracy  

---