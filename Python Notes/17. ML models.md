# ML Models  
Link to source https://www.datacamp.com/blog/machine-learning-models-explained  
1. ML regression models (supervised - numeric)  
- Liner regression  
- Ridge regresssion  
- Lasso regression  
2. ML classification models (supervised - categorical)  
- Logistic regression  
- K-nearest neighbours  
3. ML tree-based models (supervised)  
- Decision trees  
- Random forests  
4. ML clustering (unsupervised)  
- K-means clustering  

---

## ML regression models  
- Regression algorithms are used to predict a continuous outcome (y) using independent variables (x).  
- Where there are multuple input variables it is called a multivariate regression model.  
- The dependent variable (y) must be numeric.  
- For example, predicting the rent of a house based on its size, the number of bedrooms, and whether it is fully furnished.  

### Linear regression  
- `y = mx + c` there are infinite ways to draw this line based on `m` and `c`
- Line of best fit/least squares regression line is found by *minimizing the sum of squared distance between the true and predicted values*.  

### Regresssion metrics  
- A common misconception is that a regression model can be evaluated using a metric like accuracy.  
- Accuracy is a metric used to assess the performance of classification models.  
- Regression models, on the other hand, are evaluated using metrics such as MAE (Mean Absolute Error), MSE (Mean Squared Error), and RMSE (Root Mean Squared Error).  

#### Mean absolute error  
- Calculates the sum of the difference between all true and predicted values, and divides this by the total number of observations.  

#### Mean squared error  
- Calculates the sum of the difference between all true and predicted values squared, and divides this by the total number of observations.  

#### Root mean squared error  
- Calculated as the square root of the mean squared error.  
- One advantage of the RMSE over its MSE is that the error is returned in the same unit of the variable we are predicting.  

### Lasso & Ridge  
- Helps reduce model complexity through regularisation by adding a penalty term (lambda) to the cost function.  
- In this way they reduce the coefficients of the features.  
- Lasso reduces some coefficients to zero and can thus perform feature selection.  

---

## ML classification models  
- We use classification algorithms to predict a discrete outcome (y) using independent variables (x).  
- The dependent variable is always a class or category.  
- For example, predicting whether a patient is likely to develop heart disease based on their risk factors is a classification problem.  
- If there are only two possible outcomes (e.g. Yes and No), this is called a binary classification problem.  
- Other examples of `binary classification` problems include classifying whether email is spam or legitimate, customer churn prediction, and deciding whether to provide someone a loan.  
- `Multiclass classification` problems incorporate three or more possible outcomes, such as weather forecasting or distinguishing between different animal species.  

### Logistic regression  
- Predicts the probability of an event taking place.  
- Unlike linear regression, logistic regression is modelled with an s-shaped curve, which is known as a logistic function.  
- The model predicts probability of an event occurring between 0 and 1. As such, there is a lower and upper bounds, unlike linear regression.  

#### Example  
- Spam email: if the text contains little to no suspicious keywords, then the probability of it being spam will be low and close to 0, conversely many suspicious keywords will have a high probability of being spam, close to 1.  
- This probability is then turned into a classification outcome.  
- Data points with a probability >= 0.5 of being spam are classified as spam and the logistic regression model will return a classification outcome of 1.  
- Data points with a probability < 0.5 of being spam are classified by the model as “Not Spam” and will return a classification outcome of 0.  
- For binary classification problems, the default threshold of a logistic regression model is 0.5, which means that data points with a higher probability than 0.5 will automatically be assigned a label of 1. This threshold value can be manually changed depending on your use case to achieve better results.  
- In linear regression we found the line of best fit by minimizing the sum of squared error between the predicted and true values. In logistic regression, however, the coefficients are estimated using a technique called *maximum likelihood estimation* instead of least squares.  

### K-nearest neighbours (KNN)  
- Classification algorithm that classifies a data point based on what group the data points nearest to it belong to.  

#### Example  
The K-Nearest Neighbors algorithm works like this:  
- Step 1: The model first stores all the training data.  
- Step 2: Then, it calculates the distance from the new data point to all points in the dataset.  
- Step 3: The model sorts these data points based on their distance to the new data point.  
- Step 4: The new data point is assigned to the class of its nearest neighbors depending on the value of “k.”  
- If we assign k as 1, the model looks at only one closest neighbor to the data point and assigns the data point to that class.  
- Choosing different values of k will impact what class the new point is assigned to.  

### Classification metrics  
- While accuracy is the most used metric, it is not always the most reliable.  

#### Accuracy  
- The fraction of correct predictions made by the machine learning model.  

#### Precision  
- Calculates the quality of positive predictions made by the model.  

#### Recall  
- Used to calculate the quality of negative predictions made by the model.  
- The more false negatives the worse the score.  

#### Example  
- There is a rare, fatal disease that affects a fraction of the population. 95% of the patients in a hospital’s database do not have the disease, while only 5% do. 
- If the ML algorithm predicts that nobody has the disease, then the training accuracy of this model will be 95%. Despite the high accuracy, we know this is not a good model.  
- Precision, or specificity, tells us the ability of the model to correctly identify people without the disease.  
- Recall, or sensitivity, tells us how well the model identifies people with the disease.  
- A “good” precision and recall value is subjective and depends on your use case.  
- In this disease prediction scenario, we always want to identify people with the disease, even if this comes with the risk of a false positive. Here, we will build the model to have higher recall than precision.  
- However, if we were to build a model that prevents malicious actors from entering an e-commerce website, we might want higher precision since blocking legitimate users will lead to a decline in sales.
- We often use a metric called the `F1-Score` to find the harmonic mean of a classifier’s precision and recall. Simply put, the F1-Score combines precision and recall into a single metric by computing their average.  
- AUC, or Area Under the Curve, is another popular metric used to measure the performance of a classification model. An algorithm’s AUC tells us about its ability to distinguish between positive and negative classes.  

---

## TP, FP, FN, TN  
- **False positive**: when a scientist determines something is true when it is actually false (also called a type I error). A false positive is a “false alarm”.  
- **False negative**: saying something is false when it is actually true (also called a type II error). A false negative means something that is there was not detected; something was missed.  

### Example 1 (healthy is the prediction class)
Consider healthy as the positive class and not healthy (diseased) as the negative class, the classifications would be as follows:  
1. Predict healthy & they are healthy: This is a True Positive (TP) because the model correctly predicts a healthy individual as healthy.  
2. Predict healthy & they aren't healthy: This is a False Positive (FP) because the model incorrectly predicts a healthy individual as not healthy.  
3. Predict aren't healthy & they are healthy: This is a False Negative (FN) because the model incorrectly predicts a not healthy individual when they are actually healthy.  
4. Predict aren't healthy & they aren't healthy: This is a True Negative (TN) because the model correctly predicts a not healthy individual as not healthy.  

### Example 2 (unhealthy is the prediction class)  
Consider unhealthy as the positive class and healthy as the negative class, the classifications would be as follows:  
1. Predict healthy & they are healthy: This is a True Negative (TN) because the model correctly predicts a healthy individual as healthy.  
2. Predict healthy & they aren't healthy: This is a False Negative (FN) because the model incorrectly predicts a healthy individual as not healthy.  
3. Predict aren't healthy & they are healthy: This is a False Positive (FP) because the model incorrectly predicts a not healthy individual when they are actually healthy.  
4. Predict aren't healthy & they aren't healthy: This is a True Positive (TP) because the model correctly predicts a not healthy individual as not healthy.  

---

## ML tree-based models  
- Tree-based models are *supervised* machine learning algorithms that construct a tree-like structure to make predictions.  
- They can be used for both classification and regression problems.  
- A tree's depth is a measure of how many splits it makes before coming to a prediction.  
- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html  

### Decision trees  
- This model allows us to continuously split the dataset based on specific parameters until a final decision is made.  
- The decision tree will choose a variable to split on first based on a metric called `entropy`.  
- Entropy is a measure of disorder or impurity in a node. Thus, a node with more variable composition, such as Pass and Fail would be considered to have higher entropy than a node which has only pass or only fail.  
- It will stop splitting when a `pure split` is obtained, i.e., when all the data points belong to a single class.  
- The structure is created based on a metric called `information gain`. The best possible decision tree is one with the highest information gain.  
- Decision trees are prone to overfitting if left to grow completely. This is because they are designed to split perfectly on all samples of the training dataset, which makes them unable to generalize well to external data.  

### Random forests  
- Created by combining the predictions made by multiple decision tree models and returning a single output.  
- Step 1: The rows and variables of the dataset are randomly sampled with replacement. Multiple decision trees are then created and trained on each data sample.  
- Step 2: The predictions made by all these decision trees are combined to come up with a single output. For instance, if 3 separate decision trees were trained and 2 of them predicted “Yes” while 1 predicted “No,” then the final outcome of the random forest algorithm would be “Yes.”  
- In case of a regression problem, the outcome will be the average prediction of all decision trees.  
- One of the biggest advantages of the random forest algorithm is that it generalizes well, since it combines the output of multiple decision trees that are trained on a subset of features.  
- Known as an ensemble method - combines the predictions of multiple models.  

---

## ML clustering models  
- *Unsupervised* learning approach.  
- Clustering is the task of creating a group of objects that are similar to each other but different from others.  
- Business use cases include: recommending movies to users with similar viewing patterns on a video streaming site, anomaly detection, and customer segmentation.  

### K-means clustering  
- Used to group similar objects together in data.  
- Step 1: Initially, each observation will be assigned to a cluster at random. A centroid will then be computed for each cluster.  
- Step 2: The distance of each data point to the centroid is measured, and each point is assigned to the nearest centroid.  
- Step 3: The centroid of the new cluster is then recalculated, and data points will be reassigned accordingly.  
- Step 4: This process is repeated until data points are no longer being reassigned.  

---

## The steps to building and using a model  
- Define: What type of model will it be? A decision tree? Some other type of model?  
- Fit: Capture patterns from provided data. This is the heart of modeling.  
- Predict: Just what it sounds like.  
- Evaluate: Determine how accurate the model's predictions are.  

## How models works  
- Fitting/training the model is when we use data to decide how to split the data into categories, and then, based on that category, what the prediction would be.  
- After the model has been fit, you can apply it to new data to predict prices of additional homes.  
- You can capture more factors using a tree that has more `splits`. These are called "deeper" trees.  
- The point at the bottom where we make a prediction is called a `leaf`.  

## scikit-learn    
- These import statements allow you to use different scikit-learn functionalities, such as loading datasets, splitting data, preprocessing, building models, and evaluating model performance.  

    from sklearn import datasets  

> For loading example datasets  

    from sklearn.model_selection import train_test_split  

> For splitting data into training and testing sets  

    from sklearn.preprocessing import StandardScaler  

> For feature scaling  

    from sklearn.linear_model import LogisticRegression  

> For logistic regression model

    from sklearn.tree import DecisionTreeClassifier  

> For decision tree classifier

    from sklearn.metrics import accuracy_score  

> For evaluating model accuracy  

### Model prediction    
- Store the `prediction target` as `y`.  
- Store the `features`, containing a list of columns, as `X`.  

        import pandas as pd

        melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
        melbourne_data = pd.read_csv(melbourne_file_path) 

        filtered_melbourne_data = melbourne_data.dropna(axis=0)

        y = filtered_melbourne_data.Price
        melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']
        X = filtered_melbourne_data[melbourne_features]

        from sklearn.tree import DecisionTreeRegressor
        melbourne_model = DecisionTreeRegressor(random_state=1)
        melbourne_model.fit(X, y)
        melbourne_model.predict(X)

### Model validation  
- Measures the quality of the model.  
- In most applications, the relevant measure of model quality is predictive accuracy.  
- The mistake is to make predictions with training data and compare those predictions to the target values in the training data.  
- The most straightforward way to do this is to exclude some data from the model-building process, and then use those to test the model's accuracy on data it hasn't seen before. This data is called *validation* data.  
- The best model will have the lowest MAE.  

#### The wrong way  
- Using *in-sample* data.  
- We used a single "sample" of houses for both building the model and evaluating it.  

        from sklearn.metrics import mean_absolute_error

        predicted_home_prices = melbourne_model.predict(X)
        mean_absolute_error(y, predicted_home_prices)

#### The correct way  
- Using *out-of-sample* data.  
- `train_test_split` is used to break up the data into two pieces.  
- We'll use some of that data as training data to fit the model, and we'll use the other data as validation data to calculate `mean_absolute_error`.

        from sklearn.model_selection import train_test_split

        train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1)
        melbourne_model = DecisionTreeRegressor(random_state = 1)
        melbourne_model.fit(train_X, train_y)

        val_predictions = melbourne_model.predict(val_X)

        print("The predictions are", melbourne_model.predict(val_X.head()))
        print("The actual data is", y.head().tolist())

        print(mean_absolute_error(val_y, val_predictions))

---

## Underfitting & overfitting  
- When we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are quite close to those homes' actual values, but they may make very unreliable predictions for new data (because each prediction is based on only a few houses).  
- This is a phenomenon called *overfitting*, where a model matches the training data almost perfectly, but does poorly in validation and other new data.  
- *Overfitting*: capturing spurious patterns that won't recur in the future, leading to less accurate predictions.  
- On the flip side, if we make our tree very shallow, it doesn't divide up the houses into very distinct groups.  
- When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called *underfitting*.  
- *Underfitting*: failing to capture relevant patterns, leading to less accurate predictions.  
- There are a few alternatives for controlling the tree depth, and many allow for some routes through the tree to have greater depth than other routes.  
- We use *validation* data, which isn't used in model training, to measure a candidate model's accuracy. This lets us try many candidate models and keep the best one.  
- `max_leaf_nodes` argument provides a very sensible way to control overfitting vs underfitting.  

#### Example  

    from sklearn.metrics import mean_absolute_error
    from sklearn.tree import DecisionTreeRegressor

    def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
        model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
        model.fit(train_X, train_y)
        preds_val = model.predict(val_X)
        mae = mean_absolute_error(val_y, preds_val)
        return(mae)

    for max_leaf_nodes in [5, 50, 500, 5000]:
        my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
        print("Max leaf nodes: %d  \t\t Mean Absolute Error:  %d" %(max_leaf_nodes, my_mae))

> The data is loaded into train_X, val_X, train_y and val_y using the code seen previously.  
> Here we are comparing MAE with differing values for max_leaf_nodes.  

---

#### Full example  

    import pandas as pd
    from sklearn.metrics import mean_absolute_error
    from sklearn.model_selection import train_test_split
    from sklearn.tree import DecisionTreeRegressor

    iowa_file_path = '../input/home-data-for-ml-course/train.csv'

    home_data = pd.read_csv(iowa_file_path)
    y = home_data.SalePrice
    features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']
    X = home_data[features]

    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)

    iowa_model = DecisionTreeRegressor(random_state=1)
    iowa_model.fit(train_X, train_y)

    val_predictions = iowa_model.predict(val_X)
    val_mae = mean_absolute_error(val_predictions, val_y)
    print(f"Validation MAE: {val_mae:,.0f}")

    def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
        model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
        model.fit(train_X, train_y)
        preds_val = model.predict(val_X)
        mae = mean_absolute_error(val_y, preds_val)
        return(mae)

    candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]
    scores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}
    best_tree_size = min(scores, key=scores.get)

    final_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)

    final_model.fit(X, y)

> Because we now know the `best_tree_size`, with the lowest MAE (being 100), we will make the model more accruate by using all of the data with that tree size. This means we won't save any validation data.  
> As such we use `final_model.fit(X,y)` and not `final_model.fit(train_X,train_y)`.  

---

## Random forests  
- The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters.  

        import pandas as pd
            
        melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
        melbourne_data = pd.read_csv(melbourne_file_path) 
        melbourne_data = melbourne_data.dropna(axis=0)
        y = melbourne_data.Price
        melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']
        X = melbourne_data[melbourne_features]

        from sklearn.model_selection import train_test_split

        train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0, n_estimators = 20)

        from sklearn.ensemble import RandomForestRegressor
        from sklearn.metrics import mean_absolute_error

        forest_model = RandomForestRegressor(random_state=1)
        forest_model.fit(train_X, train_y)
        melb_preds = forest_model.predict(val_X)
        print(mean_absolute_error(val_y, melb_preds))

> `n_estimators` sets the number of decision trees to be included in the random forest, in this case 20.  
> Increasing the number of estimators can improve the performance of the random forest by reducing the variance of the predictions.  
> However, it also increases the computational cost, as more decision trees need to be trained and evaluated.  

---

## Missing values  

### Drop columns with missing values  

    cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]

    reduced_X_train = X_train.drop(cols_with_missing, axis=1)
    reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)

> Identifies and creates a list of columns with missing values.  
> Drops those columns from both the training and validation datasets.  

### Imputation  

    from sklearn.impute import SimpleImputer

    my_imputer = SimpleImputer()
    imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
    imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))

    imputed_X_train.columns = X_train.columns
    imputed_X_valid.columns = X_valid.columns

    print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))

> Imputation removed column names so we put them back.  

### Compare approaches  

    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_absolute_error

    def score_dataset(X_train, X_valid, y_train, y_valid):
        model = RandomForestRegressor(n_estimators=100, random_state=0)
        model.fit(X_train, y_train)
        preds = model.predict(X_valid)
        return mean_absolute_error(y_valid, preds)

> This is a generic function that you can reuse to compare different approaches to dealing with missing values.  
> The approach that results in the lowest MAE should be chosen.  

    score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid)

> We could use this function to check the MAE of the approach above where we dropped missing columns.  

---

## Categorical variables  

### Drop categorical variables  
- The easiest approach to dealing with categorical variables is to simply remove them from the dataset.  
- This approach will only work well if the columns did not contain useful information.

### Ordinal encoding  
- Assigns each unique value to a different integer.  
- This approach assumes an ordering of the categories: "Never" (0) < "Rarely" (1) < "Most days" (2) < "Every day" (3).  
- This assumption makes sense in this example, because there is an indisputable ranking to the categories.  
- But, not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables.  
- For tree-based models (like decision trees and random forests), you can expect ordinal encoding to work well with ordinal variables.  

### One-hot encoding  
- Creates new columns indicating the presence (or absence) of each possible value in the original data.  
- E.g. if "Color" is a categorical variable with three categories: "Red", "Yellow", and "Green". The corresponding one-hot encoding contains one column for each possible value, and one row for each row in the original dataset. Wherever the original value was "Red", we put a 1 in the "Red" column; if the original value was "Yellow", we put a 1 in the "Yellow" column, and so on.  
- One-hot encoding does not assume an ordering of the categories.  
- We refer to categorical variables without an intrinsic ranking as nominal variables.  
- One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values).  
- We refer to the number of unique entries of a categorical variable as the cardinality of that categorical variable.  
- For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset. For this reason, we typically will only one-hot encode columns with relatively low cardinality. Then, high cardinality columns can either be dropped from the dataset, or we can use ordinal encoding.  

#### Syntax  

    from sklearn.preprocessing import OneHotEncoder

    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)

    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))
    OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))

    OH_cols_train.index = X_train.index
    OH_cols_valid.index = X_valid.index

    num_X_train = X_train.select_dtypes(exclude='object')
    num_X_valid = X_valid.select_dtypes(exclude='object')

    OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
    OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)

    OH_X_train.columns = OH_X_train.columns.astype(str)
    OH_X_valid.columns = OH_X_valid.columns.astype(str)

#### Selecting categorical columns with low cardinality  

    categorical_cols = [col for col in X_train_full.columns if X_train_full[col].nunique() < 10 and X_train_full[col].dtype == "object"]  

#### Selecting numerical columns  

    numerical_cols = [col for col in X_train_full.columns if X_train_full[col].dtype in ['int64', 'float64']]  

---

## Pipelines  
- Simple way to keep your data preprocessing and modeling code organized.  
- Bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.  
- Pipelines help with cleaner code, fewer bugs, and make it easier to productionize.  

### Three steps  
1. Define preprocessing steps:  
2. Define the model  
3. Create & evaluate the pipeline  

#### Step  

    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    from sklearn.impute import SimpleImputer
    from sklearn.preprocessing import OneHotEncoder

    numerical_transformer = SimpleImputer(strategy='constant')

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_cols),
            ('cat', categorical_transformer, categorical_cols)
        ])

> Pre-processing for numerical & categorical data is bundled into a second pre-processing step using ColumnTransformer.  
> Imputes missing values in numerical data.  
> Imputes missing values and applies one-hot encoding to categorical data.  
> Use the `ColumnTransformer` class to bundle together different preprocessing steps.  

#### Step 2  

    from sklearn.ensemble import RandomForestRegressor

    model = RandomForestRegressor(n_estimators=100, random_state=0)

> We set the number of trees in the random forest model with the `n_estimators` parameter, and setting `random_state` ensures reproducibility.  

#### Step 3  
- Use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps.  
- With the pipeline, we preprocess the training data and fit the model in a single line of code.  
- In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps.  
- This becomes especially messy if we have to deal with both numerical and categorical variables.  
- With the pipeline, we supply the unprocessed features in `X_valid` to the `predict()` command, and the pipeline automatically preprocesses the features before generating predictions.  
- Without a pipeline, we have to remember to preprocess the validation data before making predictions.  

    from sklearn.metrics import mean_absolute_error

    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                ('model', model)
                                ])

    my_pipeline.fit(X_train, y_train)

    preds = my_pipeline.predict(X_valid)

    score = mean_absolute_error(y_valid, preds)
    print('MAE:', score)

> Bundle preprocessing and modeling code in a pipeline.  

---

## Cross-validation  
- In general, the larger the validation set, the less randomness (aka "noise") there is in our measure of model quality, and the more reliable it will be.  
- In cross-validation we use every fold once as the holdout set meaning 100% of the data is used as holdout data at some point.  
- We end up with a measure of model quality that is based on all of the rows in the dataset (even if we don't use all rows simultaneously).  
- For small datasets, where extra computational burden isn't a big deal, you should run cross-validation.  
- For larger datasets, a single validation set is sufficient. Your code will run faster, and you may have enough data that there's little need to re-use some of it for holdout.  
- Using cross-validation has the added benefit of cleaning up our code as we no longer need to keep track of separate training and validation sets.  


## XGBoost (extreme gradient boosting)  
- https://xgboost.readthedocs.io/en/latest/  
- Leading library for working with standard tabular data, like Pandas DataFrames, as opposed to more exotic types of data like images and videos.  
- Type of ensemble method.  
- Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble.  
- It begins by initializing the ensemble with a single model, whose predictions can be pretty naive.  
Method:  
- We use the current ensemble to generate predictions for each observation in the dataset.  
- These predictions are used to calculate a loss function (like mean squared error, for instance).  
- Then, we use the loss function to fit a new model by determining model parameters that reduce the loss (using gradient descent on the loss function).  
- Finally, we add the new model to ensemble, and repeat.  
Process: naive model -> make predications -> calculate loss -> train new model -> add new model to ensemble.  
Parameter tuning:  
- `n_estimators` specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble. Typical values range from 100-1000. Too low a value causes underfitting, too high a value causes overfitting.  
- `early_stopping_rounds` offers a way to automatically find the ideal value for `n_estimators`. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for `n_estimators`. You specify a number for how many rounds of straight deterioration to allow before stopping. Setting `early_stopping_rounds=5` is a reasonable choice. In this case, we stop after 5 straight rounds of deteriorating validation scores. When using `early_stopping_rounds`, you also need to set aside some data for calculating the validation scores - this is done by setting the `eval_set` parameter. If you later want to fit a model with all of your data, set `n_estimators` to whatever value you found to be optimal when running `early_stopping_rounds` stopping.  
- `learning rate` means that instead of using the full prediction from each tree in the ensemble, you multiply each tree's prediction by a small number (the learning rate) before adding them together. This makes each tree's contribution smaller, and it helps prevent overfitting. A small learning rate means slower learning, but it can lead to more accurate models with a larger number of trees (`n_estimators`).  
- `n_jobs` is used on larger datasets where runtime is a consideration. You can use parallelism to build your models faster. It's common to set the parameter `n_jobs` equal to the number of cores on your machine. It's useful in large datasets where you would otherwise spend a long time waiting during the fit command.  

## Data leakage  
- Happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction.  
- This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production.  
Two types of leakage:  
1. Target leakage:  
- Occurs when your predictors include data that will not be available at the time you make predictions.  
- It is important to think about target leakage in terms of the timing or chronological order that data becomes available.  
- Variables that are updated/dependent on the target value cause data leakage.  
- E.g. if you are trying to predict if a person has pneumonia or not, the feature took_antibiotics is updated after the target is determined and will cause data leakage.  
- To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded.  
- A combination of caution, common sense, and data exploration can help identify target leakage.  
2. Train-test contamination:  
- Occurs when you aren't careful to distinguish training data from validation data.  
- E.g. imagine you run preprocessing (like fitting an imputer for missing values) before calling train_test_split(). You incorporated data from the validation data into how you make predictions using the training data. Your model may get good validation scores, but perform poorly when you deploy it to make decisions.  
- If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. This is easier if you use scikit-learn pipelines and perform preprocessing inside the pipeline.  
- Careful separation of training and validation data can prevent train-test contamination, and pipelines can help implement this separation.  

